{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import retro to play Street Fighter using a ROM\n",
    "import retro\n",
    "# Import time to slow down game\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m retro.import . # Run this from the roms folder, or where you have your game roms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class for a wrapper \n",
    "from gym import Env \n",
    "# Import the space shapes for the environment\n",
    "from gym.spaces import MultiBinary, Discrete, Box\n",
    "# Import numpy to calculate frame delta \n",
    "import numpy as np\n",
    "# Import opencv for grayscaling\n",
    "import cv2\n",
    "# Import matplotlib for plotting the image\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, A, _, _, UP, DOWN, LEFT, RIGHT, C, Y, X, Z\n",
    "# Every possible action for a given step in Street Fighter II\n",
    "possible_actions = {\n",
    "    # Idle\n",
    "    0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Left\n",
    "    1 : [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    # Right\n",
    "    2 : [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    # Up\n",
    "    3 : [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Down\n",
    "    4 : [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    # Light Kick\n",
    "    5 : [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Medium Kick\n",
    "    6 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Hard Kick\n",
    "    7 : [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    # Light Punch\n",
    "    8 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    # Medium Punch\n",
    "    9 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    # Hard Punch\n",
    "    10 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    # Down Left\n",
    "    11 : [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "    # Down Right\n",
    "    12 : [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
    "    # Up Left\n",
    "    13 : [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    # Up Right\n",
    "    14 : [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class based on github\n",
    "class StreetFighter(Env): # pass in basic env from above to preprocessing\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit from base env\n",
    "        # Specify action space and observation space \n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(84, 84, 3),  # Keep 3 channels for RGB\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.action_space = Discrete(12) # type of actions that can be taken\n",
    "        self.health = 144\n",
    "        self.enemy_health = 144\n",
    "        self.score = 0\n",
    "        self.matches_won = 0\n",
    "        self.continue_timer = 100\n",
    "        self.enemy_matches_won = 0\n",
    "        # self.previous_action = np.zeros(12)\n",
    "        # self.combo_scaler = 1\n",
    "        # self.last_damage_instance = 0\n",
    "        # Startup and instance of the game \n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.DISCRETE) # used to get valid button combos\n",
    "    \n",
    "    def reset(self): # restart\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs # sets previous frame to current frame\n",
    "        \n",
    "        # Create a attribute to hold the score delta \n",
    "        self.score = 0 \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        # Resize first to reduce computation\n",
    "        resized = cv2.resize(observation, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Simple color quantization using bitwise operations\n",
    "        # Reduce to 3 bits per channel (8 values per channel)\n",
    "        quantized = resized & 0b11100000\n",
    "        \n",
    "        # Optional: Create more distinct colors by increasing contrast\n",
    "        # This helps make different elements more distinguishable\n",
    "        quantized = cv2.convertScaleAbs(quantized, alpha=1.2, beta=10)\n",
    "        \n",
    "        # Method 1: Simple bitwise quantization\n",
    "        return quantized\n",
    "    \n",
    "    def reward_function(self, state):\n",
    "        # Extract variables\n",
    "        continuetimer = state['continuetimer']\n",
    "        enemy_matches_won = state['enemy_matches_won']\n",
    "        enemy_health = state['enemy_health']\n",
    "        health = state['health']\n",
    "        matches_won = state['matches_won']\n",
    "        score = state['score']\n",
    "\n",
    "        # Initialize reward\n",
    "        reward = 0\n",
    "\n",
    "        # Reward for increasing score each frame (scaled down to avoid excessively large rewards)\n",
    "        reward += score * 0.001  \n",
    "\n",
    "        enemy_health_diff = self.enemy_health - enemy_health\n",
    "        health_diff = self.health - health\n",
    "\n",
    "        # catching edge cases to make sure no reward is being earned outside of a fight (i.e. in between rounds)\n",
    "        if (self.enemy_health != 0 and state['enemy_health'] == 0 and self.health != 0 and state['health'] == 0) or (enemy_health_diff == 0 and health_diff == 0) or (self.health == 0 and self.enemy_health == 0):\n",
    "            reward += 0\n",
    "        else:\n",
    "            if enemy_health_diff > health_diff:\n",
    "                reward += ((enemy_health_diff) - (health_diff)) * 10\n",
    "            else:\n",
    "                reward += ((enemy_health_diff) - (health_diff))\n",
    "\n",
    "        # Update previous states to enable frame-by-frame comparison\n",
    "        self.enemy_health = enemy_health\n",
    "        self.health = health\n",
    "        self.matches_won = matches_won\n",
    "        self.enemy_matches_won = enemy_matches_won\n",
    "        self.continue_timer = continuetimer\n",
    "        self.score = score\n",
    "        # self.last_damage_instance += 1\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def step(self, action): # how do we process action\n",
    "        # Take a step \n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs) \n",
    "\n",
    "        self.previous_action = action\n",
    "        \n",
    "        # Frame delta \n",
    "        frame_delta = obs\n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Reshape the reward function\n",
    "        reward = self.reward_function(info)\n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs): # unpack any args and kwargs from stable baseline\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the optimzation frame - HPO\n",
    "import optuna\n",
    "# PPO algo for RL\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "# Bring in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import the sb3 monitor for logging \n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "# Import os to deal with filepaths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative version to use later to bypass factor 64 error\n",
    "\n",
    "# PPO optimization parameters\n",
    "PPO_PARAMS = {\n",
    "    # 1. Core Parameters\n",
    "    'n_steps_range': (2048, 4096),            \n",
    "    # How many steps to run before updating the policy\n",
    "    # - Larger values (4096): More stable learning, better for complex patterns\n",
    "    # - Smaller values (2048): Faster updates, but potentially less stable\n",
    "    # - Must be factor of 64 for efficiency\n",
    "    \n",
    "    'gamma_range': (0.95, 0.9999),             \n",
    "    # Discount factor for future rewards\n",
    "    # - Higher values (0.995): Values future rewards more (good for long-term strategy)\n",
    "    # - Lower values (0.95): More focus on immediate rewards\n",
    "    # - Street Fighter needs high gamma for combo learning\n",
    "    \n",
    "    'learning_rate_range': (5e-8, 1e-6),      \n",
    "    # How big steps to take when updating the policy\n",
    "    # - Higher values (5e-5): Faster learning but more unstable\n",
    "    # - Lower values (1e-5): More stable but slower learning\n",
    "    # - Lowered from original due to training instability\n",
    "    \n",
    "    'clip_range_range': (0.1, 0.3),           \n",
    "    # How much to limit policy updates\n",
    "    # - Higher values (0.3): Allows bigger policy changes\n",
    "    # - Lower values (0.1): More conservative updates\n",
    "    # - Centered around PPO standard of 0.2\n",
    "    \n",
    "    'gae_lambda_range': (0.9, 0.98),          \n",
    "    # Controls advantage estimation smoothing\n",
    "    # - Higher values (0.98): Better for long-term credit assignment\n",
    "    # - Lower values (0.9): More focus on immediate advantages\n",
    "    # - Important for linking moves into combos\n",
    "    \n",
    "    # 2. Advanced Parameters\n",
    "    'ent_coef_range': (1e-8, 1e-3),          \n",
    "    # Controls exploration vs exploitation\n",
    "    # - Higher values (1e-3): More random actions/exploration\n",
    "    # - Lower values (1e-8): More focused on best known moves\n",
    "    # - Critical for discovering new combat strategies\n",
    "    \n",
    "    'vf_coef_range': (0.5, 1.0),             \n",
    "    # Balances value function vs policy learning\n",
    "    # - Higher values (1.0): More emphasis on state value estimation\n",
    "    # - Lower values (0.5): More emphasis on action selection\n",
    "    # - Helps agent understand positioning and health advantages\n",
    "    \n",
    "    'n_epochs_range': (5, 15),                \n",
    "    # How many times to reuse each batch of data\n",
    "    # - Higher values (15): More thorough learning from each experience\n",
    "    # - Lower values (5): Less chance of overfitting to recent experiences\n",
    "    # - Balances learning efficiency vs stability\n",
    "    \n",
    "    'batch_size_range': (64, 256)             \n",
    "    # Size of chunks for processing training data\n",
    "    # - Larger sizes (256): More stable updates but slower\n",
    "    # - Smaller sizes (64): Faster but potentially noisier updates\n",
    "    # - Must be power of 2 for efficiency\n",
    "}\n",
    "\n",
    "A2C_PARAMS = {\n",
    "    # Your current parameters\n",
    "    'n_steps_range': (5, 30),                  # Small steps, more frequent updates\n",
    "    'gamma_range': (0.95, 0.9999),               # General discount range\n",
    "    'learning_rate_range': (5e-8, 1e-6),       # Higher learning rates typically better\n",
    "    'ent_coef_range': (1e-8, 1e-3),           # Entropy coefficient for exploration\n",
    "    'vf_coef_range': (0.5, 1.0),              # Value function coefficient   \n",
    "    'gae_lambda_range': (0.9, 0.98),           # Generalized Advantage Estimation lambda           \n",
    "}\n",
    "\n",
    "DQN_PARAMS = {\n",
    "    'buffer_size_range': (50000, 100000),      # Smaller buffer size for quicker access\n",
    "    'gamma_range': (0.9, 0.99),                # High gamma for long-term rewards\n",
    "    'learning_rate_range': (1e-5, 1e-4),       # Lower learning rate for stability\n",
    "    'batch_size_range': (32, 128),             # Standard DQN batch sizes\n",
    "    'train_freq_range': (4, 16),               # Frequent updates to maintain stability\n",
    "}\n",
    "\n",
    "# Define the optimization function for PPO\n",
    "def optimize_ppo(trial):\n",
    "    return {\n",
    "        # Dynamically Optimized Parameters\n",
    "        'n_steps': trial.suggest_categorical('n_steps', range(PPO_PARAMS['n_steps_range'][0], PPO_PARAMS['n_steps_range'][1], 64)),\n",
    "\n",
    "        'gamma': trial.suggest_loguniform('gamma', *PPO_PARAMS['gamma_range']),\n",
    "\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', *PPO_PARAMS['learning_rate_range']),\n",
    "\n",
    "        'clip_range': trial.suggest_uniform('clip_range', *PPO_PARAMS['clip_range_range']),\n",
    "\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', *PPO_PARAMS['gae_lambda_range']),\n",
    "\n",
    "        'ent_coef': trial.suggest_loguniform('ent_coef', *PPO_PARAMS['ent_coef_range']),\n",
    "\n",
    "        'vf_coef': trial.suggest_uniform('vf_coef', *PPO_PARAMS['vf_coef_range']),\n",
    "\n",
    "        'n_epochs': trial.suggest_int('n_epochs', *PPO_PARAMS['n_epochs_range']),\n",
    "\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "        \n",
    "        # Fixed Parameters (Stability Controls)\n",
    "        'max_grad_norm': 0.5,          # Prevents explosive gradients\n",
    "        'clip_range_vf': None,         # Uses same clipping as policy\n",
    "        'target_kl': None,             # No KL divergence target\n",
    "    }\n",
    "\n",
    "def optimize_a2c(trial):\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', *A2C_PARAMS['n_steps_range']),\n",
    "\n",
    "        'gamma': trial.suggest_loguniform('gamma', *A2C_PARAMS['gamma_range']),\n",
    "\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', *A2C_PARAMS['learning_rate_range']),\n",
    "\n",
    "        'ent_coef': trial.suggest_loguniform('ent_coef', *A2C_PARAMS['ent_coef_range']),\n",
    "\n",
    "        'vf_coef': trial.suggest_uniform('vf_coef', *A2C_PARAMS['vf_coef_range']),\n",
    "        \n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', *A2C_PARAMS['gae_lambda_range']),\n",
    "\n",
    "        # Fixed Parameters (Stability Controls)\n",
    "        'max_grad_norm': 0.5,          # Prevents explosive gradients\n",
    "    }\n",
    "\n",
    "def optimize_dqn(trial):\n",
    "    return {\n",
    "        'buffer_size': trial.suggest_int('buffer_size', *DQN_PARAMS['buffer_size_range']),\n",
    "        'gamma': trial.suggest_loguniform('gamma', *DQN_PARAMS['gamma_range']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', *DQN_PARAMS['learning_rate_range']),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', range(*DQN_PARAMS['batch_size_range'])),\n",
    "        'train_freq': trial.suggest_categorical('train_freq', range(*DQN_PARAMS['train_freq_range'])),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHMS = {\n",
    "    'PPO': (PPO, optimize_ppo),\n",
    "    'A2C': (A2C, optimize_a2c),\n",
    "    'DQN': (DQN, optimize_dqn),\n",
    "}\n",
    "\n",
    "def optimize_agent(trial, algo_name='PPO'):\n",
    "    try:\n",
    "        # Select algorithm and get hyperparameters\n",
    "        ModelClass, optimize_fn = ALGORITHMS[algo_name]\n",
    "        model_params = optimize_fn(trial)\n",
    "\n",
    "        # Create environment\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        # Initialize and train model\n",
    "        model = ModelClass('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params) # would recommend looking into other cnnpolicy's if they are compatible\n",
    "        model.learn(total_timesteps=100000)\n",
    "\n",
    "        # Evaluate model\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the experiment \n",
    "study = optuna.create_study(direction='maximize') # since mean reward is positive we maximize, otherwise minimize\n",
    "study.optimize(lambda trial: optimize_agent(trial, algo_name='DQN'), n_trials=25) # for prod used n_trials=100 (change algo name to change algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = A2C.load(os.path.join(OPT_DIR, 'trial_4_best_model.zip'))\n",
    "model = DQN.load(os.path.join(OPT_DIR, 'trial_1_best_model.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback \n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback): # continuously learn by starting from best parameters done above\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment \n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = study.best_params\n",
    "# model_params = hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params['n_steps'] = model_params['n_steps'] - (model_params['n_steps'] % 64)  # set n_steps to 7488 or a factor of 64 (ONLY NEEDED FOR PPO, CHECK YOUR STEPS AND CHANGE TO FAC OF 64)\n",
    "# model_params['learning_rate'] = 5e-7 -> if really slow at training\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params) # verbose 1 shows results as training\n",
    "# model = A2C('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params) # verbose 1 shows results as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload previous weights from HPO\n",
    "model.load(os.path.join(OPT_DIR, 'trial_6_best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off training \n",
    "model.learn(total_timesteps=5000000, callback=callback) # timestep 5000000 recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=. \n",
    "# cd to logs\n",
    "# ^ use to visually see learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load('./train/best_model_500000.zip')\n",
    "# model = A2C.load('./train/best_model_7000000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset game to starting state\n",
    "try:\n",
    "    obs = env.reset()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set flag to flase\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        print(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.01)\n",
    "        # print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
