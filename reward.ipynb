{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import retro to play Street Fighter using a ROM\n",
    "import retro\n",
    "# Import time to slow down game\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%python -m retro.import . # Run this from the roms folder, or where you have your game roms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class for a wrapper \n",
    "from gym import Env \n",
    "# Import the space shapes for the environment\n",
    "from gym.spaces import MultiBinary, Box, Discrete\n",
    "# Import numpy to calculate frame delta \n",
    "import numpy as np\n",
    "# Import opencv for grayscaling\n",
    "import cv2\n",
    "# Import matplotlib for plotting the image\n",
    "from matplotlib import pyplot as plt\n",
    "# Import deque for the frame stack\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude version 2\n",
    "\n",
    "class StreetFighter(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(12)\n",
    "        \n",
    "        # Initialize state tracking variables\n",
    "        self.health = 144\n",
    "        self.enemy_health = 144\n",
    "        self.score = 0\n",
    "        self.matches_won = 0\n",
    "        self.enemy_matches_won = 0\n",
    "        self.continue_timer = 100\n",
    "        \n",
    "        # Combo tracking variables\n",
    "        self.damage_window = deque(maxlen=30)  # Track damage over 30 frames\n",
    "        self.score_window = deque(maxlen=30)   # Track score changes over 30 frames\n",
    "        self.current_combo = 0\n",
    "        self.frames_since_last_hit = 0\n",
    "        \n",
    "        # Anti-spam tracking\n",
    "        self.action_history = deque(maxlen=60)  # Track last 60 frames of actions\n",
    "        self.last_hit_frame = 0                 # Track when we last dealt damage\n",
    "        self.whiff_counter = 0                  # Count actions without score/health changes\n",
    "        \n",
    "        # Constants for reward shaping\n",
    "        self.HEALTH_SCALE = 10.0\n",
    "        self.ROUND_WIN_BONUS = 100.0\n",
    "        self.MATCH_WIN_BONUS = 500.0\n",
    "        self.DAMAGE_TRADE_SCALE = 1.5\n",
    "        self.COMBO_SCALE = 2.0\n",
    "        self.MAX_COMBO_BONUS = 5.0\n",
    "        self.COMBO_TIMEOUT = 30\n",
    "        \n",
    "        # Spam prevention constants\n",
    "        self.WHIFF_PENALTY = -2.0           # Penalty for missing attacks\n",
    "        self.SPAM_THRESHOLD = 0.8           # Percentage of similar actions that triggers spam penalty\n",
    "        self.SPAM_PENALTY = -5.0            # Penalty for move spamming\n",
    "        \n",
    "        self.game = retro.make(\n",
    "            game='StreetFighterIISpecialChampionEdition-Genesis',\n",
    "            use_restricted_actions=retro.Actions.FILTERED\n",
    "        )\n",
    "        \n",
    "        # Initialize last state\n",
    "        self.last_state = {\n",
    "            'enemy_health': 144,\n",
    "            'health': 144,\n",
    "            'score': 0,\n",
    "            'matches_won': 0,\n",
    "            'enemy_matches_won': 0,\n",
    "            'continuetimer': 100\n",
    "        }\n",
    "\n",
    "    def calculate_action_diversity(self):\n",
    "        \"\"\"\n",
    "        Calculate how diverse the recent actions have been\n",
    "        Returns a penalty if actions are too repetitive\n",
    "        \"\"\"\n",
    "        if len(self.action_history) < 30:\n",
    "            return 0\n",
    "        \n",
    "        # Convert binary actions to move types for easier analysis\n",
    "        recent_moves = list(self.action_history)\n",
    "        move_counts = {}\n",
    "        \n",
    "        for move in recent_moves:\n",
    "            move_str = ''.join(map(str, move))\n",
    "            move_counts[move_str] = move_counts.get(move_str, 0) + 1\n",
    "        \n",
    "        # Calculate the ratio of the most common move\n",
    "        most_common_ratio = max(move_counts.values()) / len(recent_moves)\n",
    "        \n",
    "        # Apply penalty if the same move is being spammed\n",
    "        if most_common_ratio > self.SPAM_THRESHOLD:\n",
    "            return self.SPAM_PENALTY\n",
    "        return 0\n",
    "\n",
    "    def detect_combo(self, enemy_health_diff, score_diff):\n",
    "        \"\"\"\n",
    "        Detect combos based on damage and score changes\n",
    "        \"\"\"\n",
    "        # Update tracking windows\n",
    "        self.damage_window.append(enemy_health_diff)\n",
    "        self.score_window.append(score_diff)\n",
    "        \n",
    "        # If we dealt damage or got points this frame\n",
    "        if enemy_health_diff > 0 or score_diff > 0:\n",
    "            self.whiff_counter = 0  # Reset whiff counter on successful hit\n",
    "            if self.frames_since_last_hit < self.COMBO_TIMEOUT:\n",
    "                self.current_combo += 1\n",
    "            else:\n",
    "                self.current_combo = 1\n",
    "            self.frames_since_last_hit = 0\n",
    "            self.last_hit_frame = 0\n",
    "        else:\n",
    "            self.frames_since_last_hit += 1\n",
    "            \n",
    "            # If we're executing moves but not getting results\n",
    "            if any(self.action_history[-1] if self.action_history else [0]):\n",
    "                self.whiff_counter += 1\n",
    "        \n",
    "        # Reset combo if too much time has passed\n",
    "        if self.frames_since_last_hit >= self.COMBO_TIMEOUT:\n",
    "            self.current_combo = 0\n",
    "            \n",
    "        # Calculate combo multiplier\n",
    "        combo_multiplier = min(1.0 + (self.current_combo * 0.5), self.MAX_COMBO_BONUS)\n",
    "        \n",
    "        # Detect if this seems to be a \"true\" combo\n",
    "        recent_damage = sum(self.damage_window)\n",
    "        recent_score = sum(self.score_window)\n",
    "        \n",
    "        is_true_combo = (\n",
    "            self.current_combo > 1 and \n",
    "            (recent_damage > 10 or recent_score > 100)\n",
    "        )\n",
    "        \n",
    "        return is_true_combo, combo_multiplier\n",
    "\n",
    "    def reward_function(self, state):\n",
    "        reward = 0\n",
    "        \n",
    "        # Extract current state\n",
    "        enemy_health = state['enemy_health']\n",
    "        health = state['health']\n",
    "        score = state['score']\n",
    "        matches_won = state['matches_won']\n",
    "        enemy_matches_won = state['enemy_matches_won']\n",
    "        \n",
    "        # Calculate changes\n",
    "        score_diff = score - self.last_state['score']\n",
    "        enemy_health_diff = self.last_state['enemy_health'] - enemy_health\n",
    "        health_diff = self.last_state['health'] - health\n",
    "        \n",
    "        # Check if round is active (either player has health)\n",
    "        is_round_active = (enemy_health > 0 or health > 0)\n",
    "        \n",
    "        if is_round_active:\n",
    "            # Detect combo state\n",
    "            is_combo, combo_multiplier = self.detect_combo(enemy_health_diff, score_diff)\n",
    "            \n",
    "            # Reward for dealing damage, with combo scaling\n",
    "            if enemy_health_diff > 0:\n",
    "                base_damage_reward = enemy_health_diff * self.HEALTH_SCALE\n",
    "                if is_combo:\n",
    "                    reward += base_damage_reward * combo_multiplier\n",
    "                    reward += self.COMBO_SCALE * self.current_combo\n",
    "                else:\n",
    "                    reward += base_damage_reward\n",
    "                \n",
    "                # Extra reward for trading damage favorably\n",
    "                if health_diff > 0:\n",
    "                    if enemy_health_diff > health_diff:\n",
    "                        reward += (enemy_health_diff - health_diff) * self.DAMAGE_TRADE_SCALE\n",
    "            \n",
    "            # Apply anti-spam mechanics\n",
    "            spam_penalty = self.calculate_action_diversity()\n",
    "            whiff_penalty = self.WHIFF_PENALTY * min(self.whiff_counter, 5) if self.whiff_counter > 2 else 0\n",
    "            \n",
    "            reward += spam_penalty\n",
    "            reward += whiff_penalty\n",
    "            \n",
    "            # Penalty for taking damage\n",
    "            if health_diff > 0:\n",
    "                reward -= health_diff * self.HEALTH_SCALE\n",
    "                self.current_combo = 0\n",
    "                self.frames_since_last_hit = self.COMBO_TIMEOUT\n",
    "            \n",
    "            # Small reward for score increases\n",
    "            if score_diff > 0:\n",
    "                if is_combo:\n",
    "                    reward += score_diff * 0.2\n",
    "                else:\n",
    "                    reward += score_diff * 0.1\n",
    "        \n",
    "        # Round end rewards/penalties\n",
    "        if self.last_state['enemy_health'] > 0 and enemy_health == 0:  # Won the round\n",
    "            reward += self.ROUND_WIN_BONUS\n",
    "            if matches_won > self.last_state['matches_won']:  # Won the match\n",
    "                reward += self.MATCH_WIN_BONUS\n",
    "        \n",
    "        if self.last_state['health'] > 0 and health == 0:  # Lost the round\n",
    "            reward -= self.ROUND_WIN_BONUS / 2\n",
    "            if enemy_matches_won > self.last_state['enemy_matches_won']:  # Lost the match\n",
    "                reward -= self.MATCH_WIN_BONUS / 2\n",
    "        \n",
    "        # Update last state\n",
    "        self.last_state = {\n",
    "            'enemy_health': enemy_health,\n",
    "            'health': health,\n",
    "            'score': score,\n",
    "            'matches_won': matches_won,\n",
    "            'enemy_matches_won': enemy_matches_won,\n",
    "            'continuetimer': state['continuetimer']\n",
    "        }\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, _, done, info = self.game.step(action)\n",
    "        self.action_history.append(action)  # Track action for spam detection\n",
    "        obs = self.preprocess(obs)\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        reward = self.reward_function(info)\n",
    "        return frame_delta, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        # Reset state variables\n",
    "        self.last_state = {\n",
    "            'enemy_health': 144,\n",
    "            'health': 144,\n",
    "            'score': 0,\n",
    "            'matches_won': 0,\n",
    "            'enemy_matches_won': 0,\n",
    "            'continuetimer': 100\n",
    "        }\n",
    "        \n",
    "        # Reset tracking variables\n",
    "        self.damage_window.clear()\n",
    "        self.score_window.clear()\n",
    "        self.action_history.clear()\n",
    "        self.current_combo = 0\n",
    "        self.frames_since_last_hit = self.COMBO_TIMEOUT\n",
    "        self.whiff_counter = 0\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84, 84, 1))\n",
    "        return channels\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the optimzation frame - HPO\n",
    "import optuna\n",
    "# PPO algo for RL\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "# Bring in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import the sb3 monitor for logging \n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "# Import os to deal with filepaths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return test hyperparameters - define the object function\n",
    "def optimize_ppo(trial): \n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps', 2048, 8192), # number of frames used in one batch of training (must use a factor of 64) (maybe take a number and multiply it by 64? 😎🤝😈)\n",
    "        'gamma':trial.suggest_loguniform('gamma', 0.8, 0.9999), # discount rate\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate', 1e-5, 1e-4), # how fast we tune optimizer (Critic and Actor for PPO)\n",
    "        'clip_range':trial.suggest_uniform('clip_range', 0.1, 0.4), # how far we want to clip for our advantage value in PPO\n",
    "        'gae_lambda':trial.suggest_uniform('gae_lambda', 0.8, 0.99) # smoothing parameter (used when calculating advantage)\n",
    "    }\n",
    "\n",
    "# IF U WANT TO USE OTHER ALGOS THE HYPERPARAMS MUST BE SWITCHED AS WELL (DQN, SAC, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHMS = {\n",
    "    'PPO': (PPO, optimize_ppo),\n",
    "    # 'A2C': (A2C, optimize_a2c),\n",
    "    # 'DQN': (DQN, optimize_dqn),\n",
    "}\n",
    "\n",
    "def optimize_agent(trial, algo_name='PPO'):\n",
    "    try:\n",
    "        # Select algorithm and get hyperparameters\n",
    "        ModelClass, optimize_fn = ALGORITHMS[algo_name]\n",
    "        model_params = optimize_fn(trial)\n",
    "\n",
    "        # Create environment\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        # Initialize and train model\n",
    "        model = ModelClass('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params) # would recommend looking into other cnnpolicy's if they are compatible\n",
    "        model.learn(total_timesteps=100000)\n",
    "\n",
    "        # Evaluate model\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 00:44:46,421] A new study created in memory with name: no-name-b35fe3fa-c665-4884-9439-19ca9f1b3b72\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_45964\\3706438210.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'gamma':trial.suggest_loguniform('gamma', 0.8, 0.9999), # discount rate\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_45964\\3706438210.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate':trial.suggest_loguniform('learning_rate', 1e-5, 1e-4), # how fast we tune optimizer (Critic and Actor for PPO)\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_45964\\3706438210.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'clip_range':trial.suggest_uniform('clip_range', 0.1, 0.4), # how far we want to clip for our advantage value in PPO\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_45964\\3706438210.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'gae_lambda':trial.suggest_uniform('gae_lambda', 0.8, 0.99) # smoothing parameter (used when calculating advantage)\n",
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:137: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3560`, after every 55 untruncated mini-batches, there will be a truncated mini-batch of size 40\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3560 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 00:58:58,372] Trial 0 finished with value: -105164.0 and parameters: {'n_steps': 3560, 'gamma': 0.9041179200591378, 'learning_rate': 6.15978899282089e-05, 'clip_range': 0.19876558678820333, 'gae_lambda': 0.9758469364237689}. Best is trial 0 with value: -105164.0.\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_45964\\3706438210.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'gamma':trial.suggest_loguniform('gamma', 0.8, 0.9999), # discount rate\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_45964\\3706438210.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate':trial.suggest_loguniform('learning_rate', 1e-5, 1e-4), # how fast we tune optimizer (Critic and Actor for PPO)\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_45964\\3706438210.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'clip_range':trial.suggest_uniform('clip_range', 0.1, 0.4), # how far we want to clip for our advantage value in PPO\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_45964\\3706438210.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'gae_lambda':trial.suggest_uniform('gae_lambda', 0.8, 0.99) # smoothing parameter (used when calculating advantage)\n",
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:137: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3570`, after every 55 untruncated mini-batches, there will be a truncated mini-batch of size 50\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3570 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 01:14:37,465] Trial 1 finished with value: -147797.0 and parameters: {'n_steps': 3570, 'gamma': 0.8723848038224911, 'learning_rate': 1.783773943827151e-05, 'clip_range': 0.14610008809550556, 'gae_lambda': 0.9313345475308633}. Best is trial 0 with value: -105164.0.\n",
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:137: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3089`, after every 48 untruncated mini-batches, there will be a truncated mini-batch of size 17\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3089 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 01:26:40,829] Trial 2 finished with value: -44463.0 and parameters: {'n_steps': 3089, 'gamma': 0.8482634234145207, 'learning_rate': 7.056142209381235e-05, 'clip_range': 0.10576391391756874, 'gae_lambda': 0.8043934599923289}. Best is trial 2 with value: -44463.0.\n",
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:137: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2529`, after every 39 untruncated mini-batches, there will be a truncated mini-batch of size 33\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2529 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 01:39:47,586] Trial 3 finished with value: -78998.0 and parameters: {'n_steps': 2529, 'gamma': 0.8962768863798612, 'learning_rate': 1.9003375345458423e-05, 'clip_range': 0.12519734662193, 'gae_lambda': 0.8594319423070363}. Best is trial 2 with value: -44463.0.\n",
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:137: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2065`, after every 32 untruncated mini-batches, there will be a truncated mini-batch of size 17\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2065 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2024-11-13 01:53:17,101] Trial 4 finished with value: -71222.0 and parameters: {'n_steps': 2065, 'gamma': 0.8596334879612655, 'learning_rate': 4.0705400782914706e-05, 'clip_range': 0.22837433991919398, 'gae_lambda': 0.8262271911093723}. Best is trial 2 with value: -44463.0.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize') # since mean reward is positive we maximize, otherwise minimize\n",
    "study.optimize(lambda trial: optimize_agent(trial, algo_name='PPO'), n_trials=5) # for prod used n_trials=100 (change algo name to change algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:435: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  th_object = th.load(file_content, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(os.path.join(OPT_DIR, 'trial_2_best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback \n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback): # continuously learn by starting from best parameters done above\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment \n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_steps': 7488,\n",
       " 'gamma': 0.8482634234145207,\n",
       " 'learning_rate': 7.056142209381235e-05,\n",
       " 'clip_range': 0.10576391391756874,\n",
       " 'gae_lambda': 0.8043934599923289}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = study.best_params\n",
    "model_params['n_steps'] = 7488  # set n_steps to 7488 or a factor of 64\n",
    "# model_params['learning_rate'] = 5e-7 -> if really slow at training\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params) # verbose 1 shows results as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2728a67e430>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload previous weights from HPO\n",
    "model.load(os.path.join(OPT_DIR, 'trial_2_best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/PPO_1\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 5.29e+03  |\n",
      "|    ep_rew_mean     | -5.51e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 420       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 17        |\n",
      "|    total_timesteps | 7488      |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 4.91e+03  |\n",
      "|    ep_rew_mean          | -5.18e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 217       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 68        |\n",
      "|    total_timesteps      | 14976     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2107653 |\n",
      "|    clip_fraction        | 0.4       |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.891    |\n",
      "|    explained_variance   | 0.447     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 13.3      |\n",
      "|    n_updates            | 2260      |\n",
      "|    policy_gradient_loss | 0.0391    |\n",
      "|    value_loss           | 227       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.53e+03   |\n",
      "|    ep_rew_mean          | -6.79e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 119        |\n",
      "|    total_timesteps      | 22464      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22318211 |\n",
      "|    clip_fraction        | 0.435      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.885     |\n",
      "|    explained_variance   | 0.248      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 13.4       |\n",
      "|    n_updates            | 2270       |\n",
      "|    policy_gradient_loss | 0.0368     |\n",
      "|    value_loss           | 354        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.53e+03   |\n",
      "|    ep_rew_mean          | -6.79e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 170        |\n",
      "|    total_timesteps      | 29952      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39451903 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.8       |\n",
      "|    explained_variance   | 0.348      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 604        |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | 0.0384     |\n",
      "|    value_loss           | 375        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 7.24e+03  |\n",
      "|    ep_rew_mean          | -7.42e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 169       |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 220       |\n",
      "|    total_timesteps      | 37440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0294974 |\n",
      "|    clip_fraction        | 0.451     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.757    |\n",
      "|    explained_variance   | 0.187     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 833       |\n",
      "|    n_updates            | 2290      |\n",
      "|    policy_gradient_loss | 0.0667    |\n",
      "|    value_loss           | 529       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.16e+03   |\n",
      "|    ep_rew_mean          | -7.38e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 270        |\n",
      "|    total_timesteps      | 44928      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34486607 |\n",
      "|    clip_fraction        | 0.426      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.659     |\n",
      "|    explained_variance   | 0.336      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 21.5       |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | 0.0463     |\n",
      "|    value_loss           | 288        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.42e+03   |\n",
      "|    ep_rew_mean          | -7.63e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 320        |\n",
      "|    total_timesteps      | 52416      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30166617 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.755     |\n",
      "|    explained_variance   | 0.236      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 8.5        |\n",
      "|    n_updates            | 2310       |\n",
      "|    policy_gradient_loss | 0.0447     |\n",
      "|    value_loss           | 223        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.22e+03   |\n",
      "|    ep_rew_mean          | -7.43e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 372        |\n",
      "|    total_timesteps      | 59904      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.28898415 |\n",
      "|    clip_fraction        | 0.431      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.768     |\n",
      "|    explained_variance   | 0.27       |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 36.9       |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | 0.046      |\n",
      "|    value_loss           | 374        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.1e+03    |\n",
      "|    ep_rew_mean          | -7.31e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 422        |\n",
      "|    total_timesteps      | 67392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31539702 |\n",
      "|    clip_fraction        | 0.4        |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.867     |\n",
      "|    explained_variance   | 0.367      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 11.4       |\n",
      "|    n_updates            | 2330       |\n",
      "|    policy_gradient_loss | 0.0503     |\n",
      "|    value_loss           | 279        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.09e+03   |\n",
      "|    ep_rew_mean          | -7.29e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 473        |\n",
      "|    total_timesteps      | 74880      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31399393 |\n",
      "|    clip_fraction        | 0.44       |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.81      |\n",
      "|    explained_variance   | 0.402      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 6.08       |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | 0.0516     |\n",
      "|    value_loss           | 273        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.16e+03   |\n",
      "|    ep_rew_mean          | -7.35e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 156        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 526        |\n",
      "|    total_timesteps      | 82368      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54385173 |\n",
      "|    clip_fraction        | 0.463      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.806     |\n",
      "|    explained_variance   | 0.187      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 169        |\n",
      "|    n_updates            | 2350       |\n",
      "|    policy_gradient_loss | 0.0496     |\n",
      "|    value_loss           | 350        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.9e+03    |\n",
      "|    ep_rew_mean          | -7.09e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 155        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 579        |\n",
      "|    total_timesteps      | 89856      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48520213 |\n",
      "|    clip_fraction        | 0.457      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.783     |\n",
      "|    explained_variance   | 0.259      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 55         |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | 0.045      |\n",
      "|    value_loss           | 601        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.86e+03   |\n",
      "|    ep_rew_mean          | -7.05e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 630        |\n",
      "|    total_timesteps      | 97344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42416146 |\n",
      "|    clip_fraction        | 0.436      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.827     |\n",
      "|    explained_variance   | 0.374      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 38.9       |\n",
      "|    n_updates            | 2370       |\n",
      "|    policy_gradient_loss | 0.0475     |\n",
      "|    value_loss           | 285        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.76e+03  |\n",
      "|    ep_rew_mean          | -6.96e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 153       |\n",
      "|    iterations           | 14        |\n",
      "|    time_elapsed         | 681       |\n",
      "|    total_timesteps      | 104832    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.521999  |\n",
      "|    clip_fraction        | 0.426     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.723    |\n",
      "|    explained_variance   | 0.254     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 150       |\n",
      "|    n_updates            | 2380      |\n",
      "|    policy_gradient_loss | 0.0484    |\n",
      "|    value_loss           | 311       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.77e+03   |\n",
      "|    ep_rew_mean          | -6.97e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 153        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 732        |\n",
      "|    total_timesteps      | 112320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56541216 |\n",
      "|    clip_fraction        | 0.436      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.711     |\n",
      "|    explained_variance   | 0.286      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 54.6       |\n",
      "|    n_updates            | 2390       |\n",
      "|    policy_gradient_loss | 0.0552     |\n",
      "|    value_loss           | 377        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.62e+03  |\n",
      "|    ep_rew_mean          | -6.86e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 153       |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 782       |\n",
      "|    total_timesteps      | 119808    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5930522 |\n",
      "|    clip_fraction        | 0.411     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.712    |\n",
      "|    explained_variance   | 0.172     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 10.6      |\n",
      "|    n_updates            | 2400      |\n",
      "|    policy_gradient_loss | 0.0486    |\n",
      "|    value_loss           | 376       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.57e+03  |\n",
      "|    ep_rew_mean          | -6.82e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 152       |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 833       |\n",
      "|    total_timesteps      | 127296    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4226607 |\n",
      "|    clip_fraction        | 0.409     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.65     |\n",
      "|    explained_variance   | 0.388     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 31.4      |\n",
      "|    n_updates            | 2410      |\n",
      "|    policy_gradient_loss | 0.049     |\n",
      "|    value_loss           | 312       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.46e+03  |\n",
      "|    ep_rew_mean          | -6.72e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 152       |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 884       |\n",
      "|    total_timesteps      | 134784    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5485054 |\n",
      "|    clip_fraction        | 0.506     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.735    |\n",
      "|    explained_variance   | 0.275     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 20        |\n",
      "|    n_updates            | 2420      |\n",
      "|    policy_gradient_loss | 0.0664    |\n",
      "|    value_loss           | 347       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.46e+03  |\n",
      "|    ep_rew_mean          | -6.73e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 152       |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 935       |\n",
      "|    total_timesteps      | 142272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4710267 |\n",
      "|    clip_fraction        | 0.377     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.58     |\n",
      "|    explained_variance   | 0.338     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 74.1      |\n",
      "|    n_updates            | 2430      |\n",
      "|    policy_gradient_loss | 0.0522    |\n",
      "|    value_loss           | 262       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.49e+03   |\n",
      "|    ep_rew_mean          | -6.79e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 151        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 985        |\n",
      "|    total_timesteps      | 149760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30812132 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.582     |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 83.6       |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | 0.0544     |\n",
      "|    value_loss           | 355        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.44e+03   |\n",
      "|    ep_rew_mean          | -6.75e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 151        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 1035       |\n",
      "|    total_timesteps      | 157248     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.64770675 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.564     |\n",
      "|    explained_variance   | 0.467      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 72.9       |\n",
      "|    n_updates            | 2450       |\n",
      "|    policy_gradient_loss | 0.0402     |\n",
      "|    value_loss           | 229        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.43e+03   |\n",
      "|    ep_rew_mean          | -6.76e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 151        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 1087       |\n",
      "|    total_timesteps      | 164736     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47972184 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.56      |\n",
      "|    explained_variance   | 0.315      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 22.1       |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | 0.049      |\n",
      "|    value_loss           | 383        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.46e+03   |\n",
      "|    ep_rew_mean          | -6.81e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 151        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 1138       |\n",
      "|    total_timesteps      | 172224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46686277 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.521     |\n",
      "|    explained_variance   | 0.249      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 96.3       |\n",
      "|    n_updates            | 2470       |\n",
      "|    policy_gradient_loss | 0.0496     |\n",
      "|    value_loss           | 351        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.35e+03   |\n",
      "|    ep_rew_mean          | -6.74e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 150        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 1190       |\n",
      "|    total_timesteps      | 179712     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32308984 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.489     |\n",
      "|    explained_variance   | 0.302      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 24.8       |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | 0.0329     |\n",
      "|    value_loss           | 370        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.34e+03  |\n",
      "|    ep_rew_mean          | -6.76e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 150       |\n",
      "|    iterations           | 25        |\n",
      "|    time_elapsed         | 1242      |\n",
      "|    total_timesteps      | 187200    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4855674 |\n",
      "|    clip_fraction        | 0.284     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.459    |\n",
      "|    explained_variance   | 0.449     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 39.2      |\n",
      "|    n_updates            | 2490      |\n",
      "|    policy_gradient_loss | 0.0373    |\n",
      "|    value_loss           | 302       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.31e+03   |\n",
      "|    ep_rew_mean          | -6.75e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 150        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 1296       |\n",
      "|    total_timesteps      | 194688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42667204 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.408     |\n",
      "|    explained_variance   | 0.32       |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 266        |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | 0.0571     |\n",
      "|    value_loss           | 316        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.27e+03  |\n",
      "|    ep_rew_mean          | -6.76e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 149       |\n",
      "|    iterations           | 27        |\n",
      "|    time_elapsed         | 1348      |\n",
      "|    total_timesteps      | 202176    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3224914 |\n",
      "|    clip_fraction        | 0.257     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.448    |\n",
      "|    explained_variance   | 0.493     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 216       |\n",
      "|    n_updates            | 2510      |\n",
      "|    policy_gradient_loss | 0.0263    |\n",
      "|    value_loss           | 280       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.24e+03  |\n",
      "|    ep_rew_mean          | -6.75e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 149       |\n",
      "|    iterations           | 28        |\n",
      "|    time_elapsed         | 1401      |\n",
      "|    total_timesteps      | 209664    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3691502 |\n",
      "|    clip_fraction        | 0.268     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.457    |\n",
      "|    explained_variance   | 0.415     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 32        |\n",
      "|    n_updates            | 2520      |\n",
      "|    policy_gradient_loss | 0.0421    |\n",
      "|    value_loss           | 292       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.18e+03   |\n",
      "|    ep_rew_mean          | -6.71e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 1453       |\n",
      "|    total_timesteps      | 217152     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.76966953 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.445     |\n",
      "|    explained_variance   | 0.267      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 192        |\n",
      "|    n_updates            | 2530       |\n",
      "|    policy_gradient_loss | 0.0445     |\n",
      "|    value_loss           | 353        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.2e+03   |\n",
      "|    ep_rew_mean          | -6.74e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 149       |\n",
      "|    iterations           | 30        |\n",
      "|    time_elapsed         | 1505      |\n",
      "|    total_timesteps      | 224640    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3840799 |\n",
      "|    clip_fraction        | 0.268     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.403    |\n",
      "|    explained_variance   | 0.335     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 239       |\n",
      "|    n_updates            | 2540      |\n",
      "|    policy_gradient_loss | 0.0311    |\n",
      "|    value_loss           | 368       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.17e+03   |\n",
      "|    ep_rew_mean          | -6.72e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 1556       |\n",
      "|    total_timesteps      | 232128     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45610666 |\n",
      "|    clip_fraction        | 0.327      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.507     |\n",
      "|    explained_variance   | 0.326      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 33         |\n",
      "|    n_updates            | 2550       |\n",
      "|    policy_gradient_loss | 0.0441     |\n",
      "|    value_loss           | 319        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.11e+03   |\n",
      "|    ep_rew_mean          | -6.68e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 1606       |\n",
      "|    total_timesteps      | 239616     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38161027 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.508     |\n",
      "|    explained_variance   | 0.421      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 32.2       |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | 0.0457     |\n",
      "|    value_loss           | 279        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.09e+03  |\n",
      "|    ep_rew_mean          | -6.66e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 149       |\n",
      "|    iterations           | 33        |\n",
      "|    time_elapsed         | 1658      |\n",
      "|    total_timesteps      | 247104    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7530132 |\n",
      "|    clip_fraction        | 0.387     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.628    |\n",
      "|    explained_variance   | 0.373     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 46.9      |\n",
      "|    n_updates            | 2570      |\n",
      "|    policy_gradient_loss | 0.0489    |\n",
      "|    value_loss           | 372       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.05e+03  |\n",
      "|    ep_rew_mean          | -6.64e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 148       |\n",
      "|    iterations           | 34        |\n",
      "|    time_elapsed         | 1708      |\n",
      "|    total_timesteps      | 254592    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5511789 |\n",
      "|    clip_fraction        | 0.352     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.615    |\n",
      "|    explained_variance   | 0.242     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 27.3      |\n",
      "|    n_updates            | 2580      |\n",
      "|    policy_gradient_loss | 0.0428    |\n",
      "|    value_loss           | 366       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.05e+03   |\n",
      "|    ep_rew_mean          | -6.64e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 1758       |\n",
      "|    total_timesteps      | 262080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39862087 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.613     |\n",
      "|    explained_variance   | 0.373      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 51.5       |\n",
      "|    n_updates            | 2590       |\n",
      "|    policy_gradient_loss | 0.0524     |\n",
      "|    value_loss           | 363        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.09e+03  |\n",
      "|    ep_rew_mean          | -6.68e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 149       |\n",
      "|    iterations           | 36        |\n",
      "|    time_elapsed         | 1809      |\n",
      "|    total_timesteps      | 269568    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7006723 |\n",
      "|    clip_fraction        | 0.403     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.658    |\n",
      "|    explained_variance   | 0.212     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 587       |\n",
      "|    n_updates            | 2600      |\n",
      "|    policy_gradient_loss | 0.0654    |\n",
      "|    value_loss           | 366       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.05e+03   |\n",
      "|    ep_rew_mean          | -6.66e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 1859       |\n",
      "|    total_timesteps      | 277056     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48489654 |\n",
      "|    clip_fraction        | 0.388      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.646     |\n",
      "|    explained_variance   | 0.364      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 852        |\n",
      "|    n_updates            | 2610       |\n",
      "|    policy_gradient_loss | 0.057      |\n",
      "|    value_loss           | 359        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.04e+03   |\n",
      "|    ep_rew_mean          | -6.65e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 1911       |\n",
      "|    total_timesteps      | 284544     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41048354 |\n",
      "|    clip_fraction        | 0.389      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.676     |\n",
      "|    explained_variance   | 0.379      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 1.44e+03   |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | 0.0461     |\n",
      "|    value_loss           | 322        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.03e+03   |\n",
      "|    ep_rew_mean          | -6.65e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 1962       |\n",
      "|    total_timesteps      | 292032     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48407084 |\n",
      "|    clip_fraction        | 0.394      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.669     |\n",
      "|    explained_variance   | 0.439      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 444        |\n",
      "|    n_updates            | 2630       |\n",
      "|    policy_gradient_loss | 0.0448     |\n",
      "|    value_loss           | 239        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.02e+03   |\n",
      "|    ep_rew_mean          | -6.65e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 2014       |\n",
      "|    total_timesteps      | 299520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31505552 |\n",
      "|    clip_fraction        | 0.327      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.546     |\n",
      "|    explained_variance   | 0.423      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 54         |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | 0.0426     |\n",
      "|    value_loss           | 373        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6e+03     |\n",
      "|    ep_rew_mean          | -6.63e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 148       |\n",
      "|    iterations           | 41        |\n",
      "|    time_elapsed         | 2066      |\n",
      "|    total_timesteps      | 307008    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6189046 |\n",
      "|    clip_fraction        | 0.328     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.545    |\n",
      "|    explained_variance   | 0.341     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 16        |\n",
      "|    n_updates            | 2650      |\n",
      "|    policy_gradient_loss | 0.0453    |\n",
      "|    value_loss           | 303       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 5.97e+03  |\n",
      "|    ep_rew_mean          | -6.62e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 148       |\n",
      "|    iterations           | 42        |\n",
      "|    time_elapsed         | 2118      |\n",
      "|    total_timesteps      | 314496    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7457656 |\n",
      "|    clip_fraction        | 0.332     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.459    |\n",
      "|    explained_variance   | 0.298     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 466       |\n",
      "|    n_updates            | 2660      |\n",
      "|    policy_gradient_loss | 0.0568    |\n",
      "|    value_loss           | 589       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 5.97e+03  |\n",
      "|    ep_rew_mean          | -6.63e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 148       |\n",
      "|    iterations           | 43        |\n",
      "|    time_elapsed         | 2171      |\n",
      "|    total_timesteps      | 321984    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4576868 |\n",
      "|    clip_fraction        | 0.241     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.436    |\n",
      "|    explained_variance   | 0.351     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 37.2      |\n",
      "|    n_updates            | 2670      |\n",
      "|    policy_gradient_loss | 0.0448    |\n",
      "|    value_loss           | 335       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.97e+03   |\n",
      "|    ep_rew_mean          | -6.66e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 2222       |\n",
      "|    total_timesteps      | 329472     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41363364 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.451     |\n",
      "|    explained_variance   | 0.354      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 395        |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | 0.0356     |\n",
      "|    value_loss           | 252        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.98e+03   |\n",
      "|    ep_rew_mean          | -6.68e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 2273       |\n",
      "|    total_timesteps      | 336960     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22686525 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.475     |\n",
      "|    explained_variance   | 0.459      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 17         |\n",
      "|    n_updates            | 2690       |\n",
      "|    policy_gradient_loss | 0.0266     |\n",
      "|    value_loss           | 340        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.99e+03   |\n",
      "|    ep_rew_mean          | -6.69e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 2323       |\n",
      "|    total_timesteps      | 344448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29810756 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.408     |\n",
      "|    explained_variance   | 0.446      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 706        |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | 0.0338     |\n",
      "|    value_loss           | 281        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.98e+03   |\n",
      "|    ep_rew_mean          | -6.69e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 2372       |\n",
      "|    total_timesteps      | 351936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43615472 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.553     |\n",
      "|    explained_variance   | 0.247      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 26.8       |\n",
      "|    n_updates            | 2710       |\n",
      "|    policy_gradient_loss | 0.0346     |\n",
      "|    value_loss           | 579        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6e+03      |\n",
      "|    ep_rew_mean          | -6.72e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 2421       |\n",
      "|    total_timesteps      | 359424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54383665 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.592     |\n",
      "|    explained_variance   | 0.332      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 47.8       |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | 0.0687     |\n",
      "|    value_loss           | 356        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6e+03      |\n",
      "|    ep_rew_mean          | -6.71e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 2471       |\n",
      "|    total_timesteps      | 366912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44376367 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.602     |\n",
      "|    explained_variance   | 0.373      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 51.4       |\n",
      "|    n_updates            | 2730       |\n",
      "|    policy_gradient_loss | 0.0414     |\n",
      "|    value_loss           | 446        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6e+03      |\n",
      "|    ep_rew_mean          | -6.73e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 2520       |\n",
      "|    total_timesteps      | 374400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35445508 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.541     |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 556        |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | 0.0484     |\n",
      "|    value_loss           | 383        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.01e+03   |\n",
      "|    ep_rew_mean          | -6.73e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 2569       |\n",
      "|    total_timesteps      | 381888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32892585 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.548     |\n",
      "|    explained_variance   | 0.424      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 596        |\n",
      "|    n_updates            | 2750       |\n",
      "|    policy_gradient_loss | 0.0404     |\n",
      "|    value_loss           | 292        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6e+03      |\n",
      "|    ep_rew_mean          | -6.74e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 2619       |\n",
      "|    total_timesteps      | 389376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32202196 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.661     |\n",
      "|    explained_variance   | 0.354      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 15.6       |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | 0.0366     |\n",
      "|    value_loss           | 258        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6e+03      |\n",
      "|    ep_rew_mean          | -6.74e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 2667       |\n",
      "|    total_timesteps      | 396864     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26560324 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.595     |\n",
      "|    explained_variance   | 0.378      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 768        |\n",
      "|    n_updates            | 2770       |\n",
      "|    policy_gradient_loss | 0.0414     |\n",
      "|    value_loss           | 387        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.01e+03   |\n",
      "|    ep_rew_mean          | -6.75e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 2717       |\n",
      "|    total_timesteps      | 404352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34688044 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.561     |\n",
      "|    explained_variance   | 0.352      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 358        |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | 0.0423     |\n",
      "|    value_loss           | 359        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.99e+03   |\n",
      "|    ep_rew_mean          | -6.73e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 2766       |\n",
      "|    total_timesteps      | 411840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39753708 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.624     |\n",
      "|    explained_variance   | 0.332      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 179        |\n",
      "|    n_updates            | 2790       |\n",
      "|    policy_gradient_loss | 0.0591     |\n",
      "|    value_loss           | 361        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.04e+03  |\n",
      "|    ep_rew_mean          | -6.78e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 148       |\n",
      "|    iterations           | 56        |\n",
      "|    time_elapsed         | 2816      |\n",
      "|    total_timesteps      | 419328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4453669 |\n",
      "|    clip_fraction        | 0.325     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.666    |\n",
      "|    explained_variance   | 0.196     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 31.9      |\n",
      "|    n_updates            | 2800      |\n",
      "|    policy_gradient_loss | 0.056     |\n",
      "|    value_loss           | 540       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.09e+03  |\n",
      "|    ep_rew_mean          | -6.84e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 148       |\n",
      "|    iterations           | 57        |\n",
      "|    time_elapsed         | 2865      |\n",
      "|    total_timesteps      | 426816    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4746199 |\n",
      "|    clip_fraction        | 0.315     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.577    |\n",
      "|    explained_variance   | 0.384     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 33.5      |\n",
      "|    n_updates            | 2810      |\n",
      "|    policy_gradient_loss | 0.05      |\n",
      "|    value_loss           | 1.17e+03  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.08e+03  |\n",
      "|    ep_rew_mean          | -6.84e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 149       |\n",
      "|    iterations           | 58        |\n",
      "|    time_elapsed         | 2914      |\n",
      "|    total_timesteps      | 434304    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6414995 |\n",
      "|    clip_fraction        | 0.313     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.553    |\n",
      "|    explained_variance   | 0.273     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 41.6      |\n",
      "|    n_updates            | 2820      |\n",
      "|    policy_gradient_loss | 0.0551    |\n",
      "|    value_loss           | 451       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.07e+03  |\n",
      "|    ep_rew_mean          | -6.84e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 149       |\n",
      "|    iterations           | 59        |\n",
      "|    time_elapsed         | 2963      |\n",
      "|    total_timesteps      | 441792    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6044469 |\n",
      "|    clip_fraction        | 0.242     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.45     |\n",
      "|    explained_variance   | 0.379     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 37.5      |\n",
      "|    n_updates            | 2830      |\n",
      "|    policy_gradient_loss | 0.0306    |\n",
      "|    value_loss           | 332       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.04e+03  |\n",
      "|    ep_rew_mean          | -6.81e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 149       |\n",
      "|    iterations           | 60        |\n",
      "|    time_elapsed         | 3012      |\n",
      "|    total_timesteps      | 449280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6828595 |\n",
      "|    clip_fraction        | 0.285     |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.487    |\n",
      "|    explained_variance   | 0.307     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 25.8      |\n",
      "|    n_updates            | 2840      |\n",
      "|    policy_gradient_loss | 0.0437    |\n",
      "|    value_loss           | 368       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.04e+03   |\n",
      "|    ep_rew_mean          | -6.81e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 3062       |\n",
      "|    total_timesteps      | 456768     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61665505 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.456     |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 34         |\n",
      "|    n_updates            | 2850       |\n",
      "|    policy_gradient_loss | 0.0261     |\n",
      "|    value_loss           | 370        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.05e+03   |\n",
      "|    ep_rew_mean          | -6.82e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 3111       |\n",
      "|    total_timesteps      | 464256     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60604495 |\n",
      "|    clip_fraction        | 0.252      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.49      |\n",
      "|    explained_variance   | 0.288      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 853        |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | 0.0403     |\n",
      "|    value_loss           | 423        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.04e+03   |\n",
      "|    ep_rew_mean          | -6.82e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 63         |\n",
      "|    time_elapsed         | 3161       |\n",
      "|    total_timesteps      | 471744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58846724 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.522     |\n",
      "|    explained_variance   | 0.242      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 1.41e+03   |\n",
      "|    n_updates            | 2870       |\n",
      "|    policy_gradient_loss | 0.0396     |\n",
      "|    value_loss           | 527        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.04e+03   |\n",
      "|    ep_rew_mean          | -6.82e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 3210       |\n",
      "|    total_timesteps      | 479232     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71300066 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.628     |\n",
      "|    explained_variance   | 0.36       |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 20         |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | 0.0431     |\n",
      "|    value_loss           | 356        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.05e+03  |\n",
      "|    ep_rew_mean          | -6.83e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 149       |\n",
      "|    iterations           | 65        |\n",
      "|    time_elapsed         | 3259      |\n",
      "|    total_timesteps      | 486720    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6146802 |\n",
      "|    clip_fraction        | 0.3       |\n",
      "|    clip_range           | 0.106     |\n",
      "|    entropy_loss         | -0.617    |\n",
      "|    explained_variance   | 0.426     |\n",
      "|    learning_rate        | 7.06e-05  |\n",
      "|    loss                 | 34.3      |\n",
      "|    n_updates            | 2890      |\n",
      "|    policy_gradient_loss | 0.0462    |\n",
      "|    value_loss           | 314       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.04e+03   |\n",
      "|    ep_rew_mean          | -6.82e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 3309       |\n",
      "|    total_timesteps      | 494208     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48389885 |\n",
      "|    clip_fraction        | 0.339      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.717     |\n",
      "|    explained_variance   | 0.359      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 161        |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | 0.0564     |\n",
      "|    value_loss           | 407        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.03e+03   |\n",
      "|    ep_rew_mean          | -6.81e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 67         |\n",
      "|    time_elapsed         | 3358       |\n",
      "|    total_timesteps      | 501696     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30028233 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.106      |\n",
      "|    entropy_loss         | -0.752     |\n",
      "|    explained_variance   | 0.311      |\n",
      "|    learning_rate        | 7.06e-05   |\n",
      "|    loss                 | 18.6       |\n",
      "|    n_updates            | 2910       |\n",
      "|    policy_gradient_loss | 0.044      |\n",
      "|    value_loss           | 351        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x272e1dfe0a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kick off training \n",
    "model.learn(total_timesteps=500000, callback=callback) # timestep 5000000 recommended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=. \n",
    "# cd to logs\n",
    "# ^ use to visually see learning progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./train/best_model_570000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_reward, _ = evaluate_policy(model, env, render=True, n_eval_episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument 1: <class 'OverflowError'>: int too long to convert",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \n\u001b[0;32m      8\u001b[0m     obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 9\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m action \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(np\u001b[38;5;241m.\u001b[39marray(action))\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:281\u001b[0m, in \u001b[0;36mVecEnvWrapper.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:85\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03mGym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03mthey are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m:param mode: The rendering type.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\gym\\core.py:254\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 244\u001b[0m, in \u001b[0;36mStreetFighter.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\retro\\retro_env.py:230\u001b[0m, in \u001b[0;36mRetroEnv.render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic_control\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrendering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImageViewer\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m SimpleImageViewer()\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39misopen\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:417\u001b[0m, in \u001b[0;36mSimpleImageViewer.imshow\u001b[1;34m(self, arr)\u001b[0m\n\u001b[0;32m    415\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(scale \u001b[38;5;241m*\u001b[39m width)\n\u001b[0;32m    416\u001b[0m     height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(scale \u001b[38;5;241m*\u001b[39m height)\n\u001b[1;32m--> 417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m \u001b[43mget_window\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresizable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m width\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m=\u001b[39m height\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:72\u001b[0m, in \u001b[0;36mget_window\u001b[1;34m(width, height, display, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m screen[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_best_config()  \u001b[38;5;66;03m# selecting the first screen\u001b[39;00m\n\u001b[0;32m     70\u001b[0m context \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mcreate_context(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# create GL context\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyglet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWindow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py:136\u001b[0m, in \u001b[0;36mWin32Window.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_always_dwm \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetwindowsversion() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWin32Window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\pyglet\\window\\__init__.py:659\u001b[0m, in \u001b[0;36mBaseWindow.__init__\u001b[1;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, file_drops, display, screen, config, context, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m app\n\u001b[0;32m    658\u001b[0m app\u001b[38;5;241m.\u001b[39mwindows\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 659\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# Raise a warning if an OpenGL 2.0 context is not available. This is a common case\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# with virtual machines, or on Windows without fully supported GPU drivers.\u001b[39;00m\n\u001b[0;32m    663\u001b[0m gl_info \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mget_info()\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py:235\u001b[0m, in \u001b[0;36mWin32Window._create\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# View Hwnd is for the client area so certain events (mouse events) don't trigger outside of area.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_view_hwnd \u001b[38;5;241m=\u001b[39m _user32\u001b[38;5;241m.\u001b[39mCreateWindowExW(\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_view_window_class\u001b[38;5;241m.\u001b[39mlpszClassName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_view_window_class\u001b[38;5;241m.\u001b[39mhInstance,\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dc \u001b[38;5;241m=\u001b[39m \u001b[43m_user32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetDC\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_view_hwnd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Only allow files being dropped if specified.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_drops:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# Allows UAC to not block the drop files request if low permissions. All 3 must be set.\u001b[39;00m\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument 1: <class 'OverflowError'>: int too long to convert"
     ]
    }
   ],
   "source": [
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "# Set flag to flase\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(np.array(action))\n",
    "        # time.sleep(0.01)\n",
    "        print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
