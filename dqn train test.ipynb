{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import retro to play Street Fighter using a ROM\n",
    "import retro\n",
    "# Import time to slow down game\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m retro.import . # Run this from the roms folder, or where you have your game roms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class for a wrapper \n",
    "from gym import Env \n",
    "# Import the space shapes for the environment\n",
    "from gym.spaces import MultiBinary, Discrete, Box\n",
    "# Import numpy to calculate frame delta \n",
    "import numpy as np\n",
    "# Import opencv for grayscaling\n",
    "import cv2\n",
    "# Import matplotlib for plotting the image\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, A, _, _, UP, DOWN, LEFT, RIGHT, C, Y, X, Z\n",
    "# Every possible action for a given step in Street Fighter II\n",
    "possible_actions = {\n",
    "    # Idle\n",
    "    0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Left\n",
    "    1 : [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    # Right\n",
    "    2 : [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    # Up\n",
    "    3 : [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Down\n",
    "    4 : [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    # Light Kick\n",
    "    5 : [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Medium Kick\n",
    "    6 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Hard Kick\n",
    "    7 : [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    # Light Punch\n",
    "    8 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    # Medium Punch\n",
    "    9 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    # Hard Punch\n",
    "    10 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    # Down Left\n",
    "    11 : [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "    # Down Right\n",
    "    12 : [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
    "    # Up Left\n",
    "    13 : [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    # Up Right\n",
    "    14 : [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class based on github\n",
    "class StreetFighter(Env): # pass in basic env from above to preprocessing\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit from base env\n",
    "        # Specify action space and observation space \n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(84, 84, 3),  # Keep 3 channels for RGB\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.action_space = Discrete(12) # type of actions that can be taken\n",
    "        self.health = 144\n",
    "        self.enemy_health = 144\n",
    "        self.score = 0\n",
    "        self.matches_won = 0\n",
    "        self.continue_timer = 100\n",
    "        self.enemy_matches_won = 0\n",
    "        # self.previous_action = np.zeros(12)\n",
    "        # self.combo_scaler = 1\n",
    "        # self.last_damage_instance = 0\n",
    "        # Startup and instance of the game \n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.DISCRETE) # used to get valid button combos\n",
    "    \n",
    "    def reset(self): # restart\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs # sets previous frame to current frame\n",
    "        \n",
    "        # Create a attribute to hold the score delta \n",
    "        self.score = 0 \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        # Resize first to reduce computation\n",
    "        resized = cv2.resize(observation, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Simple color quantization using bitwise operations\n",
    "        # Reduce to 3 bits per channel (8 values per channel)\n",
    "        quantized = resized & 0b11100000\n",
    "        \n",
    "        # Optional: Create more distinct colors by increasing contrast\n",
    "        # This helps make different elements more distinguishable\n",
    "        quantized = cv2.convertScaleAbs(quantized, alpha=1.2, beta=10)\n",
    "        \n",
    "        # Method 1: Simple bitwise quantization\n",
    "        return quantized\n",
    "    \n",
    "    def reward_function(self, state):\n",
    "        # Extract variables\n",
    "        continuetimer = state['continuetimer']\n",
    "        enemy_matches_won = state['enemy_matches_won']\n",
    "        enemy_health = state['enemy_health']\n",
    "        health = state['health']\n",
    "        matches_won = state['matches_won']\n",
    "        score = state['score']\n",
    "\n",
    "        # Initialize reward\n",
    "        reward = 0\n",
    "\n",
    "        # Reward for increasing score each frame (scaled down to avoid excessively large rewards)\n",
    "        reward += score * 0.001  \n",
    "\n",
    "        enemy_health_diff = self.enemy_health - enemy_health\n",
    "        health_diff = self.health - health\n",
    "\n",
    "        # catching edge cases to make sure no reward is being earned outside of a fight (i.e. in between rounds)\n",
    "        if (self.enemy_health != 0 and state['enemy_health'] == 0 and self.health != 0 and state['health'] == 0) or (enemy_health_diff == 0 and health_diff == 0) or (self.health == 0 and self.enemy_health == 0):\n",
    "            reward += 0\n",
    "        else:\n",
    "            if enemy_health_diff > health_diff:\n",
    "                reward += ((enemy_health_diff) - (health_diff)) * 10\n",
    "            else:\n",
    "                reward += ((enemy_health_diff) - (health_diff))\n",
    "\n",
    "        # Update previous states to enable frame-by-frame comparison\n",
    "        self.enemy_health = enemy_health\n",
    "        self.health = health\n",
    "        self.matches_won = matches_won\n",
    "        self.enemy_matches_won = enemy_matches_won\n",
    "        self.continue_timer = continuetimer\n",
    "        self.score = score\n",
    "        # self.last_damage_instance += 1\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def step(self, action): # how do we process action\n",
    "        # Take a step \n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs) \n",
    "\n",
    "        self.previous_action = action\n",
    "        \n",
    "        # Frame delta \n",
    "        frame_delta = obs\n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Reshape the reward function\n",
    "        reward = self.reward_function(info)\n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs): # unpack any args and kwargs from stable baseline\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the optimzation frame - HPO\n",
    "import optuna\n",
    "# PPO algo for RL\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "# Bring in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import the sb3 monitor for logging \n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "# Import os to deal with filepaths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative version to use later to bypass factor 64 error\n",
    "\n",
    "# PPO optimization parameters\n",
    "PPO_PARAMS = {\n",
    "    # 1. Core Parameters\n",
    "    'n_steps_range': (2048, 4096),            \n",
    "    # How many steps to run before updating the policy\n",
    "    # - Larger values (4096): More stable learning, better for complex patterns\n",
    "    # - Smaller values (2048): Faster updates, but potentially less stable\n",
    "    # - Must be factor of 64 for efficiency\n",
    "    \n",
    "    'gamma_range': (0.95, 0.9999),             \n",
    "    # Discount factor for future rewards\n",
    "    # - Higher values (0.995): Values future rewards more (good for long-term strategy)\n",
    "    # - Lower values (0.95): More focus on immediate rewards\n",
    "    # - Street Fighter needs high gamma for combo learning\n",
    "    \n",
    "    'learning_rate_range': (5e-8, 1e-6),      \n",
    "    # How big steps to take when updating the policy\n",
    "    # - Higher values (5e-5): Faster learning but more unstable\n",
    "    # - Lower values (1e-5): More stable but slower learning\n",
    "    # - Lowered from original due to training instability\n",
    "    \n",
    "    'clip_range_range': (0.1, 0.3),           \n",
    "    # How much to limit policy updates\n",
    "    # - Higher values (0.3): Allows bigger policy changes\n",
    "    # - Lower values (0.1): More conservative updates\n",
    "    # - Centered around PPO standard of 0.2\n",
    "    \n",
    "    'gae_lambda_range': (0.9, 0.98),          \n",
    "    # Controls advantage estimation smoothing\n",
    "    # - Higher values (0.98): Better for long-term credit assignment\n",
    "    # - Lower values (0.9): More focus on immediate advantages\n",
    "    # - Important for linking moves into combos\n",
    "    \n",
    "    # 2. Advanced Parameters\n",
    "    'ent_coef_range': (1e-8, 1e-3),          \n",
    "    # Controls exploration vs exploitation\n",
    "    # - Higher values (1e-3): More random actions/exploration\n",
    "    # - Lower values (1e-8): More focused on best known moves\n",
    "    # - Critical for discovering new combat strategies\n",
    "    \n",
    "    'vf_coef_range': (0.5, 1.0),             \n",
    "    # Balances value function vs policy learning\n",
    "    # - Higher values (1.0): More emphasis on state value estimation\n",
    "    # - Lower values (0.5): More emphasis on action selection\n",
    "    # - Helps agent understand positioning and health advantages\n",
    "    \n",
    "    'n_epochs_range': (5, 15),                \n",
    "    # How many times to reuse each batch of data\n",
    "    # - Higher values (15): More thorough learning from each experience\n",
    "    # - Lower values (5): Less chance of overfitting to recent experiences\n",
    "    # - Balances learning efficiency vs stability\n",
    "    \n",
    "    'batch_size_range': (64, 256)             \n",
    "    # Size of chunks for processing training data\n",
    "    # - Larger sizes (256): More stable updates but slower\n",
    "    # - Smaller sizes (64): Faster but potentially noisier updates\n",
    "    # - Must be power of 2 for efficiency\n",
    "}\n",
    "\n",
    "A2C_PARAMS = {\n",
    "    # Your current parameters\n",
    "    'n_steps_range': (5, 30),                  # Small steps, more frequent updates\n",
    "    'gamma_range': (0.95, 0.9999),               # General discount range\n",
    "    'learning_rate_range': (5e-8, 1e-6),       # Higher learning rates typically better\n",
    "    'ent_coef_range': (1e-8, 1e-3),           # Entropy coefficient for exploration\n",
    "    'vf_coef_range': (0.5, 1.0),              # Value function coefficient   \n",
    "    'gae_lambda_range': (0.9, 0.98),           # Generalized Advantage Estimation lambda           \n",
    "}\n",
    "\n",
    "DQN_PARAMS = {\n",
    "    'buffer_size_range': (50000, 100000),      # Smaller buffer size for quicker access\n",
    "    'gamma_range': (0.9, 0.99),                # High gamma for long-term rewards\n",
    "    'learning_rate_range': (1e-5, 1e-4),       # Lower learning rate for stability\n",
    "    'batch_size_range': (32, 128),             # Standard DQN batch sizes\n",
    "    'train_freq_range': (4, 16),               # Frequent updates to maintain stability\n",
    "}\n",
    "\n",
    "# Define the optimization function for PPO\n",
    "def optimize_ppo(trial):\n",
    "    return {\n",
    "        # Dynamically Optimized Parameters\n",
    "        'n_steps': trial.suggest_categorical('n_steps', range(PPO_PARAMS['n_steps_range'][0], PPO_PARAMS['n_steps_range'][1], 64)),\n",
    "\n",
    "        'gamma': trial.suggest_loguniform('gamma', *PPO_PARAMS['gamma_range']),\n",
    "\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', *PPO_PARAMS['learning_rate_range']),\n",
    "\n",
    "        'clip_range': trial.suggest_uniform('clip_range', *PPO_PARAMS['clip_range_range']),\n",
    "\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', *PPO_PARAMS['gae_lambda_range']),\n",
    "\n",
    "        'ent_coef': trial.suggest_loguniform('ent_coef', *PPO_PARAMS['ent_coef_range']),\n",
    "\n",
    "        'vf_coef': trial.suggest_uniform('vf_coef', *PPO_PARAMS['vf_coef_range']),\n",
    "\n",
    "        'n_epochs': trial.suggest_int('n_epochs', *PPO_PARAMS['n_epochs_range']),\n",
    "\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "        \n",
    "        # Fixed Parameters (Stability Controls)\n",
    "        'max_grad_norm': 0.5,          # Prevents explosive gradients\n",
    "        'clip_range_vf': None,         # Uses same clipping as policy\n",
    "        'target_kl': None,             # No KL divergence target\n",
    "    }\n",
    "\n",
    "def optimize_a2c(trial):\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', *A2C_PARAMS['n_steps_range']),\n",
    "\n",
    "        'gamma': trial.suggest_loguniform('gamma', *A2C_PARAMS['gamma_range']),\n",
    "\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', *A2C_PARAMS['learning_rate_range']),\n",
    "\n",
    "        'ent_coef': trial.suggest_loguniform('ent_coef', *A2C_PARAMS['ent_coef_range']),\n",
    "\n",
    "        'vf_coef': trial.suggest_uniform('vf_coef', *A2C_PARAMS['vf_coef_range']),\n",
    "        \n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', *A2C_PARAMS['gae_lambda_range']),\n",
    "\n",
    "        # Fixed Parameters (Stability Controls)\n",
    "        'max_grad_norm': 0.5,          # Prevents explosive gradients\n",
    "    }\n",
    "\n",
    "def optimize_dqn(trial):\n",
    "    return {\n",
    "        'buffer_size': trial.suggest_int('buffer_size', *DQN_PARAMS['buffer_size_range']),\n",
    "        'gamma': trial.suggest_loguniform('gamma', *DQN_PARAMS['gamma_range']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', *DQN_PARAMS['learning_rate_range']),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', range(*DQN_PARAMS['batch_size_range'])),\n",
    "        'train_freq': trial.suggest_categorical('train_freq', range(*DQN_PARAMS['train_freq_range'])),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHMS = {\n",
    "    'PPO': (PPO, optimize_ppo),\n",
    "    'A2C': (A2C, optimize_a2c),\n",
    "    'DQN': (DQN, optimize_dqn),\n",
    "}\n",
    "\n",
    "def optimize_agent(trial, algo_name='PPO'):\n",
    "    try:\n",
    "        # Select algorithm and get hyperparameters\n",
    "        ModelClass, optimize_fn = ALGORITHMS[algo_name]\n",
    "        model_params = optimize_fn(trial)\n",
    "\n",
    "        # Create environment\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        # Initialize and train model\n",
    "        model = ModelClass('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params) # would recommend looking into other cnnpolicy's if they are compatible\n",
    "        model.learn(total_timesteps=100000)\n",
    "\n",
    "        # Evaluate model\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:21:53,743] A new study created in memory with name: no-name-9674381e-9d79-42d6-b593-ef3525b1e988\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_7088\\2333909226.py:129: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'gamma': trial.suggest_loguniform('gamma', *DQN_PARAMS['gamma_range']),\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_7088\\2333909226.py:130: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', *DQN_PARAMS['learning_rate_range']),\n",
      "[I 2024-11-10 19:39:12,447] Trial 0 finished with value: 8490.099999999999 and parameters: {'buffer_size': 53292, 'gamma': 0.9387479312094781, 'learning_rate': 1.9629879304410023e-05, 'batch_size': 112, 'train_freq': 7}. Best is trial 0 with value: 8490.099999999999.\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_7088\\2333909226.py:129: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'gamma': trial.suggest_loguniform('gamma', *DQN_PARAMS['gamma_range']),\n",
      "C:\\Users\\suyog\\AppData\\Local\\Temp\\ipykernel_7088\\2333909226.py:130: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', *DQN_PARAMS['learning_rate_range']),\n",
      "[I 2024-11-10 19:55:49,063] Trial 1 finished with value: 3012.5 and parameters: {'buffer_size': 97357, 'gamma': 0.9168646825796313, 'learning_rate': 2.1138786372168886e-05, 'batch_size': 122, 'train_freq': 8}. Best is trial 0 with value: 8490.099999999999.\n",
      "[I 2024-11-10 20:08:47,893] Trial 2 finished with value: 11743.399999999998 and parameters: {'buffer_size': 73030, 'gamma': 0.9637571537874948, 'learning_rate': 5.268402595028873e-05, 'batch_size': 101, 'train_freq': 11}. Best is trial 2 with value: 11743.399999999998.\n",
      "[I 2024-11-10 20:18:32,830] Trial 3 finished with value: 4514.0 and parameters: {'buffer_size': 51607, 'gamma': 0.9394297700138379, 'learning_rate': 4.1994307627873816e-05, 'batch_size': 56, 'train_freq': 12}. Best is trial 2 with value: 11743.399999999998.\n",
      "[I 2024-11-10 20:34:53,220] Trial 4 finished with value: 11616.0 and parameters: {'buffer_size': 69566, 'gamma': 0.9024090231207107, 'learning_rate': 8.12802824141528e-05, 'batch_size': 49, 'train_freq': 4}. Best is trial 2 with value: 11743.399999999998.\n",
      "[I 2024-11-10 20:48:42,720] Trial 5 finished with value: 11327.3 and parameters: {'buffer_size': 89712, 'gamma': 0.9369823582480903, 'learning_rate': 3.530576490113367e-05, 'batch_size': 82, 'train_freq': 8}. Best is trial 2 with value: 11743.399999999998.\n",
      "[I 2024-11-10 21:00:36,898] Trial 6 finished with value: 10226.0 and parameters: {'buffer_size': 75074, 'gamma': 0.9478733085811232, 'learning_rate': 5.975645523665806e-05, 'batch_size': 95, 'train_freq': 12}. Best is trial 2 with value: 11743.399999999998.\n",
      "[I 2024-11-10 21:19:28,181] Trial 7 finished with value: 7414.2 and parameters: {'buffer_size': 59606, 'gamma': 0.9254703611625014, 'learning_rate': 5.388940036212512e-05, 'batch_size': 88, 'train_freq': 5}. Best is trial 2 with value: 11743.399999999998.\n",
      "[I 2024-11-10 21:35:16,628] Trial 8 finished with value: 7476.9 and parameters: {'buffer_size': 78078, 'gamma': 0.9195785600268485, 'learning_rate': 6.651985280937651e-05, 'batch_size': 81, 'train_freq': 6}. Best is trial 2 with value: 11743.399999999998.\n",
      "[I 2024-11-10 21:45:59,299] Trial 9 finished with value: 8963.3 and parameters: {'buffer_size': 84901, 'gamma': 0.9857455734758308, 'learning_rate': 8.542120404811644e-05, 'batch_size': 76, 'train_freq': 11}. Best is trial 2 with value: 11743.399999999998.\n",
      "[I 2024-11-10 21:56:09,447] Trial 10 finished with value: 10258.2 and parameters: {'buffer_size': 66016, 'gamma': 0.9717899921714906, 'learning_rate': 1.2379821522558654e-05, 'batch_size': 68, 'train_freq': 11}. Best is trial 2 with value: 11743.399999999998.\n",
      "[I 2024-11-10 22:08:53,542] Trial 11 finished with value: 8817.0 and parameters: {'buffer_size': 68773, 'gamma': 0.9025198480328618, 'learning_rate': 9.517625654851451e-05, 'batch_size': 33, 'train_freq': 4}. Best is trial 2 with value: 11743.399999999998.\n",
      "[I 2024-11-10 22:24:08,879] Trial 12 finished with value: 9912.5 and parameters: {'buffer_size': 66352, 'gamma': 0.9619707020553424, 'learning_rate': 4.651869686480483e-05, 'batch_size': 49, 'train_freq': 4}. Best is trial 2 with value: 11743.399999999998.\n"
     ]
    }
   ],
   "source": [
    "# Creating the experiment \n",
    "study = optuna.create_study(direction='maximize') # since mean reward is positive we maximize, otherwise minimize\n",
    "study.optimize(lambda trial: optimize_agent(trial, algo_name='DQN'), n_trials=25) # for prod used n_trials=100 (change algo name to change algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'buffer_size': 51607, 'gamma': 0.9394297700138379, 'learning_rate': 4.1994307627873816e-05, 'batch_size': 56, 'train_freq': 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:435: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  th_object = th.load(file_content, map_location=device)\n",
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:219: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 12.37GB > 7.23GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model = A2C.load(os.path.join(OPT_DIR, 'trial_4_best_model.zip'))\n",
    "model = DQN.load(os.path.join(OPT_DIR, 'trial_2_best_model.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback \n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback): # continuously learn by starting from best parameters done above\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment \n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = best_params\n",
    "# model_params = hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'buffer_size': 51607,\n",
       " 'gamma': 0.9394297700138379,\n",
       " 'learning_rate': 4.1994307627873816e-05,\n",
       " 'batch_size': 56,\n",
       " 'train_freq': 12}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_params['n_steps'] = model_params['n_steps'] - (model_params['n_steps'] % 64)  # set n_steps to 7488 or a factor of 64 (ONLY NEEDED FOR PPO, CHECK YOUR STEPS AND CHANGE TO FAC OF 64)\n",
    "# model_params['learning_rate'] = 5e-7 -> if really slow at training\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:219: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 8.74GB > 7.14GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params) # verbose 1 shows results as training\n",
    "# model = A2C('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params) # verbose 1 shows results as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:435: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  th_object = th.load(file_content, map_location=device)\n",
      "d:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:219: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 12.37GB > 7.05GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x24b9972c6a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload previous weights from HPO\n",
    "model.load(os.path.join(OPT_DIR, 'trial_2_best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/DQN_16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.06e+04 |\n",
      "|    ep_rew_mean      | 2.5e+05  |\n",
      "|    exploration_rate | 0.92     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 714      |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 42291    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.67e+03 |\n",
      "|    ep_rew_mean      | 1.95e+05 |\n",
      "|    exploration_rate | 0.853    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 252      |\n",
      "|    total_timesteps  | 77351    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 12.7     |\n",
      "|    n_updates        | 2279     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.43e+03 |\n",
      "|    ep_rew_mean      | 1.35e+05 |\n",
      "|    exploration_rate | 0.808    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 237      |\n",
      "|    time_elapsed     | 425      |\n",
      "|    total_timesteps  | 101203   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 5.88     |\n",
      "|    n_updates        | 4267     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.44e+03 |\n",
      "|    ep_rew_mean      | 1.33e+05 |\n",
      "|    exploration_rate | 0.743    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 187      |\n",
      "|    time_elapsed     | 718      |\n",
      "|    total_timesteps  | 135056   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 5.55     |\n",
      "|    n_updates        | 7088     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.44e+03 |\n",
      "|    ep_rew_mean      | 1.23e+05 |\n",
      "|    exploration_rate | 0.679    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 1032     |\n",
      "|    total_timesteps  | 168796   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 3.85     |\n",
      "|    n_updates        | 9900     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.29e+03 |\n",
      "|    ep_rew_mean      | 1.14e+05 |\n",
      "|    exploration_rate | 0.622    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 1216     |\n",
      "|    total_timesteps  | 198916   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 1.9      |\n",
      "|    n_updates        | 12410    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.15e+03 |\n",
      "|    ep_rew_mean      | 1.04e+05 |\n",
      "|    exploration_rate | 0.566    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 161      |\n",
      "|    time_elapsed     | 1409     |\n",
      "|    total_timesteps  | 228184   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 0.883    |\n",
      "|    n_updates        | 14849    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.98e+03 |\n",
      "|    ep_rew_mean      | 9.47e+04 |\n",
      "|    exploration_rate | 0.515    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 156      |\n",
      "|    time_elapsed     | 1633     |\n",
      "|    total_timesteps  | 255307   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 1.79     |\n",
      "|    n_updates        | 17109    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.03e+03 |\n",
      "|    ep_rew_mean      | 8.87e+04 |\n",
      "|    exploration_rate | 0.451    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 152      |\n",
      "|    time_elapsed     | 1892     |\n",
      "|    total_timesteps  | 289209   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 19934    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.03e+03 |\n",
      "|    ep_rew_mean      | 8.43e+04 |\n",
      "|    exploration_rate | 0.39     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 151      |\n",
      "|    time_elapsed     | 2114     |\n",
      "|    total_timesteps  | 321187   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 1.44     |\n",
      "|    n_updates        | 22599    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8e+03    |\n",
      "|    ep_rew_mean      | 8.01e+04 |\n",
      "|    exploration_rate | 0.331    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 148      |\n",
      "|    time_elapsed     | 2370     |\n",
      "|    total_timesteps  | 351949   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 0.947    |\n",
      "|    n_updates        | 25163    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.88e+03 |\n",
      "|    ep_rew_mean      | 7.44e+04 |\n",
      "|    exploration_rate | 0.282    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 147      |\n",
      "|    time_elapsed     | 2570     |\n",
      "|    total_timesteps  | 378034   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.2e-05  |\n",
      "|    loss             | 0.513    |\n",
      "|    n_updates        | 27336    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Kick off training \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# timestep 5000000 recommended\u001b[39;00m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:241\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    230\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m     reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OffPolicyAlgorithm:\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDQN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:371\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 371\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:176\u001b[0m, in \u001b[0;36mDQN.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    173\u001b[0m     target_q_values \u001b[38;5;241m=\u001b[39m replay_data\u001b[38;5;241m.\u001b[39mrewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m replay_data\u001b[38;5;241m.\u001b[39mdones) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_q_values\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Get current Q-values estimates\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m current_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# Retrieve the q-values for the actions from the replay buffer\u001b[39;00m\n\u001b[0;32m    179\u001b[0m current_q_values \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mgather(current_q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, index\u001b[38;5;241m=\u001b[39mreplay_data\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mlong())\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\dqn\\policies.py:66\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    Predict the q-values.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    :param obs: Observation\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    :return: The estimated Q-Value for each action.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\policies.py:129\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo features extractor was set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, normalize_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_images)\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:93\u001b[0m, in \u001b[0;36mNatureCNN.forward\u001b[1;34m(self, observations)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observations: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\StreetFighterAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Kick off training \n",
    "model.learn(total_timesteps=5000000, callback=callback) # timestep 5000000 recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=. \n",
    "# cd to logs\n",
    "# ^ use to visually see learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load('./train/best_model_500000.zip')\n",
    "# model = A2C.load('./train/best_model_7000000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset game to starting state\n",
    "try:\n",
    "    obs = env.reset()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set flag to flase\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        print(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.01)\n",
    "        # print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
