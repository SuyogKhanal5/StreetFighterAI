{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip==23.2\n",
      "  Using cached pip-23.2-py3-none-any.whl.metadata (4.2 kB)\n",
      "Using cached pip-23.2-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.3.1\n",
      "    Uninstalling pip-24.3.1:\n",
      "      Successfully uninstalled pip-24.3.1\n",
      "Successfully installed pip-23.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pip==23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-retro in ./.venv/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: gym in ./.venv/lib/python3.8/site-packages (from gym-retro) (0.26.2)\n",
      "Requirement already satisfied: pyglet==1.*,>=1.3.2 in ./.venv/lib/python3.8/site-packages (from gym-retro) (1.5.29)\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./.venv/lib/python3.8/site-packages (from gym->gym-retro) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.8/site-packages (from gym->gym-retro) (3.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in ./.venv/lib/python3.8/site-packages (from gym->gym-retro) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in ./.venv/lib/python3.8/site-packages (from gym->gym-retro) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym->gym-retro) (3.20.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym-retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==65.5.0\n",
      "  Obtaining dependency information for setuptools==65.5.0 from https://files.pythonhosted.org/packages/41/82/7f54bbfe5c247a8c9f78d8d1d7c051847bcb78843c397b866dba335c1e88/setuptools-65.5.0-py3-none-any.whl.metadata\n",
      "  Using cached setuptools-65.5.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting wheel<0.40.0\n",
      "  Obtaining dependency information for wheel<0.40.0 from https://files.pythonhosted.org/packages/bd/7c/d38a0b30ce22fc26ed7dbc087c6d00851fb3395e9d0dac40bec1f905030c/wheel-0.38.4-py3-none-any.whl.metadata\n",
      "  Using cached wheel-0.38.4-py3-none-any.whl.metadata (2.1 kB)\n",
      "Using cached setuptools-65.5.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: wheel, setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 49.2.1\n",
      "    Uninstalling setuptools-49.2.1:\n",
      "      Successfully uninstalled setuptools-49.2.1\n",
      "Successfully installed setuptools-65.5.0 wheel-0.38.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install setuptools==65.5.0 \"wheel<0.40.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym==0.21.0\n",
      "  Using cached gym-0.21.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./.venv/lib/python3.8/site-packages (from gym==0.21.0) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.8/site-packages (from gym==0.21.0) (3.1.0)\n",
      "Installing collected packages: gym\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.26.2\n",
      "    Uninstalling gym-0.26.2:\n",
      "      Successfully uninstalled gym-0.26.2\n",
      "Successfully installed gym-0.21.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/64/4a/016cda9ad7cf18c58ba074628a4eaae8aa55f3fd06a266398cef8831a5b9/opencv_python-4.10.0.84-cp37-abi3-macosx_12_0_x86_64.whl.metadata\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-macosx_12_0_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in ./.venv/lib/python3.8/site-packages (from opencv-python) (1.24.4)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-macosx_12_0_x86_64.whl (56.5 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in ./.venv/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: gym-retro in ./.venv/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./.venv/lib/python3.8/site-packages (from gym) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.8/site-packages (from gym) (3.1.0)\n",
      "Requirement already satisfied: pyglet==1.*,>=1.3.2 in ./.venv/lib/python3.8/site-packages (from gym-retro) (1.5.29)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym gym-retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/b1/ea/129163dcd21db6da5d559a8160c4a74c1dc5f96ac246a3d4248b43c7648d/matplotlib-3.7.5-cp38-cp38-macosx_10_12_x86_64.whl.metadata\n",
      "  Using cached matplotlib-3.7.5-cp38-cp38-macosx_10_12_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/02/7e/ffaba1bf3719088be3ad6983a5e85e1fc9edccd7b406b98e433436ecef74/contourpy-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached contourpy-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Obtaining dependency information for cycler>=0.10 from https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl.metadata\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/3f/b2/92fbceab8e287f1a25b0bdd2597fa98e33258997f1e026d854ff6f6972b7/fonttools-4.54.1-cp38-cp38-macosx_10_9_universal2.whl.metadata\n",
      "  Using cached fonttools-4.54.1-cp38-cp38-macosx_10_9_universal2.whl.metadata (163 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for kiwisolver>=1.0.1 from https://files.pythonhosted.org/packages/bd/c6/572ad7d73dbd898cffa9050ffd7ff7e78a055a1d9b7accd6b4d1f50ec858/kiwisolver-1.4.7-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached kiwisolver-1.4.7-cp38-cp38-macosx_10_9_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Obtaining dependency information for pillow>=6.2.0 from https://files.pythonhosted.org/packages/56/70/f40009702a477ce87d8d9faaa4de51d6562b3445d7a314accd06e4ffb01d/pillow-10.4.0-cp38-cp38-macosx_10_10_x86_64.whl.metadata\n",
      "  Using cached pillow-10.4.0-cp38-cp38-macosx_10_10_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Obtaining dependency information for pyparsing>=2.3.1 from https://files.pythonhosted.org/packages/e5/0c/0e3c05b1c87bb6a1c76d281b0f35e78d2d80ac91b5f8f524cebf77f51049/pyparsing-3.1.4-py3-none-any.whl.metadata\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Obtaining dependency information for importlib-resources>=3.2.0 from https://files.pythonhosted.org/packages/e1/6a/4604f9ae2fa62ef47b9de2fa5ad599589d28c9fd1d335f32759813dfa91e/importlib_resources-6.4.5-py3-none-any.whl.metadata\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Using cached matplotlib-3.7.5-cp38-cp38-macosx_10_12_x86_64.whl (7.4 MB)\n",
      "Using cached contourpy-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl (247 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.54.1-cp38-cp38-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Using cached kiwisolver-1.4.7-cp38-cp38-macosx_10_9_x86_64.whl (65 kB)\n",
      "Using cached pillow-10.4.0-cp38-cp38-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "Using cached pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.54.1 importlib-resources-6.4.5 kiwisolver-1.4.7 matplotlib-3.7.5 pillow-10.4.0 pyparsing-3.1.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take note of my versions^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import retro to play Street Fighter using a ROM\n",
    "import retro\n",
    "# Import time to slow down game\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%python is a cell magic, but the cell body is empty.\n"
     ]
    }
   ],
   "source": [
    "# %%python -m retro.import . # Run this from the roms folder, or where you have your game roms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class for a wrapper \n",
    "from gym import Env \n",
    "# Import the space shapes for the environment\n",
    "from gym.spaces import MultiBinary, Box\n",
    "# Import numpy to calculate frame delta \n",
    "import numpy as np\n",
    "# Import opencv for grayscaling\n",
    "import cv2\n",
    "# Import matplotlib for plotting the image\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom environment \n",
    "class StreetFighter(Env): # pass in basic env from above to preprocessing\n",
    "    def __init__(self):\n",
    "        super().__init__() # inherit from base env\n",
    "        # Specify action space and observation space \n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8) # grayscaled frame, smaller amt of pixels\n",
    "        self.action_space = MultiBinary(12) # type of actions that can be taken\n",
    "        self.health = 144\n",
    "        self.enemy_health = 144\n",
    "        self.score = 0\n",
    "        self.matches_won = 0\n",
    "        self.continue_timer = 100\n",
    "        self.enemy_matches_won = 0\n",
    "        self.previous_action = np.zeros(12)\n",
    "        # Startup and instance of the game \n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED) # used to get valid button combos\n",
    "    \n",
    "    def reset(self): # restart\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs # sets previous frame to current frame\n",
    "        \n",
    "        # Create a attribute to hold the score delta \n",
    "        self.score = 0 \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation): # grayscale, resize\n",
    "        # Grayscaling \n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize \n",
    "        resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)\n",
    "        # Add the channels value\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels \n",
    "    \n",
    "    def reward_function(self, state, action):\n",
    "        # Extract variables\n",
    "        continuetimer = state['continuetimer']\n",
    "        enemy_matches_won = state['enemy_matches_won']\n",
    "        enemy_health = state['enemy_health']\n",
    "        health = state['health']\n",
    "        matches_won = state['matches_won']\n",
    "        score = state['score']\n",
    "\n",
    "        # Initialize reward\n",
    "        reward = 0\n",
    "\n",
    "        # Reward for increasing score each frame (scaled down to avoid excessively large rewards)\n",
    "        reward += score * 0.001  \n",
    "\n",
    "        # Reward for inflicting damage on the enemy, only if enemy_health is reduced\n",
    "        if enemy_health < self.enemy_health:\n",
    "            reward += (self.enemy_health - enemy_health) * 10\n",
    "\n",
    "        # Penalty for losing health (scaled so health loss gives a clear penalty)\n",
    "        if health < self.health:\n",
    "            reward -= (self.health - health) * 10\n",
    "\n",
    "        # Bonus if the player maintains health\n",
    "        reward += health * 0.05  \n",
    "\n",
    "        # Encourage the AI to win matches\n",
    "        if matches_won > self.matches_won:\n",
    "            reward += 200  # Winning a match should give a significant reward\n",
    "\n",
    "        # Penalize the AI if the enemy wins a match\n",
    "        if enemy_matches_won > self.enemy_matches_won:\n",
    "            reward -= 200\n",
    "\n",
    "        # Small positive reward to encourage efficient play (keeping the timer active)\n",
    "        reward += continuetimer * 0.01  \n",
    "\n",
    "        # Penalize the AI for taking actions that do not lead to damage dealt\n",
    "        if enemy_health == self.enemy_health and action != np.zeros(12):\n",
    "            reward -= 1\n",
    "\n",
    "\n",
    "        # Update previous states to enable frame-by-frame comparison\n",
    "        self.enemy_health = enemy_health\n",
    "        self.health = health\n",
    "        self.matches_won = matches_won\n",
    "        self.enemy_matches_won = enemy_matches_won\n",
    "        self.continue_timer = continuetimer\n",
    "        self.score = score\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def step(self, action): # how do we process action\n",
    "        # Take a step \n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs) \n",
    "\n",
    "        self.previous_action = action\n",
    "        \n",
    "        # Frame delta \n",
    "        frame_delta = obs - self.previous_frame # change in pixels (was dropped in final model of tutorial)\n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Reshape the reward function\n",
    "        reward = self.reward_function(info, action)\n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs): # unpack any args and kwargs from stable baseline\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.800000000000001\n",
      "8.3\n",
      "8.100000000000001\n",
      "8.100000000000001\n",
      "8.4\n",
      "8.200000000000001\n",
      "8.100000000000001\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.600000000000001\n",
      "8.5\n",
      "8.3\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.600000000000001\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.5\n",
      "8.4\n",
      "8.4\n",
      "8.200000000000001\n",
      "8.200000000000001\n",
      "8.5\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.5\n",
      "8.100000000000001\n",
      "8.3\n",
      "7.9\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.600000000000001\n",
      "8.200000000000001\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.0\n",
      "8.4\n",
      "8.0\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.600000000000001\n",
      "7.9\n",
      "7.9\n",
      "7.9\n",
      "8.4\n",
      "8.200000000000001\n",
      "8.200000000000001\n",
      "8.200000000000001\n",
      "8.4\n",
      "8.100000000000001\n",
      "7.9\n",
      "8.3\n",
      "7.9\n",
      "8.3\n",
      "8.100000000000001\n",
      "8.200000000000001\n",
      "8.5\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.100000000000001\n",
      "8.0\n",
      "8.200000000000001\n",
      "8.100000000000001\n",
      "8.200000000000001\n",
      "8.4\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.100000000000001\n",
      "8.100000000000001\n",
      "8.100000000000001\n",
      "8.4\n",
      "8.100000000000001\n",
      "8.100000000000001\n",
      "8.200000000000001\n",
      "8.0\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.5\n",
      "8.200000000000001\n",
      "8.200000000000001\n",
      "7.9\n",
      "8.3\n",
      "7.800000000000001\n",
      "8.4\n",
      "8.200000000000001\n",
      "8.4\n",
      "7.9\n",
      "8.5\n",
      "8.4\n",
      "8.200000000000001\n",
      "8.5\n",
      "8.200000000000001\n",
      "8.0\n",
      "8.5\n",
      "8.0\n",
      "8.0\n",
      "8.4\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.0\n",
      "8.5\n",
      "8.200000000000001\n",
      "8.4\n",
      "8.200000000000001\n",
      "8.200000000000001\n",
      "8.4\n",
      "7.9\n",
      "8.3\n",
      "8.600000000000001\n",
      "8.600000000000001\n",
      "8.100000000000001\n",
      "8.5\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.4\n",
      "8.100000000000001\n",
      "8.100000000000001\n",
      "8.200000000000001\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.100000000000001\n",
      "8.100000000000001\n",
      "8.4\n",
      "8.0\n",
      "8.100000000000001\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.3\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.200000000000001\n",
      "8.3\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.4\n",
      "8.3\n",
      "8.3\n",
      "8.0\n",
      "8.5\n",
      "8.3\n",
      "8.100000000000001\n",
      "8.100000000000001\n",
      "8.5\n",
      "8.3\n",
      "8.200000000000001\n",
      "8.4\n",
      "8.0\n",
      "8.3\n",
      "8.600000000000001\n",
      "8.100000000000001\n",
      "8.100000000000001\n",
      "8.3\n",
      "8.4\n",
      "8.3\n",
      "8.3\n",
      "7.9\n",
      "-212.70000000000002\n",
      "7.1\n",
      "7.1\n",
      "7.0\n",
      "7.2\n",
      "7.2\n",
      "7.2\n",
      "7.0\n",
      "7.4\n",
      "6.8\n",
      "7.3\n",
      "7.2\n",
      "6.9\n",
      "7.0\n",
      "7.2\n",
      "7.0\n",
      "6.9\n",
      "7.2\n",
      "7.1\n",
      "6.9\n",
      "7.2\n",
      "7.2\n",
      "7.3\n",
      "7.3\n",
      "6.8\n",
      "7.3\n",
      "7.1\n",
      "7.1\n",
      "7.1\n",
      "7.1\n",
      "6.9\n",
      "7.1\n",
      "7.1\n",
      "7.0\n",
      "7.2\n",
      "6.9\n",
      "6.8\n",
      "7.2\n",
      "7.4\n",
      "7.0\n",
      "7.1\n",
      "7.3\n",
      "7.1\n",
      "7.1\n",
      "7.2\n",
      "7.1\n",
      "7.0\n",
      "7.3\n",
      "7.1\n",
      "7.1\n",
      "6.9\n",
      "7.3\n",
      "7.0\n",
      "7.1\n",
      "7.0\n",
      "7.1\n",
      "7.2\n",
      "7.4\n",
      "7.3\n",
      "7.4\n",
      "7.1\n",
      "7.3\n",
      "7.3\n",
      "7.6000000000000005\n",
      "7.0\n",
      "7.1\n",
      "7.0\n",
      "7.3\n",
      "7.2\n",
      "7.2\n",
      "7.0\n",
      "7.0\n",
      "7.3\n",
      "7.1\n",
      "6.8\n",
      "7.1\n",
      "6.8\n",
      "7.2\n",
      "7.1\n",
      "7.2\n",
      "7.2\n",
      "7.1\n",
      "7.3\n",
      "7.1\n",
      "7.2\n",
      "7.2\n",
      "7.0\n",
      "6.8\n",
      "6.9\n",
      "7.2\n",
      "7.0\n",
      "7.2\n",
      "6.9\n",
      "7.2\n",
      "7.2\n",
      "7.0\n",
      "7.2\n",
      "7.3\n",
      "7.1\n",
      "7.4\n",
      "7.0\n",
      "7.3\n",
      "6.9\n",
      "6.9\n",
      "7.5\n",
      "7.0\n",
      "7.1\n",
      "7.2\n",
      "7.1\n",
      "7.0\n",
      "7.1\n",
      "6.9\n",
      "6.9\n",
      "7.2\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "7.4\n",
      "7.2\n",
      "7.2\n",
      "7.2\n",
      "7.0\n",
      "7.0\n",
      "6.9\n",
      "7.2\n",
      "6.9\n",
      "7.1\n",
      "6.8\n",
      "7.2\n",
      "7.1\n",
      "7.0\n",
      "7.1\n",
      "6.8\n",
      "6.9\n",
      "7.0\n",
      "6.9\n",
      "7.2\n",
      "7.3\n",
      "7.1\n",
      "7.1\n",
      "7.2\n",
      "7.1\n",
      "7.0\n",
      "7.1\n",
      "7.2\n",
      "7.2\n",
      "7.4\n",
      "7.1\n",
      "7.3\n",
      "7.2\n",
      "7.0\n",
      "7.0\n",
      "7.2\n",
      "7.2\n",
      "7.0\n",
      "7.5\n",
      "7.2\n",
      "7.0\n",
      "6.9\n",
      "7.3\n",
      "6.7\n",
      "7.0\n",
      "7.0\n",
      "7.4\n",
      "7.1\n",
      "7.2\n",
      "7.1\n",
      "7.4\n",
      "7.0\n",
      "7.3\n",
      "7.3\n",
      "7.0\n",
      "7.0\n",
      "7.2\n",
      "7.2\n",
      "7.2\n",
      "7.0\n",
      "7.5\n",
      "7.1\n",
      "7.0\n",
      "6.9\n",
      "7.0\n",
      "7.1\n",
      "6.9\n",
      "7.5\n",
      "7.1\n",
      "7.0\n",
      "7.1\n",
      "7.2\n",
      "7.1\n",
      "7.2\n",
      "7.2\n",
      "7.0\n",
      "7.1\n",
      "6.9\n",
      "7.2\n",
      "7.2\n",
      "7.2\n",
      "6.8\n",
      "7.2\n",
      "7.1\n",
      "367.2\n",
      "7.499999999999999\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.3999999999999995\n",
      "7.199999999999999\n",
      "7.699999999999999\n",
      "7.3999999999999995\n",
      "7.699999999999999\n",
      "7.799999999999999\n",
      "7.699999999999999\n",
      "7.6\n",
      "7.6\n",
      "7.6\n",
      "7.3999999999999995\n",
      "7.799999999999999\n",
      "7.6\n",
      "7.699999999999999\n",
      "7.699999999999999\n",
      "7.699999999999999\n",
      "7.6\n",
      "7.3999999999999995\n",
      "7.3999999999999995\n",
      "7.499999999999999\n",
      "7.699999999999999\n",
      "7.299999999999999\n",
      "7.499999999999999\n",
      "7.699999999999999\n",
      "7.499999999999999\n",
      "7.499999999999999\n",
      "7.8999999999999995\n",
      "7.999999999999999\n",
      "7.3999999999999995\n",
      "7.6\n",
      "7.3999999999999995\n",
      "7.6\n",
      "7.8999999999999995\n",
      "7.299999999999999\n",
      "7.299999999999999\n",
      "7.6\n",
      "7.3999999999999995\n",
      "7.3999999999999995\n",
      "7.3999999999999995\n",
      "7.6\n",
      "7.699999999999999\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.3999999999999995\n",
      "7.499999999999999\n",
      "7.799999999999999\n",
      "7.8999999999999995\n",
      "7.299999999999999\n",
      "7.3999999999999995\n",
      "7.3999999999999995\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.799999999999999\n",
      "7.6\n",
      "7.6\n",
      "7.6\n",
      "7.699999999999999\n",
      "7.3999999999999995\n",
      "7.6\n",
      "7.3999999999999995\n",
      "7.3999999999999995\n",
      "7.799999999999999\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.299999999999999\n",
      "7.699999999999999\n",
      "7.799999999999999\n",
      "7.699999999999999\n",
      "7.6\n",
      "7.6\n",
      "7.3999999999999995\n",
      "7.699999999999999\n",
      "7.699999999999999\n",
      "7.3999999999999995\n",
      "7.499999999999999\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.699999999999999\n",
      "7.8999999999999995\n",
      "8.1\n",
      "7.499999999999999\n",
      "7.8999999999999995\n",
      "7.799999999999999\n",
      "7.699999999999999\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.3999999999999995\n",
      "7.499999999999999\n",
      "7.8999999999999995\n",
      "7.6\n",
      "7.699999999999999\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.699999999999999\n",
      "7.3999999999999995\n",
      "7.699999999999999\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.3999999999999995\n",
      "7.699999999999999\n",
      "7.699999999999999\n",
      "7.499999999999999\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.699999999999999\n",
      "7.799999999999999\n",
      "7.699999999999999\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.8999999999999995\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.3999999999999995\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.699999999999999\n",
      "7.499999999999999\n",
      "7.699999999999999\n",
      "7.799999999999999\n",
      "7.499999999999999\n",
      "7.299999999999999\n",
      "7.699999999999999\n",
      "7.299999999999999\n",
      "7.699999999999999\n",
      "7.699999999999999\n",
      "7.199999999999999\n",
      "7.6\n",
      "7.8999999999999995\n",
      "7.3999999999999995\n",
      "7.6\n",
      "7.8999999999999995\n",
      "7.6\n",
      "7.6\n",
      "7.799999999999999\n",
      "7.6\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.699999999999999\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.6\n",
      "7.699999999999999\n",
      "7.499999999999999\n",
      "7.799999999999999\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.3999999999999995\n",
      "7.6\n",
      "7.6\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.6\n",
      "7.6\n",
      "7.699999999999999\n",
      "7.299999999999999\n",
      "7.3999999999999995\n",
      "7.999999999999999\n",
      "7.699999999999999\n",
      "7.6\n",
      "7.499999999999999\n",
      "7.799999999999999\n",
      "7.6\n",
      "7.6\n",
      "7.799999999999999\n",
      "7.799999999999999\n",
      "7.499999999999999\n",
      "7.499999999999999\n",
      "7.699999999999999\n",
      "7.699999999999999\n",
      "-233.5\n",
      "6.4\n",
      "6.5\n",
      "6.3\n",
      "6.5\n",
      "6.4\n",
      "6.3\n",
      "6.0\n",
      "6.1\n",
      "6.2\n",
      "6.5\n",
      "6.6\n",
      "6.2\n",
      "6.4\n",
      "6.6\n",
      "6.5\n",
      "6.7\n",
      "6.7\n",
      "6.4\n",
      "6.2\n",
      "6.4\n",
      "6.5\n",
      "6.2\n",
      "6.2\n",
      "6.7\n",
      "6.6\n",
      "6.5\n",
      "6.6\n",
      "6.5\n",
      "6.1\n",
      "6.5\n",
      "6.6\n",
      "6.2\n",
      "6.4\n",
      "6.3\n",
      "6.4\n",
      "6.4\n",
      "6.4\n",
      "6.2\n",
      "6.4\n",
      "6.4\n",
      "6.3\n",
      "6.3\n",
      "6.3\n",
      "6.6\n",
      "6.2\n",
      "6.4\n",
      "6.4\n",
      "6.2\n",
      "6.7\n",
      "6.6\n",
      "6.5\n",
      "6.3\n",
      "6.7\n",
      "6.5\n",
      "6.3\n",
      "6.3\n",
      "6.3\n",
      "6.3\n",
      "6.3\n",
      "6.1\n",
      "6.3\n",
      "6.3\n",
      "6.2\n",
      "6.2\n",
      "6.6\n",
      "6.1\n",
      "6.2\n",
      "6.4\n",
      "6.5\n",
      "6.4\n",
      "6.5\n",
      "6.3\n",
      "6.1\n",
      "6.5\n",
      "6.4\n",
      "6.4\n",
      "6.2\n",
      "6.6\n",
      "6.1\n",
      "6.2\n",
      "6.3\n",
      "6.2\n",
      "6.4\n",
      "6.0\n",
      "6.5\n",
      "6.6\n",
      "6.5\n",
      "6.3\n",
      "6.4\n",
      "6.2\n",
      "6.5\n",
      "6.3\n",
      "6.2\n",
      "6.3\n",
      "6.3\n",
      "6.4\n",
      "6.4\n",
      "6.4\n",
      "6.4\n",
      "6.4\n",
      "6.1\n",
      "6.4\n",
      "6.1\n",
      "6.4\n",
      "6.2\n",
      "6.5\n",
      "6.2\n",
      "6.3\n",
      "6.2\n",
      "6.1\n",
      "6.4\n",
      "6.7\n",
      "6.5\n",
      "6.3\n",
      "6.2\n",
      "6.1\n",
      "6.2\n",
      "6.4\n",
      "6.8\n",
      "6.4\n",
      "6.3\n",
      "6.4\n",
      "6.5\n",
      "6.2\n",
      "6.4\n",
      "6.4\n",
      "6.5\n",
      "6.4\n",
      "6.6\n",
      "6.3\n",
      "6.3\n",
      "6.2\n",
      "6.4\n",
      "6.2\n",
      "6.3\n",
      "6.1\n",
      "6.5\n",
      "6.3\n",
      "6.5\n",
      "6.5\n",
      "6.3\n",
      "6.3\n",
      "6.8\n",
      "6.8\n",
      "6.5\n",
      "6.5\n",
      "6.4\n",
      "6.2\n",
      "6.6\n",
      "6.2\n",
      "6.5\n",
      "6.4\n",
      "6.4\n",
      "6.6\n",
      "6.7\n",
      "6.6\n",
      "6.6\n",
      "6.3\n",
      "6.5\n",
      "6.5\n",
      "6.4\n",
      "6.5\n",
      "6.5\n",
      "6.2\n",
      "6.7\n",
      "6.3\n",
      "6.2\n",
      "6.5\n",
      "6.5\n",
      "6.4\n",
      "6.1\n",
      "6.6\n",
      "6.5\n",
      "6.5\n",
      "6.5\n",
      "6.2\n",
      "6.4\n",
      "6.6\n",
      "6.3\n",
      "6.4\n",
      "6.4\n",
      "6.5\n",
      "6.3\n",
      "6.5\n",
      "6.8\n",
      "6.7\n",
      "6.3\n",
      "6.6\n",
      "6.4\n",
      "6.5\n",
      "6.4\n",
      "6.2\n",
      "6.6\n",
      "6.3\n",
      "6.4\n",
      "6.4\n",
      "6.1\n",
      "6.6\n",
      "6.2\n",
      "6.5\n",
      "6.3\n",
      "6.3\n",
      "6.2\n",
      "6.2\n",
      "6.4\n",
      "6.4\n",
      "6.5\n",
      "6.4\n",
      "6.2\n",
      "6.2\n",
      "6.3\n",
      "6.2\n",
      "6.4\n",
      "6.3\n",
      "6.6\n",
      "6.8\n",
      "6.1\n",
      "6.7\n",
      "6.4\n",
      "6.3\n",
      "6.3\n",
      "6.2\n",
      "6.3\n",
      "6.3\n",
      "6.5\n",
      "-53.9\n",
      "6.0\n",
      "5.8\n",
      "5.9\n",
      "6.4\n",
      "5.9\n",
      "6.1\n",
      "5.7\n",
      "6.2\n",
      "6.4\n",
      "5.7\n",
      "6.4\n",
      "6.0\n",
      "6.1\n",
      "6.2\n",
      "6.2\n",
      "6.1\n",
      "6.1\n",
      "5.9\n",
      "6.2\n",
      "5.8\n",
      "6.1\n",
      "5.8\n",
      "6.0\n",
      "6.0\n",
      "6.3\n",
      "6.0\n",
      "6.1\n",
      "5.9\n",
      "5.9\n",
      "6.3\n",
      "6.3\n",
      "6.0\n",
      "6.1\n",
      "6.1\n",
      "6.0\n",
      "6.3\n",
      "6.1\n",
      "6.4\n",
      "6.2\n",
      "6.2\n",
      "5.9\n",
      "6.0\n",
      "6.0\n",
      "6.5\n",
      "6.3\n",
      "6.3\n",
      "6.1\n",
      "6.2\n",
      "6.3\n",
      "6.2\n",
      "6.2\n",
      "6.2\n",
      "6.1\n",
      "6.1\n",
      "6.2\n",
      "6.2\n",
      "6.2\n",
      "6.1\n",
      "6.4\n",
      "6.0\n",
      "6.4\n",
      "6.2\n",
      "5.9\n",
      "6.1\n",
      "6.0\n",
      "5.9\n",
      "6.2\n",
      "6.4\n",
      "6.3\n",
      "5.8\n",
      "6.0\n",
      "5.9\n",
      "6.1\n",
      "6.1\n",
      "5.9\n",
      "6.1\n",
      "6.0\n",
      "-235.0\n",
      "4.8\n",
      "5.3\n",
      "4.9\n",
      "4.9\n",
      "4.8\n",
      "4.7\n",
      "4.9\n",
      "4.8\n",
      "4.9\n",
      "5.2\n",
      "4.9\n",
      "4.6\n",
      "4.9\n",
      "4.9\n",
      "5.0\n",
      "5.0\n",
      "4.9\n",
      "4.7\n",
      "5.0\n",
      "4.9\n",
      "4.7\n",
      "4.9\n",
      "4.7\n",
      "5.0\n",
      "4.9\n",
      "5.0\n",
      "4.9\n",
      "4.9\n",
      "5.0\n",
      "4.7\n",
      "4.9\n",
      "5.1\n",
      "4.8\n",
      "5.0\n",
      "4.8\n",
      "4.8\n",
      "4.9\n",
      "4.9\n",
      "5.2\n",
      "5.0\n",
      "4.8\n",
      "4.8\n",
      "4.9\n",
      "5.0\n",
      "5.0\n",
      "4.9\n",
      "4.5\n",
      "4.7\n",
      "5.0\n",
      "4.9\n",
      "4.8\n",
      "4.6\n",
      "4.7\n",
      "5.0\n",
      "4.8\n",
      "4.9\n",
      "4.9\n",
      "4.7\n",
      "4.9\n",
      "4.8\n",
      "4.7\n",
      "5.1\n",
      "5.0\n",
      "4.8\n",
      "4.9\n",
      "4.5\n",
      "4.9\n",
      "4.8\n",
      "5.1\n",
      "5.0\n",
      "4.9\n",
      "4.9\n",
      "4.9\n",
      "5.1\n",
      "4.9\n",
      "4.8\n",
      "4.8\n",
      "4.9\n",
      "4.7\n",
      "4.5\n",
      "4.9\n",
      "5.0\n",
      "4.7\n",
      "5.2\n",
      "4.7\n",
      "4.8\n",
      "4.9\n",
      "4.9\n",
      "5.1\n",
      "5.1\n",
      "4.8\n",
      "4.9\n",
      "4.6\n",
      "5.1\n",
      "5.0\n",
      "5.0\n",
      "4.6\n",
      "4.8\n",
      "5.2\n",
      "5.0\n",
      "4.8\n",
      "4.8\n",
      "5.1\n",
      "4.8\n",
      "4.8\n",
      "5.0\n",
      "4.8\n",
      "4.8\n",
      "4.9\n",
      "4.8\n",
      "5.0\n",
      "4.7\n",
      "4.8\n",
      "4.6\n",
      "4.8\n",
      "4.8\n",
      "4.9\n",
      "4.8\n",
      "5.0\n",
      "4.7\n",
      "4.7\n",
      "4.6\n",
      "5.1\n",
      "4.9\n",
      "5.2\n",
      "4.9\n",
      "4.7\n",
      "4.9\n",
      "4.7\n",
      "4.7\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.8\n",
      "5.2\n",
      "5.0\n",
      "4.7\n",
      "5.0\n",
      "4.8\n",
      "4.9\n",
      "4.9\n",
      "4.9\n",
      "4.8\n",
      "5.3\n",
      "4.7\n",
      "4.8\n",
      "5.1\n",
      "4.5\n",
      "5.4\n",
      "5.1\n",
      "4.8\n",
      "4.8\n",
      "4.9\n",
      "5.2\n",
      "4.7\n",
      "5.0\n",
      "4.7\n",
      "5.0\n",
      "4.8\n",
      "4.8\n",
      "4.8\n",
      "4.9\n",
      "4.9\n",
      "4.9\n",
      "4.9\n",
      "4.8\n",
      "4.7\n",
      "4.8\n",
      "5.3\n",
      "4.5\n",
      "5.0\n",
      "4.7\n",
      "4.9\n",
      "4.7\n",
      "4.9\n",
      "5.0\n",
      "4.8\n",
      "5.0\n",
      "4.7\n",
      "4.7\n",
      "5.0\n",
      "5.2\n",
      "5.2\n",
      "4.8\n",
      "5.0\n",
      "4.9\n",
      "5.2\n",
      "4.8\n",
      "4.8\n",
      "4.7\n",
      "4.9\n",
      "4.9\n",
      "5.0\n",
      "4.7\n",
      "4.9\n",
      "4.7\n",
      "5.1\n",
      "4.9\n",
      "4.9\n",
      "4.9\n",
      "4.6\n",
      "4.9\n",
      "4.7\n",
      "4.7\n",
      "4.9\n",
      "5.1\n",
      "4.8\n",
      "4.9\n",
      "5.0\n",
      "4.9\n",
      "5.1\n",
      "324.8\n",
      "5.3999999999999995\n",
      "5.0\n",
      "5.1\n",
      "5.1\n",
      "5.199999999999999\n",
      "5.3\n",
      "5.0\n",
      "5.3\n",
      "5.1\n",
      "5.1\n",
      "5.5\n",
      "5.3\n",
      "5.1\n",
      "5.0\n",
      "4.8999999999999995\n",
      "5.1\n",
      "5.0\n",
      "5.3\n",
      "5.3\n",
      "5.0\n",
      "5.1\n",
      "5.3999999999999995\n",
      "5.0\n",
      "5.199999999999999\n",
      "5.199999999999999\n",
      "4.8999999999999995\n",
      "5.5\n",
      "5.3\n",
      "5.3\n",
      "5.0\n",
      "5.199999999999999\n",
      "5.0\n",
      "5.3\n",
      "5.3999999999999995\n",
      "5.3999999999999995\n",
      "5.0\n",
      "5.3999999999999995\n",
      "5.1\n",
      "5.199999999999999\n",
      "5.199999999999999\n",
      "5.199999999999999\n",
      "5.1\n",
      "5.199999999999999\n",
      "5.1\n",
      "5.0\n",
      "5.1\n",
      "5.3999999999999995\n",
      "5.3\n",
      "5.5\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.3\n",
      "6.1\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.5\n",
      "6.3\n",
      "6.7\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.3\n",
      "6.3\n",
      "6.0\n",
      "6.3999999999999995\n",
      "6.6\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.1\n",
      "6.3999999999999995\n",
      "6.3\n",
      "5.8999999999999995\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.3\n",
      "5.8999999999999995\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.3\n",
      "6.3999999999999995\n",
      "5.8999999999999995\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "5.8999999999999995\n",
      "6.3\n",
      "6.0\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.5\n",
      "6.0\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.199999999999999\n",
      "5.8999999999999995\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.0\n",
      "6.0\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.0\n",
      "6.1\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.6\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.5\n",
      "6.1\n",
      "6.3\n",
      "5.8999999999999995\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.0\n",
      "5.8999999999999995\n",
      "6.1\n",
      "6.5\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "5.8999999999999995\n",
      "6.0\n",
      "6.3\n",
      "5.8999999999999995\n",
      "6.3999999999999995\n",
      "6.1\n",
      "5.8999999999999995\n",
      "6.0\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.5\n",
      "6.3\n",
      "6.3\n",
      "6.3999999999999995\n",
      "5.8999999999999995\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.0\n",
      "6.0\n",
      "6.5\n",
      "6.0\n",
      "6.5\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.0\n",
      "5.8999999999999995\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.6\n",
      "6.3\n",
      "6.5\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.1\n",
      "6.5\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.5\n",
      "6.199999999999999\n",
      "6.5\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.1\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.0\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.3999999999999995\n",
      "6.5\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.1\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.0\n",
      "6.3\n",
      "6.1\n",
      "6.0\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.199999999999999\n",
      "5.8\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.1\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.1\n",
      "6.7\n",
      "6.5\n",
      "6.3\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.5\n",
      "6.1\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.1\n",
      "6.0\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.6\n",
      "6.5\n",
      "5.8999999999999995\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.1\n",
      "5.8999999999999995\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.5\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "5.8999999999999995\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.6\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.0\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.0\n",
      "6.5\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "6.5\n",
      "6.0\n",
      "6.5\n",
      "6.3\n",
      "5.8999999999999995\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.1\n",
      "6.1\n",
      "6.0\n",
      "6.5\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.1\n",
      "6.1\n",
      "6.6\n",
      "6.1\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.0\n",
      "6.0\n",
      "6.199999999999999\n",
      "5.699999999999999\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "5.8999999999999995\n",
      "6.5\n",
      "6.0\n",
      "6.5\n",
      "6.5\n",
      "6.3999999999999995\n",
      "6.1\n",
      "5.8999999999999995\n",
      "6.3999999999999995\n",
      "6.6\n",
      "6.0\n",
      "6.3999999999999995\n",
      "6.5\n",
      "6.3\n",
      "6.3\n",
      "5.8\n",
      "5.8999999999999995\n",
      "5.8999999999999995\n",
      "6.3\n",
      "6.1\n",
      "6.1\n",
      "6.0\n",
      "6.1\n",
      "6.1\n",
      "6.3\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.0\n",
      "6.0\n",
      "6.3\n",
      "6.0\n",
      "5.8999999999999995\n",
      "6.0\n",
      "5.8999999999999995\n",
      "6.1\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.5\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.1\n",
      "6.1\n",
      "6.1\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.3999999999999995\n",
      "5.8999999999999995\n",
      "5.8999999999999995\n",
      "6.3999999999999995\n",
      "6.5\n",
      "6.3\n",
      "6.199999999999999\n",
      "5.699999999999999\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.1\n",
      "6.3\n",
      "6.0\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.0\n",
      "6.0\n",
      "6.7\n",
      "6.199999999999999\n",
      "6.0\n",
      "6.3999999999999995\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.1\n",
      "6.1\n",
      "5.8999999999999995\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.6\n",
      "6.0\n",
      "6.0\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "5.8\n",
      "5.8999999999999995\n",
      "6.199999999999999\n",
      "5.8999999999999995\n",
      "6.0\n",
      "6.0\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.0\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.3\n",
      "5.8999999999999995\n",
      "6.6\n",
      "6.3\n",
      "6.1\n",
      "5.699999999999999\n",
      "5.8999999999999995\n",
      "6.1\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "76.7\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.1\n",
      "6.1\n",
      "6.0\n",
      "6.3\n",
      "6.0\n",
      "6.3\n",
      "6.0\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.5\n",
      "6.5\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.3\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.0\n",
      "5.699999999999999\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.0\n",
      "6.1\n",
      "6.1\n",
      "6.3\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.3\n",
      "6.1\n",
      "6.3\n",
      "5.8999999999999995\n",
      "6.3\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.3\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "5.8999999999999995\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.0\n",
      "5.8999999999999995\n",
      "6.0\n",
      "6.1\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.5\n",
      "6.1\n",
      "6.5\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.5\n",
      "6.199999999999999\n",
      "6.199999999999999\n",
      "6.6\n",
      "6.6\n",
      "6.3\n",
      "6.3999999999999995\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "6.1\n",
      "6.6\n",
      "6.199999999999999\n",
      "5.8999999999999995\n",
      "6.0\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.1\n",
      "6.1\n",
      "6.5\n",
      "6.5\n",
      "6.3\n",
      "6.0\n",
      "6.0\n",
      "6.3\n",
      "6.0\n",
      "6.3\n",
      "6.6\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.1\n",
      "6.3999999999999995\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "6.3\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.3\n",
      "6.3\n",
      "6.3\n",
      "6.1\n",
      "6.3\n",
      "6.1\n",
      "6.199999999999999\n",
      "6.3\n",
      "6.6\n",
      "6.199999999999999\n",
      "6.3999999999999995\n",
      "-234.79999999999998\n",
      "5.1000000000000005\n",
      "5.0\n",
      "4.800000000000001\n",
      "5.0\n",
      "4.9\n",
      "5.300000000000001\n",
      "5.0\n",
      "5.0\n",
      "4.800000000000001\n",
      "4.9\n",
      "5.0\n",
      "4.9\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.7\n",
      "5.0\n",
      "5.1000000000000005\n",
      "5.4\n",
      "5.0\n",
      "5.300000000000001\n",
      "4.9\n",
      "5.1000000000000005\n",
      "5.1000000000000005\n",
      "5.1000000000000005\n",
      "5.0\n",
      "5.1000000000000005\n",
      "5.1000000000000005\n",
      "4.9\n",
      "5.300000000000001\n",
      "4.9\n",
      "4.7\n",
      "4.9\n",
      "5.0\n",
      "5.1000000000000005\n",
      "4.800000000000001\n",
      "5.0\n",
      "5.2\n",
      "4.800000000000001\n",
      "5.4\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.9\n",
      "5.300000000000001\n",
      "5.1000000000000005\n",
      "5.0\n",
      "5.0\n",
      "5.1000000000000005\n",
      "4.800000000000001\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.800000000000001\n",
      "5.2\n",
      "4.800000000000001\n",
      "4.9\n",
      "4.800000000000001\n",
      "5.2\n",
      "5.2\n",
      "4.800000000000001\n",
      "5.0\n",
      "5.2\n",
      "5.0\n",
      "4.800000000000001\n",
      "5.1000000000000005\n",
      "4.800000000000001\n",
      "4.9\n",
      "5.1000000000000005\n",
      "5.0\n",
      "4.9\n",
      "5.300000000000001\n",
      "5.2\n",
      "5.1000000000000005\n",
      "4.9\n",
      "5.2\n",
      "4.9\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "5.2\n",
      "4.800000000000001\n",
      "4.9\n",
      "5.0\n",
      "5.300000000000001\n",
      "5.4\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "5.0\n",
      "5.4\n",
      "4.9\n",
      "5.1000000000000005\n",
      "5.2\n",
      "5.1000000000000005\n",
      "5.0\n",
      "5.2\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "4.9\n",
      "4.9\n",
      "5.0\n",
      "5.0\n",
      "5.1000000000000005\n",
      "5.0\n",
      "5.0\n",
      "4.800000000000001\n",
      "4.9\n",
      "5.300000000000001\n",
      "5.0\n",
      "5.2\n",
      "5.0\n",
      "5.0\n",
      "5.300000000000001\n",
      "4.9\n",
      "4.9\n",
      "5.1000000000000005\n",
      "4.9\n",
      "5.2\n",
      "4.800000000000001\n",
      "5.1000000000000005\n",
      "5.1000000000000005\n",
      "5.0\n",
      "5.2\n",
      "4.9\n",
      "4.9\n",
      "5.2\n",
      "4.9\n",
      "5.1000000000000005\n",
      "5.0\n",
      "4.7\n",
      "5.2\n",
      "5.300000000000001\n",
      "4.9\n",
      "4.9\n",
      "5.1000000000000005\n",
      "5.1000000000000005\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "5.0\n",
      "5.1000000000000005\n",
      "5.0\n",
      "4.9\n",
      "5.0\n",
      "5.0\n",
      "5.1000000000000005\n",
      "4.7\n",
      "5.0\n",
      "5.2\n",
      "5.0\n",
      "4.9\n",
      "5.2\n",
      "5.0\n",
      "4.7\n",
      "4.7\n",
      "5.0\n",
      "5.1000000000000005\n",
      "5.4\n",
      "5.2\n",
      "4.800000000000001\n",
      "5.0\n",
      "5.300000000000001\n",
      "4.800000000000001\n",
      "5.0\n",
      "4.9\n",
      "4.7\n",
      "5.0\n",
      "5.1000000000000005\n",
      "5.0\n",
      "5.0\n",
      "4.800000000000001\n",
      "5.2\n",
      "4.800000000000001\n",
      "5.2\n",
      "5.1000000000000005\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.9\n",
      "4.800000000000001\n",
      "5.0\n",
      "4.6000000000000005\n",
      "5.0\n",
      "5.1000000000000005\n",
      "4.6000000000000005\n",
      "4.800000000000001\n",
      "4.6000000000000005\n",
      "5.0\n",
      "4.9\n",
      "4.800000000000001\n",
      "5.2\n",
      "5.1000000000000005\n",
      "4.9\n",
      "5.0\n",
      "5.2\n",
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "4.800000000000001\n",
      "5.1000000000000005\n",
      "4.9\n",
      "5.1000000000000005\n",
      "5.0\n",
      "4.7\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.800000000000001\n",
      "5.1000000000000005\n",
      "5.1000000000000005\n",
      "4.7\n",
      "4.800000000000001\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.9\n",
      "5.2\n",
      "4.800000000000001\n",
      "5.0\n",
      "5.300000000000001\n",
      "4.9\n",
      "5.0\n",
      "5.2\n",
      "5.2\n",
      "5.2\n",
      "5.1000000000000005\n",
      "5.1000000000000005\n",
      "4.800000000000001\n",
      "5.1000000000000005\n",
      "5.2\n",
      "5.0\n",
      "4.800000000000001\n",
      "4.9\n",
      "4.9\n",
      "5.2\n",
      "5.1000000000000005\n",
      "4.7\n",
      "4.9\n",
      "5.0\n",
      "5.0\n",
      "5.2\n",
      "4.7\n",
      "4.800000000000001\n",
      "4.7\n",
      "5.1000000000000005\n",
      "5.2\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.9\n",
      "-136.5\n",
      "3.4000000000000004\n",
      "3.3000000000000007\n",
      "3.5000000000000004\n",
      "3.3000000000000007\n",
      "3.3000000000000007\n",
      "3.7000000000000006\n",
      "3.4000000000000004\n",
      "3.6000000000000005\n",
      "3.7000000000000006\n",
      "3.4000000000000004\n",
      "3.5000000000000004\n",
      "3.4000000000000004\n",
      "3.3000000000000007\n",
      "3.4000000000000004\n",
      "3.4000000000000004\n",
      "3.3000000000000007\n",
      "3.6000000000000005\n",
      "3.0000000000000004\n",
      "3.5000000000000004\n",
      "3.4000000000000004\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.7000000000000006\n",
      "3.3000000000000007\n",
      "3.5000000000000004\n",
      "3.3000000000000007\n",
      "3.4000000000000004\n",
      "3.7000000000000006\n",
      "3.5000000000000004\n",
      "3.7000000000000006\n",
      "3.6000000000000005\n",
      "3.4000000000000004\n",
      "3.8000000000000007\n",
      "3.4000000000000004\n",
      "3.2000000000000006\n",
      "3.3000000000000007\n",
      "3.6000000000000005\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.8000000000000007\n",
      "3.4000000000000004\n",
      "3.3000000000000007\n",
      "3.4000000000000004\n",
      "3.7000000000000006\n",
      "3.1000000000000005\n",
      "3.8000000000000007\n",
      "3.4000000000000004\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.7000000000000006\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.4000000000000004\n",
      "3.6000000000000005\n",
      "3.4000000000000004\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.0000000000000004\n",
      "3.8000000000000007\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.2000000000000006\n",
      "3.7000000000000006\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.8000000000000007\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.8000000000000007\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.7000000000000006\n",
      "3.7000000000000006\n",
      "3.3000000000000007\n",
      "3.4000000000000004\n",
      "3.4000000000000004\n",
      "3.4000000000000004\n",
      "3.5000000000000004\n",
      "3.3000000000000007\n",
      "3.7000000000000006\n",
      "3.6000000000000005\n",
      "3.8000000000000007\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.1000000000000005\n",
      "3.8000000000000007\n",
      "3.3000000000000007\n",
      "3.6000000000000005\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.8000000000000007\n",
      "3.7000000000000006\n",
      "3.8000000000000007\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.4000000000000004\n",
      "3.7000000000000006\n",
      "3.8000000000000007\n",
      "3.8000000000000007\n",
      "3.3000000000000007\n",
      "3.4000000000000004\n",
      "3.5000000000000004\n",
      "3.4000000000000004\n",
      "3.6000000000000005\n",
      "3.3000000000000007\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.4000000000000004\n",
      "3.1000000000000005\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.3000000000000007\n",
      "3.6000000000000005\n",
      "3.7000000000000006\n",
      "3.5000000000000004\n",
      "3.3000000000000007\n",
      "3.7000000000000006\n",
      "3.5000000000000004\n",
      "3.2000000000000006\n",
      "3.8000000000000007\n",
      "3.5000000000000004\n",
      "3.3000000000000007\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.5000000000000004\n",
      "3.8000000000000007\n",
      "3.6000000000000005\n",
      "3.7000000000000006\n",
      "3.7000000000000006\n",
      "3.3000000000000007\n",
      "3.5000000000000004\n",
      "3.4000000000000004\n",
      "3.8000000000000007\n",
      "3.7000000000000006\n",
      "3.6000000000000005\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.4000000000000004\n",
      "3.4000000000000004\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.3000000000000007\n",
      "3.3000000000000007\n",
      "3.7000000000000006\n",
      "3.4000000000000004\n",
      "3.4000000000000004\n",
      "3.8000000000000007\n",
      "3.3000000000000007\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.4000000000000004\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.3000000000000007\n",
      "3.5000000000000004\n",
      "3.4000000000000004\n",
      "3.6000000000000005\n",
      "3.3000000000000007\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.7000000000000006\n",
      "3.4000000000000004\n",
      "3.2000000000000006\n",
      "3.5000000000000004\n",
      "3.8000000000000007\n",
      "3.6000000000000005\n",
      "3.5000000000000004\n",
      "3.8000000000000007\n",
      "3.8000000000000007\n",
      "3.5000000000000004\n",
      "3.2000000000000006\n",
      "3.5000000000000004\n",
      "3.4000000000000004\n",
      "3.9000000000000004\n",
      "3.4000000000000004\n",
      "3.4000000000000004\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.3000000000000007\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.3000000000000007\n",
      "3.6000000000000005\n",
      "3.7000000000000006\n",
      "3.4000000000000004\n",
      "3.6000000000000005\n",
      "3.3000000000000007\n",
      "3.6000000000000005\n",
      "3.8000000000000007\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.5000000000000004\n",
      "3.4000000000000004\n",
      "3.3000000000000007\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.7000000000000006\n",
      "3.4000000000000004\n",
      "3.3000000000000007\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.5000000000000004\n",
      "3.7000000000000006\n",
      "3.7000000000000006\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.3000000000000007\n",
      "3.6000000000000005\n",
      "3.4000000000000004\n",
      "3.7000000000000006\n",
      "3.2000000000000006\n",
      "3.5000000000000004\n",
      "3.8000000000000007\n",
      "3.4000000000000004\n",
      "3.6000000000000005\n",
      "3.1000000000000005\n",
      "3.4000000000000004\n",
      "3.3000000000000007\n",
      "3.4000000000000004\n",
      "3.5000000000000004\n",
      "3.5000000000000004\n",
      "3.8000000000000007\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.2000000000000006\n",
      "3.4000000000000004\n",
      "3.2000000000000006\n",
      "3.6000000000000005\n",
      "3.8000000000000007\n",
      "3.8000000000000007\n",
      "3.4000000000000004\n",
      "3.4000000000000004\n",
      "3.8000000000000007\n",
      "3.7000000000000006\n",
      "3.3000000000000007\n",
      "3.7000000000000006\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.6000000000000005\n",
      "3.5000000000000004\n",
      "3.6000000000000005\n",
      "3.8000000000000007\n",
      "3.5000000000000004\n",
      "3.4000000000000004\n",
      "3.7000000000000006\n",
      "3.3000000000000007\n",
      "-247.75\n",
      "2.05\n",
      "2.05\n",
      "2.35\n",
      "2.55\n",
      "2.35\n",
      "2.15\n",
      "2.25\n",
      "2.25\n",
      "2.15\n",
      "2.25\n",
      "2.25\n",
      "2.15\n",
      "2.35\n",
      "2.15\n",
      "2.25\n",
      "2.25\n",
      "2.35\n",
      "2.25\n",
      "2.55\n",
      "2.45\n",
      "2.15\n",
      "2.15\n",
      "2.85\n",
      "2.25\n",
      "2.35\n",
      "2.15\n",
      "2.25\n",
      "2.25\n",
      "1.9500000000000002\n",
      "2.35\n",
      "1.9500000000000002\n",
      "2.35\n",
      "1.9500000000000002\n",
      "2.35\n",
      "2.55\n",
      "2.35\n",
      "2.05\n",
      "2.35\n",
      "2.15\n",
      "2.15\n",
      "2.25\n",
      "2.45\n",
      "2.25\n",
      "2.25\n",
      "2.35\n",
      "1.85\n",
      "2.35\n",
      "2.35\n",
      "2.35\n",
      "2.25\n",
      "2.15\n",
      "2.55\n",
      "1.9500000000000002\n",
      "2.45\n",
      "2.15\n",
      "2.15\n",
      "2.05\n",
      "2.55\n",
      "2.35\n",
      "1.9500000000000002\n",
      "2.35\n",
      "2.45\n",
      "2.35\n",
      "1.85\n",
      "2.45\n",
      "2.55\n",
      "2.45\n",
      "2.15\n",
      "2.45\n",
      "2.45\n",
      "2.15\n",
      "2.25\n",
      "2.15\n",
      "2.25\n",
      "2.05\n",
      "2.25\n",
      "1.85\n",
      "1.9500000000000002\n",
      "2.15\n",
      "2.05\n",
      "2.05\n",
      "2.15\n",
      "2.35\n",
      "2.25\n",
      "2.55\n",
      "2.15\n",
      "2.05\n",
      "2.25\n",
      "2.05\n",
      "1.9500000000000002\n",
      "2.05\n",
      "2.25\n",
      "2.25\n",
      "2.25\n",
      "2.15\n",
      "2.55\n",
      "1.9500000000000002\n",
      "2.15\n",
      "2.15\n",
      "2.05\n",
      "2.45\n",
      "2.25\n",
      "2.25\n",
      "2.25\n",
      "2.25\n",
      "2.05\n",
      "2.45\n",
      "2.25\n",
      "2.15\n",
      "2.45\n",
      "2.55\n",
      "2.15\n",
      "2.45\n",
      "2.25\n",
      "2.25\n",
      "2.55\n",
      "2.15\n",
      "2.15\n",
      "2.15\n",
      "2.65\n",
      "2.25\n",
      "2.25\n",
      "2.25\n",
      "2.55\n",
      "2.25\n",
      "2.05\n",
      "2.15\n",
      "2.35\n",
      "2.15\n",
      "2.25\n",
      "2.15\n",
      "2.75\n",
      "2.55\n",
      "2.35\n",
      "2.45\n",
      "2.05\n",
      "2.25\n",
      "2.35\n",
      "2.15\n",
      "2.35\n",
      "2.45\n",
      "2.45\n",
      "2.45\n",
      "2.15\n",
      "2.35\n",
      "2.65\n",
      "2.05\n",
      "2.35\n",
      "2.55\n",
      "2.25\n",
      "2.55\n",
      "2.05\n",
      "2.25\n",
      "2.35\n",
      "2.45\n",
      "2.35\n",
      "1.9500000000000002\n",
      "2.25\n",
      "2.35\n",
      "1.85\n",
      "2.25\n",
      "2.15\n",
      "2.15\n",
      "2.15\n",
      "2.25\n",
      "2.55\n",
      "2.45\n",
      "2.15\n",
      "2.25\n",
      "2.35\n",
      "2.35\n",
      "2.05\n",
      "2.05\n",
      "2.35\n",
      "2.15\n",
      "2.25\n",
      "2.15\n",
      "1.9500000000000002\n",
      "2.15\n",
      "2.25\n",
      "2.55\n",
      "2.25\n",
      "2.35\n",
      "2.35\n",
      "2.35\n",
      "2.15\n",
      "2.35\n",
      "2.35\n",
      "2.15\n",
      "2.35\n",
      "2.25\n",
      "2.35\n",
      "2.25\n",
      "2.35\n",
      "2.25\n",
      "2.25\n",
      "2.15\n",
      "2.25\n",
      "2.35\n",
      "2.15\n",
      "2.35\n",
      "2.05\n",
      "2.25\n",
      "2.15\n",
      "2.25\n",
      "2.05\n",
      "2.45\n",
      "2.25\n",
      "2.15\n",
      "2.15\n",
      "2.15\n",
      "2.05\n",
      "2.55\n",
      "2.45\n",
      "1.85\n",
      "2.45\n",
      "1.9500000000000002\n",
      "2.35\n",
      "2.25\n",
      "2.35\n",
      "1.9500000000000002\n",
      "2.25\n",
      "1.9500000000000002\n",
      "2.25\n",
      "2.45\n",
      "2.15\n",
      "2.05\n",
      "2.25\n",
      "2.35\n",
      "2.35\n",
      "2.35\n",
      "1.9500000000000002\n",
      "2.55\n",
      "2.25\n",
      "2.35\n",
      "1.9500000000000002\n",
      "2.55\n",
      "2.25\n",
      "2.25\n",
      "2.05\n",
      "1.9500000000000002\n",
      "2.35\n",
      "2.35\n",
      "1.85\n",
      "2.15\n",
      "2.25\n",
      "1.9500000000000002\n",
      "2.25\n",
      "2.55\n",
      "2.15\n",
      "2.25\n",
      "2.15\n",
      "2.25\n",
      "2.15\n",
      "2.15\n",
      "2.05\n",
      "2.55\n",
      "2.25\n",
      "2.05\n",
      "2.25\n",
      "2.35\n",
      "2.25\n",
      "2.35\n",
      "2.25\n",
      "2.55\n",
      "2.15\n",
      "2.15\n",
      "2.15\n",
      "2.15\n",
      "2.25\n",
      "2.15\n",
      "2.35\n",
      "2.35\n",
      "2.15\n",
      "2.15\n",
      "2.35\n",
      "-148.5\n",
      "1.4\n",
      "1.6\n",
      "1.7000000000000002\n",
      "1.6\n",
      "1.5\n",
      "1.9000000000000001\n",
      "1.4\n",
      "1.1\n",
      "1.5\n",
      "1.5\n",
      "1.4\n",
      "1.8\n",
      "1.4\n",
      "1.5\n",
      "1.8\n",
      "1.3\n",
      "1.6\n",
      "1.6\n",
      "1.9000000000000001\n",
      "1.5\n",
      "1.6\n",
      "1.3\n",
      "1.3\n",
      "1.3\n",
      "1.1\n",
      "1.2000000000000002\n",
      "1.5\n",
      "1.6\n",
      "1.3\n",
      "1.5\n",
      "1.7000000000000002\n",
      "1.6\n",
      "1.4\n",
      "1.4\n",
      "1.8\n",
      "1.3\n",
      "1.5\n",
      "1.4\n",
      "1.4\n",
      "1.5\n",
      "1.7000000000000002\n",
      "1.4\n",
      "1.7000000000000002\n",
      "1.3\n",
      "1.5\n",
      "1.2000000000000002\n",
      "1.2000000000000002\n",
      "1.6\n",
      "1.8\n",
      "1.3\n",
      "1.3\n",
      "1.5\n",
      "1.3\n",
      "1.4\n",
      "1.4\n",
      "1.6\n",
      "1.4\n",
      "1.7000000000000002\n",
      "1.5\n",
      "1.6\n",
      "1.4\n",
      "1.4\n",
      "1.7000000000000002\n",
      "1.6\n",
      "1.8\n",
      "1.5\n",
      "1.2000000000000002\n",
      "1.6\n",
      "1.4\n",
      "1.5\n",
      "1.4\n",
      "1.2000000000000002\n",
      "1.4\n",
      "1.4\n",
      "1.7000000000000002\n",
      "1.7000000000000002\n",
      "1.7000000000000002\n",
      "1.4\n",
      "1.8\n",
      "1.6\n",
      "1.5\n",
      "1.4\n",
      "1.3\n",
      "1.4\n",
      "1.5\n",
      "1.8\n",
      "1.4\n",
      "1.6\n",
      "1.3\n",
      "1.7000000000000002\n",
      "1.5\n",
      "1.4\n",
      "1.2000000000000002\n",
      "1.4\n",
      "1.3\n",
      "1.7000000000000002\n",
      "1.7000000000000002\n",
      "1.5\n",
      "1.7000000000000002\n",
      "1.6\n",
      "1.3\n",
      "1.6\n",
      "1.6\n",
      "1.5\n",
      "1.3\n",
      "1.6\n",
      "1.8\n",
      "1.3\n",
      "1.6\n",
      "1.5\n",
      "1.5\n",
      "1.4\n",
      "1.4\n",
      "-48.75\n",
      "-198.75\n",
      "0.9500000000000001\n",
      "1.4500000000000002\n",
      "1.4500000000000002\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.05\n",
      "1.15\n",
      "1.05\n",
      "1.35\n",
      "0.9500000000000001\n",
      "1.55\n",
      "1.25\n",
      "1.35\n",
      "1.05\n",
      "1.35\n",
      "1.4500000000000002\n",
      "1.15\n",
      "1.15\n",
      "1.4500000000000002\n",
      "1.4500000000000002\n",
      "1.05\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.35\n",
      "1.25\n",
      "1.35\n",
      "1.35\n",
      "1.35\n",
      "1.55\n",
      "1.15\n",
      "1.15\n",
      "1.15\n",
      "1.55\n",
      "1.25\n",
      "1.25\n",
      "1.55\n",
      "0.9500000000000001\n",
      "1.15\n",
      "1.25\n",
      "0.8500000000000001\n",
      "1.35\n",
      "0.75\n",
      "1.35\n",
      "1.35\n",
      "1.15\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.25\n",
      "1.05\n",
      "1.25\n",
      "1.05\n",
      "1.35\n",
      "0.9500000000000001\n",
      "1.35\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.05\n",
      "1.15\n",
      "1.4500000000000002\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.35\n",
      "1.25\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.4500000000000002\n",
      "1.15\n",
      "1.4500000000000002\n",
      "1.6500000000000001\n",
      "1.35\n",
      "1.25\n",
      "1.25\n",
      "1.05\n",
      "1.15\n",
      "1.25\n",
      "0.8500000000000001\n",
      "1.05\n",
      "1.55\n",
      "1.35\n",
      "1.05\n",
      "1.05\n",
      "1.4500000000000002\n",
      "1.15\n",
      "1.15\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.25\n",
      "0.9500000000000001\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.25\n",
      "1.15\n",
      "1.4500000000000002\n",
      "1.35\n",
      "0.9500000000000001\n",
      "1.25\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.4500000000000002\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.05\n",
      "1.25\n",
      "1.55\n",
      "1.15\n",
      "1.35\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.15\n",
      "1.55\n",
      "1.35\n",
      "1.55\n",
      "1.15\n",
      "1.35\n",
      "1.25\n",
      "1.35\n",
      "1.35\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.55\n",
      "1.05\n",
      "1.25\n",
      "1.35\n",
      "0.9500000000000001\n",
      "1.15\n",
      "1.15\n",
      "1.35\n",
      "1.25\n",
      "1.05\n",
      "1.15\n",
      "1.25\n",
      "1.05\n",
      "1.25\n",
      "1.35\n",
      "1.55\n",
      "1.55\n",
      "1.35\n",
      "1.15\n",
      "1.15\n",
      "1.6500000000000001\n",
      "1.15\n",
      "1.15\n",
      "1.05\n",
      "1.05\n",
      "1.05\n",
      "1.55\n",
      "1.4500000000000002\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.25\n",
      "1.15\n",
      "1.15\n",
      "1.35\n",
      "1.55\n",
      "1.35\n",
      "1.15\n",
      "1.35\n",
      "1.15\n",
      "1.4500000000000002\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.4500000000000002\n",
      "1.4500000000000002\n",
      "1.05\n",
      "1.05\n",
      "1.25\n",
      "1.4500000000000002\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.15\n",
      "0.9500000000000001\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.15\n",
      "1.35\n",
      "1.35\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.25\n",
      "1.25\n",
      "1.15\n",
      "1.05\n",
      "1.15\n",
      "0.9500000000000001\n",
      "1.05\n",
      "1.05\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.05\n",
      "1.25\n",
      "1.35\n",
      "1.25\n",
      "0.9500000000000001\n",
      "1.05\n",
      "1.55\n",
      "1.15\n",
      "1.25\n",
      "1.35\n",
      "1.15\n",
      "0.8500000000000001\n",
      "1.25\n",
      "1.25\n",
      "1.35\n",
      "1.4500000000000002\n",
      "1.55\n",
      "1.25\n",
      "1.35\n",
      "1.35\n",
      "1.35\n",
      "1.4500000000000002\n",
      "1.05\n",
      "1.35\n",
      "1.05\n",
      "0.9500000000000001\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.15\n",
      "1.05\n",
      "1.05\n",
      "1.25\n",
      "1.55\n",
      "1.4500000000000002\n",
      "1.05\n",
      "1.25\n",
      "1.05\n",
      "1.35\n",
      "1.05\n",
      "1.25\n",
      "1.35\n",
      "1.15\n",
      "1.35\n",
      "1.25\n",
      "1.05\n",
      "1.35\n",
      "0.9500000000000001\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.25\n",
      "1.25\n",
      "1.15\n",
      "1.55\n",
      "1.25\n",
      "1.25\n",
      "1.35\n",
      "1.05\n",
      "1.15\n",
      "1.05\n",
      "1.55\n",
      "1.25\n",
      "1.35\n",
      "1.15\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.55\n",
      "0.9500000000000001\n",
      "0.9500000000000001\n",
      "1.35\n",
      "1.55\n",
      "1.15\n",
      "1.05\n",
      "1.15\n",
      "1.25\n",
      "1.35\n",
      "1.05\n",
      "1.05\n",
      "1.15\n",
      "1.35\n",
      "1.15\n",
      "0.9500000000000001\n",
      "1.55\n",
      "1.25\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.15\n",
      "1.35\n",
      "1.4500000000000002\n",
      "1.05\n",
      "1.15\n",
      "0.8500000000000001\n",
      "1.25\n",
      "1.15\n",
      "1.25\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.05\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.05\n",
      "1.15\n",
      "1.15\n",
      "1.05\n",
      "1.25\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.25\n",
      "1.25\n",
      "1.25\n",
      "1.25\n",
      "1.05\n",
      "1.35\n",
      "1.15\n",
      "1.05\n",
      "1.15\n",
      "1.25\n",
      "1.15\n",
      "1.15\n",
      "1.4500000000000002\n",
      "1.15\n",
      "1.25\n",
      "0.9500000000000001\n",
      "1.25\n",
      "1.35\n",
      "1.6500000000000001\n",
      "1.15\n",
      "1.4500000000000002\n",
      "1.55\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.15\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.15\n",
      "1.35\n",
      "1.15\n",
      "1.25\n",
      "1.25\n",
      "0.9500000000000001\n",
      "1.25\n",
      "0.8500000000000001\n",
      "1.35\n",
      "1.05\n",
      "1.35\n",
      "1.35\n",
      "1.05\n",
      "1.35\n",
      "1.05\n",
      "1.05\n",
      "1.35\n",
      "1.25\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.35\n",
      "1.25\n",
      "1.35\n",
      "1.75\n",
      "1.25\n",
      "1.35\n",
      "1.25\n",
      "1.15\n",
      "1.35\n",
      "1.05\n",
      "1.15\n",
      "1.15\n",
      "0.9500000000000001\n",
      "1.35\n",
      "1.05\n",
      "1.35\n",
      "1.55\n",
      "1.35\n",
      "1.25\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.15\n",
      "1.25\n",
      "1.25\n",
      "1.05\n",
      "1.35\n",
      "1.35\n",
      "1.25\n",
      "1.15\n",
      "0.9500000000000001\n",
      "1.15\n",
      "1.25\n",
      "1.35\n",
      "1.15\n",
      "1.35\n",
      "1.55\n",
      "1.15\n",
      "1.05\n",
      "1.35\n",
      "1.15\n",
      "1.55\n",
      "1.35\n",
      "1.35\n",
      "1.25\n",
      "1.25\n",
      "1.05\n",
      "1.05\n",
      "1.25\n",
      "1.35\n",
      "0.9500000000000001\n",
      "1.05\n",
      "1.25\n",
      "1.4500000000000002\n",
      "0.8500000000000001\n",
      "1.15\n",
      "1.15\n",
      "1.35\n",
      "1.55\n",
      "1.15\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.25\n",
      "1.25\n",
      "1.15\n",
      "1.55\n",
      "1.4500000000000002\n",
      "1.05\n",
      "1.35\n",
      "1.05\n",
      "1.25\n",
      "1.35\n",
      "1.55\n",
      "1.25\n",
      "1.35\n",
      "1.35\n",
      "1.15\n",
      "1.25\n",
      "1.25\n",
      "1.05\n",
      "1.4500000000000002\n",
      "0.9500000000000001\n",
      "1.05\n",
      "1.4500000000000002\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.15\n",
      "1.55\n",
      "1.15\n",
      "1.15\n",
      "1.25\n",
      "1.05\n",
      "1.55\n",
      "1.05\n",
      "1.35\n",
      "1.55\n",
      "1.35\n",
      "1.05\n",
      "0.8500000000000001\n",
      "1.55\n",
      "1.25\n",
      "1.15\n",
      "1.05\n",
      "1.05\n",
      "1.35\n",
      "1.25\n",
      "1.4500000000000002\n",
      "1.35\n",
      "1.05\n",
      "1.25\n",
      "1.35\n",
      "1.05\n",
      "1.25\n",
      "1.4500000000000002\n",
      "0.9500000000000001\n",
      "1.35\n",
      "1.25\n",
      "1.6500000000000001\n",
      "1.4500000000000002\n",
      "1.25\n",
      "1.15\n",
      "1.35\n",
      "1.15\n",
      "1.05\n",
      "1.15\n",
      "1.15\n",
      "1.05\n",
      "1.15\n",
      "1.05\n",
      "1.15\n",
      "1.25\n",
      "1.15\n",
      "1.05\n",
      "1.05\n",
      "1.35\n",
      "1.15\n",
      "1.25\n",
      "1.35\n",
      "1.55\n",
      "1.05\n",
      "1.25\n",
      "1.35\n",
      "1.15\n",
      "1.25\n",
      "1.15\n",
      "1.15\n",
      "831.3\n",
      "10.4\n",
      "10.200000000000001\n",
      "10.4\n",
      "10.000000000000002\n",
      "10.3\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.3\n",
      "9.9\n",
      "10.4\n",
      "10.100000000000001\n",
      "10.3\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.4\n",
      "10.000000000000002\n",
      "10.4\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "9.8\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.3\n",
      "10.000000000000002\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "9.9\n",
      "9.9\n",
      "10.100000000000001\n",
      "9.9\n",
      "10.000000000000002\n",
      "9.8\n",
      "10.000000000000002\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "9.9\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "9.9\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "9.700000000000001\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "9.9\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.3\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "9.8\n",
      "10.100000000000001\n",
      "9.9\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "10.600000000000001\n",
      "10.3\n",
      "9.9\n",
      "10.4\n",
      "9.9\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "9.9\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.4\n",
      "10.3\n",
      "10.600000000000001\n",
      "10.000000000000002\n",
      "9.8\n",
      "10.100000000000001\n",
      "10.3\n",
      "10.500000000000002\n",
      "10.000000000000002\n",
      "10.3\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "9.9\n",
      "10.200000000000001\n",
      "9.700000000000001\n",
      "10.200000000000001\n",
      "9.700000000000001\n",
      "9.8\n",
      "10.4\n",
      "9.9\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.3\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "9.8\n",
      "9.9\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.500000000000002\n",
      "9.9\n",
      "10.3\n",
      "10.3\n",
      "10.000000000000002\n",
      "9.9\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.3\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.4\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "10.4\n",
      "10.100000000000001\n",
      "10.3\n",
      "9.9\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.4\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "9.9\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.3\n",
      "9.8\n",
      "10.000000000000002\n",
      "10.000000000000002\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "10.3\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "9.8\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "9.9\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "9.9\n",
      "9.9\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "9.8\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "9.8\n",
      "10.3\n",
      "10.000000000000002\n",
      "10.3\n",
      "10.000000000000002\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "9.9\n",
      "10.3\n",
      "10.200000000000001\n",
      "10.3\n",
      "10.200000000000001\n",
      "10.4\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "9.8\n",
      "10.3\n",
      "9.8\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.4\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "9.8\n",
      "10.200000000000001\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "9.700000000000001\n",
      "10.200000000000001\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.000000000000002\n",
      "10.200000000000001\n",
      "10.3\n",
      "9.9\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "10.000000000000002\n",
      "10.000000000000002\n",
      "10.100000000000001\n",
      "10.000000000000002\n",
      "10.500000000000002\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.100000000000001\n",
      "10.200000000000001\n",
      "-210.9\n",
      "9.0\n",
      "9.1\n",
      "9.1\n",
      "9.0\n",
      "9.1\n",
      "8.9\n",
      "8.799999999999999\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.299999999999999\n",
      "9.1\n",
      "9.1\n",
      "8.799999999999999\n",
      "9.0\n",
      "8.9\n",
      "9.2\n",
      "8.7\n",
      "9.0\n",
      "9.0\n",
      "9.1\n",
      "9.1\n",
      "9.2\n",
      "8.9\n",
      "9.0\n",
      "9.0\n",
      "9.2\n",
      "9.2\n",
      "9.0\n",
      "9.1\n",
      "8.799999999999999\n",
      "8.799999999999999\n",
      "9.2\n",
      "9.1\n",
      "9.1\n",
      "9.299999999999999\n",
      "9.0\n",
      "9.4\n",
      "9.299999999999999\n",
      "8.799999999999999\n",
      "9.1\n",
      "8.799999999999999\n",
      "8.9\n",
      "9.1\n",
      "8.9\n",
      "9.5\n",
      "9.0\n",
      "8.799999999999999\n",
      "9.299999999999999\n",
      "9.2\n",
      "9.1\n",
      "9.1\n",
      "9.0\n",
      "8.9\n",
      "8.9\n",
      "8.9\n",
      "8.9\n",
      "9.299999999999999\n",
      "9.0\n",
      "9.299999999999999\n",
      "9.1\n",
      "8.799999999999999\n",
      "9.0\n",
      "8.9\n",
      "9.0\n",
      "9.299999999999999\n",
      "9.0\n",
      "8.9\n",
      "9.1\n",
      "9.2\n",
      "9.299999999999999\n",
      "8.9\n",
      "9.1\n",
      "9.0\n",
      "8.6\n",
      "9.2\n",
      "9.2\n",
      "9.299999999999999\n",
      "9.0\n",
      "9.1\n",
      "9.1\n",
      "8.799999999999999\n",
      "9.1\n",
      "8.5\n",
      "8.6\n",
      "8.9\n",
      "9.0\n",
      "8.7\n",
      "9.1\n",
      "9.0\n",
      "9.1\n",
      "9.1\n",
      "8.6\n",
      "9.0\n",
      "9.2\n",
      "9.2\n",
      "9.0\n",
      "9.1\n",
      "9.1\n",
      "9.1\n",
      "8.9\n",
      "9.0\n",
      "8.9\n",
      "9.2\n",
      "8.9\n",
      "8.7\n",
      "9.1\n",
      "9.1\n",
      "9.0\n",
      "8.9\n",
      "8.9\n",
      "9.0\n",
      "8.799999999999999\n",
      "9.2\n",
      "9.1\n",
      "9.0\n",
      "9.1\n",
      "9.1\n",
      "8.799999999999999\n",
      "8.799999999999999\n",
      "8.9\n",
      "8.7\n",
      "9.0\n",
      "8.799999999999999\n",
      "9.0\n",
      "8.9\n",
      "8.799999999999999\n",
      "8.9\n",
      "8.9\n",
      "9.0\n",
      "8.799999999999999\n",
      "8.9\n",
      "9.1\n",
      "9.0\n",
      "8.7\n",
      "9.2\n",
      "8.9\n",
      "8.799999999999999\n",
      "9.1\n",
      "9.0\n",
      "8.7\n",
      "9.0\n",
      "8.799999999999999\n",
      "8.9\n",
      "9.1\n",
      "9.1\n",
      "9.1\n",
      "8.6\n",
      "9.299999999999999\n",
      "9.0\n",
      "9.2\n",
      "9.2\n",
      "8.9\n",
      "8.9\n",
      "9.0\n",
      "9.1\n",
      "8.799999999999999\n",
      "8.799999999999999\n",
      "8.9\n",
      "9.1\n",
      "9.299999999999999\n",
      "8.9\n",
      "9.0\n",
      "9.299999999999999\n",
      "8.799999999999999\n",
      "9.2\n",
      "9.2\n",
      "9.2\n",
      "9.0\n",
      "9.1\n",
      "9.299999999999999\n",
      "8.5\n",
      "9.1\n",
      "8.7\n",
      "9.0\n",
      "8.9\n",
      "8.799999999999999\n",
      "9.0\n",
      "9.0\n",
      "9.1\n",
      "9.0\n",
      "9.0\n",
      "9.1\n",
      "8.799999999999999\n",
      "8.799999999999999\n",
      "9.0\n",
      "9.0\n",
      "9.299999999999999\n",
      "8.9\n",
      "9.0\n",
      "8.7\n",
      "9.2\n",
      "8.9\n",
      "9.1\n",
      "8.9\n",
      "8.9\n",
      "8.9\n",
      "9.0\n",
      "8.7\n",
      "9.0\n",
      "9.2\n",
      "9.4\n",
      "9.1\n",
      "8.799999999999999\n",
      "8.9\n",
      "9.4\n",
      "9.4\n",
      "8.9\n",
      "8.9\n",
      "9.0\n",
      "9.0\n",
      "8.9\n",
      "9.0\n",
      "9.1\n",
      "8.9\n",
      "8.9\n",
      "9.2\n",
      "9.1\n",
      "9.0\n",
      "8.9\n",
      "9.0\n",
      "8.9\n",
      "9.299999999999999\n",
      "8.9\n",
      "8.9\n",
      "8.9\n",
      "9.1\n",
      "8.9\n",
      "9.0\n",
      "9.0\n",
      "8.7\n",
      "9.0\n",
      "9.2\n",
      "9.299999999999999\n",
      "9.4\n",
      "9.1\n",
      "8.799999999999999\n",
      "8.799999999999999\n",
      "8.6\n",
      "9.1\n",
      "8.9\n",
      "9.2\n",
      "8.799999999999999\n",
      "9.1\n",
      "8.9\n",
      "-232.1\n",
      "8.0\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "7.6000000000000005\n",
      "7.9\n",
      "7.800000000000001\n",
      "8.0\n",
      "7.5\n",
      "7.5\n",
      "8.0\n",
      "7.800000000000001\n",
      "7.7\n",
      "7.6000000000000005\n",
      "7.7\n",
      "7.9\n",
      "7.7\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "7.9\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "8.1\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "7.7\n",
      "7.9\n",
      "7.9\n",
      "7.9\n",
      "7.7\n",
      "7.7\n",
      "7.800000000000001\n",
      "7.6000000000000005\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "7.9\n",
      "7.800000000000001\n",
      "8.0\n",
      "7.7\n",
      "8.0\n",
      "7.9\n",
      "7.6000000000000005\n",
      "7.5\n",
      "7.6000000000000005\n",
      "7.7\n",
      "7.5\n",
      "7.7\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "7.9\n",
      "7.7\n",
      "7.9\n",
      "7.5\n",
      "7.9\n",
      "7.7\n",
      "7.800000000000001\n",
      "7.6000000000000005\n",
      "7.6000000000000005\n",
      "7.9\n",
      "7.4\n",
      "7.800000000000001\n",
      "7.6000000000000005\n",
      "7.800000000000001\n",
      "7.7\n",
      "7.5\n",
      "7.800000000000001\n",
      "7.7\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "8.0\n",
      "7.800000000000001\n",
      "7.6000000000000005\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "7.6000000000000005\n",
      "7.7\n",
      "7.9\n",
      "8.1\n",
      "7.7\n",
      "7.5\n",
      "7.9\n",
      "7.800000000000001\n",
      "7.6000000000000005\n",
      "8.200000000000001\n",
      "7.7\n",
      "7.7\n",
      "8.1\n",
      "7.6000000000000005\n",
      "7.7\n",
      "7.7\n",
      "7.7\n",
      "8.0\n",
      "7.9\n",
      "7.7\n",
      "8.0\n",
      "7.800000000000001\n",
      "7.9\n",
      "7.9\n",
      "8.1\n",
      "7.6000000000000005\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "7.9\n",
      "7.9\n",
      "7.800000000000001\n",
      "7.9\n",
      "7.7\n",
      "7.7\n",
      "7.9\n",
      "7.800000000000001\n",
      "8.0\n",
      "7.800000000000001\n",
      "7.7\n",
      "7.7\n",
      "7.800000000000001\n",
      "8.200000000000001\n",
      "7.6000000000000005\n",
      "7.9\n",
      "8.0\n",
      "7.7\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "7.7\n",
      "7.800000000000001\n",
      "7.6000000000000005\n",
      "7.800000000000001\n",
      "7.9\n",
      "7.7\n",
      "7.800000000000001\n",
      "7.7\n",
      "7.9\n",
      "7.800000000000001\n",
      "7.7\n",
      "8.0\n",
      "7.800000000000001\n",
      "7.800000000000001\n",
      "7.7\n",
      "7.9\n",
      "7.7\n",
      "8.0\n",
      "8.0\n",
      "7.800000000000001\n",
      "8.1\n",
      "8.0\n",
      "7.7\n",
      "7.7\n",
      "7.800000000000001\n",
      "7.5\n",
      "7.7\n",
      "8.0\n",
      "7.5\n",
      "7.9\n",
      "7.9\n",
      "7.7\n",
      "7.6000000000000005\n",
      "8.0\n",
      "8.0\n",
      "7.800000000000001\n",
      "7.6000000000000005\n",
      "8.0\n",
      "7.6000000000000005\n",
      "7.9\n",
      "7.9\n",
      "7.7\n",
      "7.6000000000000005\n",
      "7.9\n",
      "7.800000000000001\n",
      "7.7\n",
      "8.1\n",
      "7.7\n",
      "7.800000000000001\n",
      "7.4\n",
      "7.9\n",
      "7.800000000000001\n",
      "7.6000000000000005\n",
      "7.6000000000000005\n",
      "8.0\n",
      "7.800000000000001\n",
      "7.7\n",
      "7.9\n",
      "7.800000000000001\n",
      "7.9\n",
      "8.200000000000001\n",
      "7.5\n",
      "7.800000000000001\n",
      "8.200000000000001\n",
      "7.7\n",
      "7.9\n",
      "-223.45\n",
      "6.550000000000001\n",
      "6.750000000000001\n",
      "7.55\n",
      "7.95\n",
      "7.65\n",
      "7.95\n",
      "7.85\n",
      "7.65\n",
      "7.65\n",
      "7.75\n",
      "7.65\n",
      "7.75\n",
      "7.65\n",
      "7.75\n",
      "7.85\n",
      "7.55\n",
      "7.65\n",
      "7.75\n",
      "7.95\n",
      "7.35\n",
      "7.65\n",
      "7.65\n",
      "7.75\n",
      "7.45\n",
      "7.75\n",
      "7.75\n",
      "7.65\n",
      "7.45\n",
      "7.65\n",
      "7.65\n",
      "7.55\n",
      "7.85\n",
      "7.65\n",
      "7.65\n",
      "8.05\n",
      "7.65\n",
      "7.65\n",
      "7.75\n",
      "7.75\n",
      "7.45\n",
      "7.75\n",
      "7.75\n",
      "7.65\n",
      "7.85\n",
      "7.65\n",
      "7.55\n",
      "7.55\n",
      "7.85\n",
      "7.65\n",
      "7.45\n",
      "7.65\n",
      "7.75\n",
      "7.95\n",
      "7.65\n",
      "7.45\n",
      "7.75\n",
      "7.65\n",
      "7.55\n",
      "7.65\n",
      "7.75\n",
      "7.25\n",
      "7.75\n",
      "8.05\n",
      "7.45\n",
      "7.65\n",
      "7.95\n",
      "7.55\n",
      "7.65\n",
      "7.55\n",
      "7.75\n",
      "7.55\n",
      "7.55\n",
      "7.55\n",
      "7.75\n",
      "7.85\n",
      "7.85\n",
      "7.65\n",
      "7.75\n",
      "7.65\n",
      "7.75\n",
      "7.55\n",
      "7.65\n",
      "7.55\n",
      "7.85\n",
      "7.45\n",
      "7.85\n",
      "7.45\n",
      "7.75\n",
      "7.65\n",
      "7.45\n",
      "7.65\n",
      "7.85\n",
      "7.25\n",
      "7.55\n",
      "7.95\n",
      "7.65\n",
      "7.75\n",
      "7.55\n",
      "7.45\n",
      "7.55\n",
      "7.75\n",
      "7.75\n",
      "7.55\n",
      "7.35\n",
      "7.55\n",
      "7.85\n",
      "7.45\n",
      "7.65\n",
      "7.95\n",
      "7.55\n",
      "7.85\n",
      "7.35\n",
      "7.65\n",
      "7.55\n",
      "7.55\n",
      "7.55\n",
      "7.65\n",
      "7.35\n",
      "7.65\n",
      "7.65\n",
      "7.85\n",
      "7.55\n",
      "7.55\n",
      "7.65\n",
      "7.75\n",
      "7.85\n",
      "7.35\n",
      "7.75\n",
      "7.85\n",
      "7.55\n",
      "7.85\n",
      "7.75\n",
      "7.55\n",
      "7.65\n",
      "7.55\n",
      "7.45\n",
      "7.95\n",
      "7.75\n",
      "7.85\n",
      "7.75\n",
      "7.35\n",
      "7.45\n",
      "7.55\n",
      "7.65\n",
      "7.75\n",
      "7.75\n",
      "7.95\n",
      "7.85\n",
      "7.65\n",
      "7.65\n",
      "7.45\n",
      "7.65\n",
      "7.55\n",
      "7.65\n",
      "7.75\n",
      "7.75\n",
      "7.65\n",
      "7.55\n",
      "7.65\n",
      "7.55\n",
      "7.65\n",
      "7.65\n",
      "7.65\n",
      "7.45\n",
      "7.85\n",
      "7.35\n",
      "7.85\n",
      "7.55\n",
      "7.95\n",
      "7.75\n",
      "7.75\n",
      "7.65\n",
      "7.65\n",
      "7.55\n",
      "7.65\n",
      "7.95\n",
      "7.85\n",
      "7.65\n",
      "7.85\n",
      "7.65\n",
      "7.75\n",
      "7.65\n",
      "7.55\n",
      "7.45\n",
      "7.85\n",
      "7.45\n",
      "7.85\n",
      "7.65\n",
      "7.65\n",
      "7.45\n",
      "7.35\n",
      "7.65\n",
      "7.35\n",
      "7.55\n",
      "7.35\n",
      "7.95\n",
      "7.55\n",
      "7.75\n",
      "7.75\n",
      "7.85\n",
      "7.55\n",
      "7.45\n",
      "7.65\n",
      "7.45\n",
      "7.65\n",
      "7.55\n",
      "7.95\n",
      "7.75\n",
      "7.85\n",
      "7.65\n",
      "7.45\n",
      "7.55\n",
      "7.55\n",
      "7.85\n",
      "7.65\n",
      "7.75\n",
      "7.55\n",
      "7.65\n",
      "7.45\n",
      "7.65\n",
      "8.05\n",
      "7.85\n",
      "7.65\n",
      "7.65\n",
      "7.55\n",
      "7.45\n",
      "7.35\n",
      "7.65\n",
      "7.85\n",
      "7.75\n",
      "7.45\n",
      "7.75\n",
      "7.45\n",
      "7.55\n",
      "7.55\n",
      "7.25\n",
      "7.55\n",
      "7.75\n",
      "7.55\n",
      "7.25\n",
      "7.45\n",
      "7.75\n",
      "7.65\n",
      "7.55\n",
      "7.65\n",
      "7.55\n",
      "-334.25000000000006\n",
      "6.250000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "5.750000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "5.8500000000000005\n",
      "5.750000000000001\n",
      "6.15\n",
      "6.15\n",
      "6.15\n",
      "5.950000000000001\n",
      "5.65\n",
      "6.250000000000001\n",
      "6.050000000000001\n",
      "5.750000000000001\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "5.8500000000000005\n",
      "6.3500000000000005\n",
      "6.050000000000001\n",
      "6.15\n",
      "6.050000000000001\n",
      "5.750000000000001\n",
      "6.050000000000001\n",
      "5.65\n",
      "5.65\n",
      "6.250000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "5.65\n",
      "6.3500000000000005\n",
      "5.950000000000001\n",
      "5.65\n",
      "5.8500000000000005\n",
      "5.550000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "6.3500000000000005\n",
      "5.950000000000001\n",
      "6.15\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "6.250000000000001\n",
      "6.050000000000001\n",
      "5.65\n",
      "6.15\n",
      "6.15\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "6.250000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "6.15\n",
      "6.15\n",
      "6.15\n",
      "5.750000000000001\n",
      "5.950000000000001\n",
      "5.750000000000001\n",
      "5.8500000000000005\n",
      "5.750000000000001\n",
      "6.3500000000000005\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "6.15\n",
      "5.8500000000000005\n",
      "6.15\n",
      "6.250000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "5.8500000000000005\n",
      "5.8500000000000005\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "6.250000000000001\n",
      "5.550000000000001\n",
      "5.8500000000000005\n",
      "6.15\n",
      "5.750000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "6.250000000000001\n",
      "6.050000000000001\n",
      "5.65\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "6.15\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "5.65\n",
      "5.950000000000001\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "5.750000000000001\n",
      "6.050000000000001\n",
      "5.750000000000001\n",
      "5.950000000000001\n",
      "6.15\n",
      "5.950000000000001\n",
      "5.65\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "6.250000000000001\n",
      "5.950000000000001\n",
      "6.250000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "6.15\n",
      "5.750000000000001\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "6.15\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "6.15\n",
      "6.15\n",
      "6.050000000000001\n",
      "5.750000000000001\n",
      "6.050000000000001\n",
      "5.750000000000001\n",
      "6.15\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "5.750000000000001\n",
      "5.950000000000001\n",
      "6.15\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "5.950000000000001\n",
      "6.15\n",
      "5.65\n",
      "5.65\n",
      "5.750000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "6.15\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "6.15\n",
      "5.65\n",
      "6.250000000000001\n",
      "5.950000000000001\n",
      "5.65\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "6.15\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "6.15\n",
      "5.8500000000000005\n",
      "5.8500000000000005\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "5.950000000000001\n",
      "6.250000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "6.3500000000000005\n",
      "6.050000000000001\n",
      "5.750000000000001\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "6.15\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "5.550000000000001\n",
      "5.950000000000001\n",
      "5.550000000000001\n",
      "5.950000000000001\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.750000000000001\n",
      "5.950000000000001\n",
      "5.950000000000001\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.65\n",
      "6.250000000000001\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "5.750000000000001\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "6.15\n",
      "5.950000000000001\n",
      "5.65\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "6.15\n",
      "5.550000000000001\n",
      "6.050000000000001\n",
      "6.15\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "6.15\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.750000000000001\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "6.15\n",
      "5.8500000000000005\n",
      "6.15\n",
      "5.750000000000001\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "5.65\n",
      "6.050000000000001\n",
      "5.65\n",
      "5.750000000000001\n",
      "6.050000000000001\n",
      "6.250000000000001\n",
      "6.15\n",
      "6.250000000000001\n",
      "5.450000000000001\n",
      "6.050000000000001\n",
      "5.3500000000000005\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "6.15\n",
      "6.15\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "6.050000000000001\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "5.950000000000001\n",
      "5.750000000000001\n",
      "6.250000000000001\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "5.750000000000001\n",
      "6.15\n",
      "5.8500000000000005\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "6.15\n",
      "6.050000000000001\n",
      "5.950000000000001\n",
      "5.8500000000000005\n",
      "5.750000000000001\n",
      "6.15\n",
      "5.550000000000001\n",
      "5.750000000000001\n",
      "5.8500000000000005\n",
      "5.8500000000000005\n",
      "5.65\n",
      "5.8500000000000005\n",
      "6.15\n",
      "5.65\n",
      "6.050000000000001\n",
      "5.8500000000000005\n",
      "6.15\n",
      "6.050000000000001\n",
      "6.250000000000001\n",
      "6.15\n",
      "5.950000000000001\n",
      "6.050000000000001\n",
      "6.250000000000001\n",
      "5.8500000000000005\n",
      "-235.25\n",
      "4.85\n",
      "4.949999999999999\n",
      "4.449999999999999\n",
      "4.949999999999999\n",
      "4.75\n",
      "4.85\n",
      "4.6499999999999995\n",
      "4.85\n",
      "5.05\n",
      "4.55\n",
      "4.449999999999999\n",
      "4.449999999999999\n",
      "4.35\n",
      "4.85\n",
      "4.949999999999999\n",
      "4.75\n",
      "5.25\n",
      "4.85\n",
      "4.85\n",
      "4.75\n",
      "4.75\n",
      "4.85\n",
      "4.949999999999999\n",
      "4.6499999999999995\n",
      "4.6499999999999995\n",
      "4.949999999999999\n",
      "4.55\n",
      "4.75\n",
      "4.85\n",
      "4.949999999999999\n",
      "4.6499999999999995\n",
      "4.55\n",
      "4.85\n",
      "4.55\n",
      "4.449999999999999\n",
      "5.05\n",
      "4.6499999999999995\n",
      "4.949999999999999\n",
      "4.55\n",
      "4.75\n",
      "4.75\n",
      "4.85\n",
      "4.55\n",
      "4.85\n",
      "4.55\n",
      "4.85\n",
      "4.55\n",
      "4.75\n",
      "4.85\n",
      "4.85\n",
      "4.55\n",
      "4.6499999999999995\n",
      "4.55\n",
      "4.85\n",
      "4.85\n",
      "4.6499999999999995\n",
      "4.85\n",
      "4.75\n",
      "4.55\n",
      "4.75\n",
      "4.85\n",
      "4.6499999999999995\n",
      "4.6499999999999995\n",
      "4.6499999999999995\n",
      "4.85\n",
      "4.949999999999999\n",
      "4.6499999999999995\n",
      "4.75\n",
      "4.75\n",
      "4.85\n",
      "4.75\n",
      "4.6499999999999995\n",
      "4.55\n",
      "4.35\n",
      "4.55\n",
      "4.75\n",
      "4.55\n",
      "4.949999999999999\n",
      "4.75\n",
      "4.85\n",
      "4.6499999999999995\n",
      "5.05\n",
      "5.05\n",
      "4.449999999999999\n",
      "4.6499999999999995\n",
      "4.6499999999999995\n",
      "4.85\n",
      "4.6499999999999995\n",
      "4.85\n",
      "4.55\n",
      "4.55\n",
      "4.75\n",
      "4.449999999999999\n",
      "4.6499999999999995\n",
      "4.55\n",
      "4.6499999999999995\n",
      "4.75\n",
      "4.6499999999999995\n",
      "4.55\n",
      "4.85\n",
      "4.85\n",
      "4.75\n",
      "5.25\n",
      "4.6499999999999995\n",
      "4.949999999999999\n",
      "4.6499999999999995\n",
      "4.6499999999999995\n",
      "4.55\n",
      "4.75\n",
      "4.85\n",
      "4.75\n",
      "4.6499999999999995\n",
      "4.949999999999999\n",
      "4.55\n",
      "4.75\n",
      "4.85\n",
      "4.55\n",
      "4.75\n",
      "4.6499999999999995\n",
      "4.75\n",
      "4.6499999999999995\n",
      "4.949999999999999\n",
      "5.05\n",
      "4.85\n",
      "4.75\n",
      "33.549999999999976\n",
      "3.65\n",
      "3.65\n",
      "4.05\n",
      "3.75\n",
      "3.45\n",
      "3.65\n",
      "3.95\n",
      "3.55\n",
      "3.45\n",
      "3.85\n",
      "3.55\n",
      "3.75\n",
      "3.55\n",
      "3.65\n",
      "3.45\n",
      "3.85\n",
      "3.55\n",
      "3.75\n",
      "3.35\n",
      "3.55\n",
      "3.55\n",
      "3.55\n",
      "3.65\n",
      "3.65\n",
      "3.65\n",
      "3.55\n",
      "3.25\n",
      "3.75\n",
      "3.25\n",
      "3.95\n",
      "3.75\n",
      "3.55\n",
      "3.25\n",
      "3.95\n",
      "3.35\n",
      "3.65\n",
      "3.95\n",
      "3.75\n",
      "3.45\n",
      "3.95\n",
      "3.65\n",
      "3.65\n",
      "3.55\n",
      "3.65\n",
      "3.75\n",
      "3.45\n",
      "3.75\n",
      "3.65\n",
      "3.55\n",
      "3.65\n",
      "3.65\n",
      "3.35\n",
      "3.85\n",
      "3.55\n",
      "3.45\n",
      "3.65\n",
      "3.45\n",
      "3.85\n",
      "3.65\n",
      "3.55\n",
      "3.65\n",
      "3.65\n",
      "3.65\n",
      "3.65\n",
      "3.65\n",
      "3.45\n",
      "3.55\n",
      "3.65\n",
      "3.95\n",
      "3.75\n",
      "3.55\n",
      "3.45\n",
      "3.25\n",
      "3.95\n",
      "3.75\n",
      "3.65\n",
      "3.75\n",
      "3.65\n",
      "3.85\n",
      "3.85\n",
      "3.75\n",
      "3.55\n",
      "3.75\n",
      "3.45\n",
      "3.55\n",
      "3.85\n",
      "3.95\n",
      "3.35\n",
      "3.75\n",
      "3.65\n",
      "3.75\n",
      "3.65\n",
      "3.65\n",
      "3.65\n",
      "3.35\n",
      "3.75\n",
      "3.55\n",
      "3.55\n",
      "3.75\n",
      "3.95\n",
      "3.55\n",
      "3.75\n",
      "3.65\n",
      "3.75\n",
      "3.55\n",
      "3.75\n",
      "3.75\n",
      "3.65\n",
      "3.65\n",
      "3.65\n",
      "3.55\n",
      "3.55\n",
      "4.05\n",
      "3.65\n",
      "3.85\n",
      "3.75\n",
      "3.65\n",
      "4.05\n",
      "3.55\n",
      "3.55\n",
      "3.75\n",
      "3.85\n",
      "3.95\n",
      "3.45\n",
      "3.85\n",
      "3.95\n",
      "3.65\n",
      "3.35\n",
      "3.55\n",
      "3.95\n",
      "3.65\n",
      "3.45\n",
      "3.65\n",
      "3.55\n",
      "3.65\n",
      "3.65\n",
      "3.95\n",
      "3.25\n",
      "3.65\n",
      "3.75\n",
      "3.85\n",
      "3.85\n",
      "3.55\n",
      "3.65\n",
      "3.75\n",
      "3.85\n",
      "3.75\n",
      "3.65\n",
      "3.65\n",
      "3.45\n",
      "4.05\n",
      "3.55\n",
      "3.85\n",
      "3.75\n",
      "3.55\n",
      "3.55\n",
      "3.45\n",
      "3.65\n",
      "3.65\n",
      "3.75\n",
      "3.45\n",
      "3.65\n",
      "3.45\n",
      "3.65\n",
      "3.55\n",
      "3.55\n",
      "3.65\n",
      "3.65\n",
      "3.75\n",
      "3.65\n",
      "3.75\n",
      "3.65\n",
      "3.55\n",
      "3.75\n",
      "3.65\n",
      "3.45\n",
      "3.85\n",
      "3.85\n",
      "3.55\n",
      "293.75\n",
      "4.45\n",
      "4.25\n",
      "4.15\n",
      "4.35\n",
      "4.15\n",
      "4.05\n",
      "3.95\n",
      "3.85\n",
      "4.05\n",
      "3.95\n",
      "4.15\n",
      "4.35\n",
      "4.25\n",
      "4.05\n",
      "4.05\n",
      "4.35\n",
      "4.15\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "4.25\n",
      "4.05\n",
      "4.15\n",
      "4.35\n",
      "4.35\n",
      "4.05\n",
      "3.95\n",
      "4.35\n",
      "4.55\n",
      "4.35\n",
      "4.05\n",
      "4.35\n",
      "4.25\n",
      "4.25\n",
      "4.25\n",
      "4.35\n",
      "4.35\n",
      "4.45\n",
      "3.95\n",
      "3.85\n",
      "4.05\n",
      "4.25\n",
      "4.15\n",
      "3.75\n",
      "3.85\n",
      "3.95\n",
      "4.15\n",
      "3.95\n",
      "3.95\n",
      "4.05\n",
      "3.95\n",
      "4.15\n",
      "4.05\n",
      "4.15\n",
      "3.95\n",
      "4.15\n",
      "4.15\n",
      "4.05\n",
      "4.15\n",
      "4.55\n",
      "4.25\n",
      "4.15\n",
      "4.15\n",
      "4.15\n",
      "3.95\n",
      "4.25\n",
      "4.35\n",
      "4.35\n",
      "3.85\n",
      "4.15\n",
      "4.05\n",
      "4.35\n",
      "4.25\n",
      "4.25\n",
      "4.05\n",
      "4.05\n",
      "4.15\n",
      "4.15\n",
      "4.45\n",
      "4.15\n",
      "4.15\n",
      "3.95\n",
      "4.15\n",
      "3.85\n",
      "4.15\n",
      "4.25\n",
      "3.85\n",
      "4.45\n",
      "4.05\n",
      "4.35\n",
      "4.05\n",
      "4.15\n",
      "4.35\n",
      "4.25\n",
      "4.45\n",
      "4.35\n",
      "4.05\n",
      "3.75\n",
      "3.95\n",
      "4.15\n",
      "3.95\n",
      "3.75\n",
      "4.15\n",
      "3.85\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "4.35\n",
      "4.45\n",
      "4.25\n",
      "3.95\n",
      "3.85\n",
      "4.15\n",
      "4.05\n",
      "3.95\n",
      "4.05\n",
      "3.95\n",
      "4.15\n",
      "4.25\n",
      "3.95\n",
      "4.25\n",
      "4.25\n",
      "4.25\n",
      "4.25\n",
      "3.95\n",
      "3.95\n",
      "4.35\n",
      "3.95\n",
      "4.05\n",
      "4.15\n",
      "4.25\n",
      "4.25\n",
      "3.85\n",
      "4.05\n",
      "4.05\n",
      "4.45\n",
      "4.15\n",
      "4.45\n",
      "3.75\n",
      "4.45\n",
      "4.15\n",
      "4.05\n",
      "4.05\n",
      "4.05\n",
      "4.15\n",
      "4.35\n",
      "4.45\n",
      "4.45\n",
      "3.65\n",
      "4.05\n",
      "4.15\n",
      "4.15\n",
      "4.15\n",
      "4.15\n",
      "4.25\n",
      "4.05\n",
      "3.95\n",
      "4.05\n",
      "4.05\n",
      "4.35\n",
      "4.25\n",
      "4.35\n",
      "4.25\n",
      "4.15\n",
      "4.35\n",
      "4.15\n",
      "4.15\n",
      "4.35\n",
      "3.95\n",
      "4.35\n",
      "4.25\n",
      "4.35\n",
      "4.05\n",
      "4.15\n",
      "4.15\n",
      "4.15\n",
      "4.35\n",
      "4.35\n",
      "4.15\n",
      "4.05\n",
      "4.05\n",
      "4.05\n",
      "4.25\n",
      "4.05\n",
      "3.95\n",
      "4.05\n",
      "4.05\n",
      "4.05\n",
      "4.35\n",
      "4.25\n",
      "3.95\n",
      "4.15\n",
      "4.05\n",
      "3.85\n",
      "4.05\n",
      "4.25\n",
      "4.35\n",
      "4.25\n",
      "4.55\n",
      "4.05\n",
      "4.15\n",
      "4.05\n",
      "4.05\n",
      "3.95\n",
      "3.95\n",
      "4.15\n",
      "4.35\n",
      "4.25\n",
      "3.95\n",
      "4.05\n",
      "4.15\n",
      "4.35\n",
      "4.05\n",
      "3.95\n",
      "4.25\n",
      "4.05\n",
      "4.05\n",
      "4.25\n",
      "4.45\n",
      "4.25\n",
      "4.35\n",
      "4.45\n",
      "4.15\n",
      "4.25\n",
      "4.15\n",
      "4.05\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "4.15\n",
      "4.05\n",
      "4.15\n",
      "4.05\n",
      "4.25\n",
      "4.05\n",
      "4.35\n",
      "4.45\n",
      "3.85\n",
      "4.25\n",
      "4.05\n",
      "4.05\n",
      "4.15\n",
      "4.15\n",
      "4.15\n",
      "4.25\n",
      "4.05\n",
      "4.15\n",
      "4.05\n",
      "4.25\n",
      "4.05\n",
      "4.15\n",
      "3.85\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "3.95\n",
      "3.95\n",
      "4.35\n",
      "4.15\n",
      "3.95\n",
      "4.35\n",
      "4.25\n",
      "4.45\n",
      "4.15\n",
      "4.15\n",
      "4.45\n",
      "4.35\n",
      "3.85\n",
      "4.25\n",
      "4.05\n",
      "4.55\n",
      "4.15\n",
      "4.25\n",
      "4.25\n",
      "4.35\n",
      "4.35\n",
      "4.35\n",
      "4.15\n",
      "4.25\n",
      "4.35\n",
      "4.35\n",
      "4.35\n",
      "4.35\n",
      "4.45\n",
      "3.75\n",
      "4.25\n",
      "4.15\n",
      "4.05\n",
      "4.15\n",
      "4.05\n",
      "4.45\n",
      "4.05\n",
      "4.15\n",
      "4.15\n",
      "3.95\n",
      "3.95\n",
      "4.15\n",
      "3.85\n",
      "4.55\n",
      "3.85\n",
      "3.75\n",
      "4.35\n",
      "4.45\n",
      "4.25\n",
      "4.35\n",
      "4.25\n",
      "4.15\n",
      "3.95\n",
      "4.45\n",
      "4.35\n",
      "4.45\n",
      "4.15\n",
      "3.85\n",
      "4.15\n",
      "4.05\n",
      "3.85\n",
      "4.35\n",
      "4.15\n",
      "4.25\n",
      "4.15\n",
      "4.55\n",
      "4.45\n",
      "4.25\n",
      "3.95\n",
      "3.95\n",
      "3.85\n",
      "4.25\n",
      "4.35\n",
      "4.45\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "4.05\n",
      "3.85\n",
      "4.25\n",
      "4.35\n",
      "4.35\n",
      "4.15\n",
      "4.05\n",
      "4.25\n",
      "4.05\n",
      "4.25\n",
      "4.15\n",
      "4.25\n",
      "4.25\n",
      "3.95\n",
      "4.15\n",
      "4.15\n",
      "3.95\n",
      "4.25\n",
      "4.15\n",
      "4.35\n",
      "4.05\n",
      "4.15\n",
      "4.25\n",
      "4.15\n",
      "4.05\n",
      "4.15\n",
      "4.05\n",
      "4.35\n",
      "4.35\n",
      "4.25\n",
      "4.05\n",
      "3.95\n",
      "4.55\n",
      "4.45\n",
      "4.15\n",
      "4.25\n",
      "3.75\n",
      "4.15\n",
      "4.35\n",
      "4.05\n",
      "4.35\n",
      "4.35\n",
      "4.05\n",
      "4.05\n",
      "4.15\n",
      "4.25\n",
      "3.95\n",
      "4.25\n",
      "4.25\n",
      "4.05\n",
      "4.15\n",
      "4.35\n",
      "4.05\n",
      "4.25\n",
      "4.25\n",
      "4.05\n",
      "4.35\n",
      "4.05\n",
      "3.85\n",
      "4.25\n",
      "4.35\n",
      "3.95\n",
      "4.15\n",
      "4.05\n",
      "4.35\n",
      "4.15\n",
      "4.35\n",
      "4.25\n",
      "4.45\n",
      "4.15\n",
      "4.05\n",
      "4.25\n",
      "4.25\n",
      "4.25\n",
      "4.35\n",
      "4.05\n",
      "4.05\n",
      "4.05\n",
      "4.05\n",
      "4.45\n",
      "4.25\n",
      "4.05\n",
      "4.25\n",
      "4.05\n",
      "3.95\n",
      "4.15\n",
      "3.95\n",
      "3.95\n",
      "4.15\n",
      "4.15\n",
      "4.15\n",
      "4.05\n",
      "4.05\n",
      "4.25\n",
      "4.05\n",
      "4.45\n",
      "3.85\n",
      "3.95\n",
      "4.35\n",
      "4.25\n",
      "4.05\n",
      "4.35\n",
      "3.95\n",
      "4.15\n",
      "4.25\n",
      "3.95\n",
      "4.15\n",
      "4.15\n",
      "4.15\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "4.25\n",
      "4.45\n",
      "4.05\n",
      "4.15\n",
      "3.95\n",
      "4.45\n",
      "4.05\n",
      "3.95\n",
      "4.25\n",
      "3.95\n",
      "4.25\n",
      "4.15\n",
      "4.25\n",
      "3.95\n",
      "4.45\n",
      "4.15\n",
      "4.25\n",
      "4.05\n",
      "4.35\n",
      "4.05\n",
      "4.15\n",
      "4.35\n",
      "4.15\n",
      "4.15\n",
      "4.05\n",
      "4.05\n",
      "4.05\n",
      "4.25\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "3.95\n",
      "4.45\n",
      "4.35\n",
      "4.25\n",
      "4.25\n",
      "3.95\n",
      "4.15\n",
      "4.25\n",
      "4.15\n",
      "4.15\n",
      "4.05\n",
      "4.25\n",
      "4.15\n",
      "3.95\n",
      "3.95\n",
      "4.15\n",
      "4.25\n",
      "3.85\n",
      "4.35\n",
      "4.15\n",
      "3.75\n",
      "3.75\n",
      "4.15\n",
      "4.15\n",
      "4.05\n",
      "4.15\n",
      "4.25\n",
      "4.15\n",
      "4.05\n",
      "3.95\n",
      "4.35\n",
      "4.05\n",
      "3.85\n",
      "4.45\n",
      "4.35\n",
      "4.25\n",
      "4.25\n",
      "4.05\n",
      "3.95\n",
      "4.35\n",
      "4.05\n",
      "4.25\n",
      "4.35\n",
      "4.15\n",
      "4.05\n",
      "3.95\n",
      "3.85\n",
      "3.85\n",
      "4.25\n",
      "3.75\n",
      "4.05\n",
      "4.15\n",
      "3.95\n",
      "4.15\n",
      "4.45\n",
      "4.35\n",
      "4.05\n",
      "4.55\n",
      "4.25\n",
      "4.35\n",
      "3.95\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "4.15\n",
      "4.45\n",
      "4.15\n",
      "4.15\n",
      "3.85\n",
      "3.75\n",
      "4.55\n",
      "3.95\n",
      "-116.64999999999999\n",
      "3.5500000000000003\n",
      "3.8500000000000005\n",
      "3.45\n",
      "3.6500000000000004\n",
      "3.5500000000000003\n",
      "3.5500000000000003\n",
      "3.3500000000000005\n",
      "3.5500000000000003\n",
      "3.7500000000000004\n",
      "3.7500000000000004\n",
      "3.95\n",
      "3.5500000000000003\n",
      "3.6500000000000004\n",
      "3.45\n",
      "3.45\n",
      "3.2500000000000004\n",
      "3.3500000000000005\n",
      "3.45\n",
      "3.45\n",
      "3.5500000000000003\n",
      "3.8500000000000005\n",
      "3.7500000000000004\n",
      "3.5500000000000003\n",
      "3.7500000000000004\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.2500000000000004\n",
      "3.45\n",
      "3.3500000000000005\n",
      "3.2500000000000004\n",
      "3.45\n",
      "3.7500000000000004\n",
      "3.5500000000000003\n",
      "3.3500000000000005\n",
      "3.2500000000000004\n",
      "3.3500000000000005\n",
      "3.6500000000000004\n",
      "3.5500000000000003\n",
      "3.3500000000000005\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.5500000000000003\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.5500000000000003\n",
      "3.2500000000000004\n",
      "3.45\n",
      "3.5500000000000003\n",
      "3.5500000000000003\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.5500000000000003\n",
      "3.5500000000000003\n",
      "3.7500000000000004\n",
      "3.1500000000000004\n",
      "3.1500000000000004\n",
      "3.2500000000000004\n",
      "3.5500000000000003\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.5500000000000003\n",
      "3.45\n",
      "3.45\n",
      "3.6500000000000004\n",
      "3.45\n",
      "3.6500000000000004\n",
      "3.5500000000000003\n",
      "3.2500000000000004\n",
      "3.3500000000000005\n",
      "3.3500000000000005\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.45\n",
      "3.2500000000000004\n",
      "3.5500000000000003\n",
      "3.95\n",
      "3.5500000000000003\n",
      "3.6500000000000004\n",
      "3.5500000000000003\n",
      "3.2500000000000004\n",
      "3.7500000000000004\n",
      "3.5500000000000003\n",
      "3.45\n",
      "3.7500000000000004\n",
      "3.7500000000000004\n",
      "3.2500000000000004\n",
      "3.45\n",
      "3.5500000000000003\n",
      "3.45\n",
      "3.5500000000000003\n",
      "3.5500000000000003\n",
      "3.45\n",
      "3.3500000000000005\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.5500000000000003\n",
      "3.3500000000000005\n",
      "3.45\n",
      "3.45\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.3500000000000005\n",
      "3.45\n",
      "3.45\n",
      "3.45\n",
      "3.7500000000000004\n",
      "3.6500000000000004\n",
      "3.45\n",
      "3.6500000000000004\n",
      "3.45\n",
      "3.2500000000000004\n",
      "3.6500000000000004\n",
      "3.45\n",
      "3.3500000000000005\n",
      "3.5500000000000003\n",
      "3.6500000000000004\n",
      "3.2500000000000004\n",
      "3.3500000000000005\n",
      "3.3500000000000005\n",
      "3.5500000000000003\n",
      "3.5500000000000003\n",
      "3.8500000000000005\n",
      "3.45\n",
      "3.7500000000000004\n",
      "3.6500000000000004\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.95\n",
      "3.7500000000000004\n",
      "3.45\n",
      "313.45\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "4.3500000000000005\n",
      "4.15\n",
      "3.95\n",
      "3.95\n",
      "4.050000000000001\n",
      "4.15\n",
      "4.15\n",
      "4.3500000000000005\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.8500000000000005\n",
      "4.15\n",
      "4.15\n",
      "4.050000000000001\n",
      "4.050000000000001\n",
      "3.95\n",
      "4.15\n",
      "3.8500000000000005\n",
      "4.050000000000001\n",
      "3.95\n",
      "4.15\n",
      "4.050000000000001\n",
      "4.25\n",
      "4.3500000000000005\n",
      "3.95\n",
      "4.050000000000001\n",
      "3.95\n",
      "3.8500000000000005\n",
      "3.8500000000000005\n",
      "3.95\n",
      "3.8500000000000005\n",
      "4.15\n",
      "4.15\n",
      "3.95\n",
      "4.15\n",
      "4.15\n",
      "3.8500000000000005\n",
      "4.15\n",
      "4.050000000000001\n",
      "4.050000000000001\n",
      "4.050000000000001\n",
      "3.8500000000000005\n",
      "4.15\n",
      "4.15\n",
      "3.95\n",
      "3.95\n",
      "4.15\n",
      "4.050000000000001\n",
      "3.8500000000000005\n",
      "4.15\n",
      "3.95\n",
      "4.15\n",
      "4.050000000000001\n",
      "4.050000000000001\n",
      "4.15\n",
      "4.15\n",
      "3.8500000000000005\n",
      "4.25\n",
      "4.3500000000000005\n",
      "3.7500000000000004\n",
      "3.95\n",
      "3.95\n",
      "3.8500000000000005\n",
      "4.15\n",
      "3.8500000000000005\n",
      "4.050000000000001\n",
      "4.15\n",
      "3.8500000000000005\n",
      "4.45\n",
      "3.95\n",
      "3.8500000000000005\n",
      "4.050000000000001\n",
      "4.45\n",
      "4.050000000000001\n",
      "4.15\n",
      "4.15\n",
      "3.8500000000000005\n",
      "3.95\n",
      "4.050000000000001\n",
      "4.25\n",
      "4.050000000000001\n",
      "3.95\n",
      "4.25\n",
      "3.7500000000000004\n",
      "3.95\n",
      "4.050000000000001\n",
      "4.15\n",
      "3.95\n",
      "4.15\n",
      "4.15\n",
      "4.050000000000001\n",
      "4.3500000000000005\n",
      "4.15\n",
      "4.050000000000001\n",
      "4.25\n",
      "4.15\n",
      "3.7500000000000004\n",
      "3.95\n",
      "4.15\n",
      "4.050000000000001\n",
      "3.95\n",
      "4.3500000000000005\n",
      "3.8500000000000005\n",
      "4.25\n",
      "3.95\n",
      "3.95\n",
      "4.050000000000001\n",
      "4.15\n",
      "3.95\n",
      "4.25\n",
      "4.3500000000000005\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "4.15\n",
      "3.95\n",
      "3.95\n",
      "3.95\n",
      "3.8500000000000005\n",
      "4.050000000000001\n",
      "3.8500000000000005\n",
      "3.95\n",
      "4.25\n",
      "4.050000000000001\n",
      "4.050000000000001\n",
      "4.050000000000001\n",
      "4.25\n",
      "4.25\n",
      "4.15\n",
      "3.7500000000000004\n",
      "4.15\n",
      "3.95\n",
      "4.15\n",
      "4.25\n",
      "3.5500000000000003\n",
      "3.7500000000000004\n",
      "4.3500000000000005\n",
      "4.3500000000000005\n",
      "3.95\n",
      "4.15\n",
      "4.3500000000000005\n",
      "4.15\n",
      "4.050000000000001\n",
      "4.25\n",
      "4.15\n",
      "4.15\n",
      "4.050000000000001\n",
      "4.25\n",
      "3.95\n",
      "4.15\n",
      "4.050000000000001\n",
      "-56.35\n",
      "-196.55\n",
      "3.8500000000000005\n",
      "3.5500000000000007\n",
      "3.7500000000000004\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.7500000000000004\n",
      "3.5500000000000007\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.7500000000000004\n",
      "3.4500000000000006\n",
      "3.3500000000000005\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.9500000000000006\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.8500000000000005\n",
      "3.5500000000000007\n",
      "3.9500000000000006\n",
      "3.7500000000000004\n",
      "3.6500000000000004\n",
      "3.4500000000000006\n",
      "4.15\n",
      "3.6500000000000004\n",
      "4.050000000000001\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.9500000000000006\n",
      "3.4500000000000006\n",
      "3.7500000000000004\n",
      "3.9500000000000006\n",
      "3.5500000000000007\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.7500000000000004\n",
      "4.15\n",
      "3.7500000000000004\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.5500000000000007\n",
      "3.5500000000000007\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.7500000000000004\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.4500000000000006\n",
      "3.8500000000000005\n",
      "3.4500000000000006\n",
      "3.5500000000000007\n",
      "4.050000000000001\n",
      "4.050000000000001\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.5500000000000007\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.7500000000000004\n",
      "3.9500000000000006\n",
      "3.3500000000000005\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.7500000000000004\n",
      "3.5500000000000007\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.7500000000000004\n",
      "3.6500000000000004\n",
      "3.5500000000000007\n",
      "3.9500000000000006\n",
      "3.3500000000000005\n",
      "3.8500000000000005\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "4.050000000000001\n",
      "3.6500000000000004\n",
      "3.3500000000000005\n",
      "3.7500000000000004\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.7500000000000004\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.8500000000000005\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.5500000000000007\n",
      "3.4500000000000006\n",
      "4.050000000000001\n",
      "4.050000000000001\n",
      "3.9500000000000006\n",
      "3.8500000000000005\n",
      "3.9500000000000006\n",
      "3.8500000000000005\n",
      "4.050000000000001\n",
      "3.9500000000000006\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.5500000000000007\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "4.050000000000001\n",
      "3.9500000000000006\n",
      "4.050000000000001\n",
      "3.8500000000000005\n",
      "4.050000000000001\n",
      "3.9500000000000006\n",
      "3.8500000000000005\n",
      "3.7500000000000004\n",
      "3.9500000000000006\n",
      "3.7500000000000004\n",
      "3.5500000000000007\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.6500000000000004\n",
      "4.050000000000001\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.5500000000000007\n",
      "3.8500000000000005\n",
      "3.5500000000000007\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.8500000000000005\n",
      "3.7500000000000004\n",
      "3.7500000000000004\n",
      "3.7500000000000004\n",
      "3.7500000000000004\n",
      "3.8500000000000005\n",
      "3.3500000000000005\n",
      "3.4500000000000006\n",
      "3.8500000000000005\n",
      "3.5500000000000007\n",
      "3.6500000000000004\n",
      "4.15\n",
      "3.8500000000000005\n",
      "3.9500000000000006\n",
      "3.8500000000000005\n",
      "3.9500000000000006\n",
      "3.6500000000000004\n",
      "3.5500000000000007\n",
      "3.5500000000000007\n",
      "3.6500000000000004\n",
      "3.8500000000000005\n",
      "3.8500000000000005\n",
      "3.8500000000000005\n",
      "3.9500000000000006\n",
      "3.7500000000000004\n",
      "3.6500000000000004\n",
      "4.050000000000001\n",
      "4.050000000000001\n",
      "3.5500000000000007\n",
      "3.5500000000000007\n",
      "3.6500000000000004\n",
      "3.6500000000000004\n",
      "3.7500000000000004\n",
      "3.9500000000000006\n",
      "3.8500000000000005\n",
      "3.8500000000000005\n",
      "3.6500000000000004\n",
      "3.9500000000000006\n",
      "3.8500000000000005\n",
      "3.7500000000000004\n",
      "3.9500000000000006\n",
      "3.5500000000000007\n",
      "4.65\n",
      "4.8500000000000005\n",
      "4.550000000000001\n",
      "4.550000000000001\n",
      "4.550000000000001\n",
      "4.75\n",
      "4.65\n",
      "5.050000000000001\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.65\n",
      "4.65\n",
      "4.95\n",
      "4.8500000000000005\n",
      "4.75\n",
      "4.75\n",
      "4.45\n",
      "4.550000000000001\n",
      "4.65\n",
      "4.65\n",
      "4.65\n",
      "4.550000000000001\n",
      "4.550000000000001\n",
      "4.95\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "4.550000000000001\n",
      "4.45\n",
      "4.65\n",
      "4.95\n",
      "4.8500000000000005\n",
      "4.65\n",
      "4.65\n",
      "4.65\n",
      "4.75\n",
      "4.95\n",
      "4.75\n",
      "4.65\n",
      "4.8500000000000005\n",
      "5.050000000000001\n",
      "4.75\n",
      "4.65\n",
      "4.65\n",
      "4.65\n",
      "4.95\n",
      "4.65\n",
      "5.050000000000001\n",
      "4.45\n",
      "4.65\n",
      "4.550000000000001\n",
      "4.75\n",
      "4.75\n",
      "4.65\n",
      "5.050000000000001\n",
      "4.75\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.65\n",
      "4.65\n",
      "5.050000000000001\n",
      "4.65\n",
      "4.550000000000001\n",
      "4.65\n",
      "4.95\n",
      "4.550000000000001\n",
      "4.75\n",
      "4.65\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "4.65\n",
      "4.75\n",
      "4.95\n",
      "4.550000000000001\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "5.050000000000001\n",
      "4.8500000000000005\n",
      "4.95\n",
      "4.75\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.95\n",
      "4.75\n",
      "5.050000000000001\n",
      "4.75\n",
      "4.65\n",
      "4.95\n",
      "4.3500000000000005\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "4.550000000000001\n",
      "4.95\n",
      "4.65\n",
      "5.250000000000001\n",
      "4.75\n",
      "4.65\n",
      "4.95\n",
      "4.8500000000000005\n",
      "4.65\n",
      "4.550000000000001\n",
      "4.550000000000001\n",
      "5.050000000000001\n",
      "4.75\n",
      "5.050000000000001\n",
      "4.75\n",
      "5.050000000000001\n",
      "4.75\n",
      "4.75\n",
      "4.550000000000001\n",
      "4.95\n",
      "4.65\n",
      "5.15\n",
      "5.050000000000001\n",
      "4.65\n",
      "4.45\n",
      "4.3500000000000005\n",
      "4.45\n",
      "4.95\n",
      "4.75\n",
      "4.550000000000001\n",
      "4.8500000000000005\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.65\n",
      "4.75\n",
      "4.45\n",
      "4.65\n",
      "4.75\n",
      "5.050000000000001\n",
      "5.050000000000001\n",
      "4.550000000000001\n",
      "4.75\n",
      "4.75\n",
      "4.95\n",
      "4.45\n",
      "4.8500000000000005\n",
      "4.45\n",
      "4.75\n",
      "4.75\n",
      "4.95\n",
      "4.75\n",
      "4.65\n",
      "4.95\n",
      "4.45\n",
      "4.8500000000000005\n",
      "4.75\n",
      "4.95\n",
      "4.65\n",
      "4.8500000000000005\n",
      "4.550000000000001\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.65\n",
      "4.75\n",
      "4.65\n",
      "4.95\n",
      "4.75\n",
      "4.75\n",
      "4.8500000000000005\n",
      "5.050000000000001\n",
      "5.15\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.75\n",
      "4.95\n",
      "4.65\n",
      "4.65\n",
      "4.550000000000001\n",
      "4.550000000000001\n",
      "4.65\n",
      "5.15\n",
      "4.75\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.75\n",
      "4.75\n",
      "4.75\n",
      "4.75\n",
      "4.95\n",
      "4.75\n",
      "4.95\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "4.65\n",
      "5.050000000000001\n",
      "4.65\n",
      "5.050000000000001\n",
      "4.75\n",
      "4.75\n",
      "4.75\n",
      "4.65\n",
      "4.550000000000001\n",
      "4.65\n",
      "4.95\n",
      "4.550000000000001\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.95\n",
      "4.8500000000000005\n",
      "4.550000000000001\n",
      "4.65\n",
      "5.15\n",
      "4.65\n",
      "5.050000000000001\n",
      "4.550000000000001\n",
      "4.75\n",
      "4.3500000000000005\n",
      "4.550000000000001\n",
      "4.8500000000000005\n",
      "5.050000000000001\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "4.65\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.75\n",
      "4.65\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.75\n",
      "4.95\n",
      "4.75\n",
      "4.75\n",
      "4.45\n",
      "4.75\n",
      "4.75\n",
      "5.050000000000001\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "4.75\n",
      "4.95\n",
      "5.15\n",
      "4.8500000000000005\n",
      "4.65\n",
      "4.65\n",
      "5.050000000000001\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "4.550000000000001\n",
      "4.75\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.8500000000000005\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.75\n",
      "4.550000000000001\n",
      "4.550000000000001\n",
      "4.65\n",
      "4.65\n",
      "4.8500000000000005\n",
      "4.550000000000001\n",
      "4.75\n",
      "4.3500000000000005\n",
      "4.8500000000000005\n",
      "4.65\n",
      "5.050000000000001\n",
      "4.65\n",
      "4.95\n",
      "4.65\n",
      "4.550000000000001\n",
      "5.050000000000001\n",
      "4.95\n",
      "4.550000000000001\n",
      "4.65\n",
      "4.65\n",
      "4.65\n",
      "4.550000000000001\n",
      "5.15\n",
      "4.65\n",
      "4.65\n",
      "4.75\n",
      "4.75\n",
      "4.8500000000000005\n",
      "4.95\n",
      "4.65\n",
      "5.15\n",
      "4.65\n",
      "4.95\n",
      "815.0\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.6000000000000005\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "4.7\n",
      "4.6000000000000005\n",
      "4.800000000000001\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.7\n",
      "4.7\n",
      "4.6000000000000005\n",
      "5.0\n",
      "4.9\n",
      "4.5\n",
      "5.0\n",
      "4.5\n",
      "4.7\n",
      "4.7\n",
      "5.0\n",
      "4.7\n",
      "5.2\n",
      "4.7\n",
      "5.0\n",
      "4.5\n",
      "4.9\n",
      "4.800000000000001\n",
      "5.0\n",
      "5.0\n",
      "4.5\n",
      "4.800000000000001\n",
      "5.0\n",
      "4.9\n",
      "4.6000000000000005\n",
      "4.800000000000001\n",
      "4.6000000000000005\n",
      "4.7\n",
      "4.9\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.7\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "5.0\n",
      "4.7\n",
      "4.6000000000000005\n",
      "5.0\n",
      "4.6000000000000005\n",
      "4.800000000000001\n",
      "4.9\n",
      "4.9\n",
      "4.9\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.4\n",
      "4.7\n",
      "4.9\n",
      "5.0\n",
      "4.9\n",
      "5.0\n",
      "4.9\n",
      "5.1000000000000005\n",
      "4.7\n",
      "5.0\n",
      "4.800000000000001\n",
      "4.7\n",
      "4.7\n",
      "4.800000000000001\n",
      "4.7\n",
      "4.800000000000001\n",
      "4.7\n",
      "4.800000000000001\n",
      "4.9\n",
      "4.7\n",
      "4.7\n",
      "4.7\n",
      "4.800000000000001\n",
      "4.4\n",
      "4.6000000000000005\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.5\n",
      "4.9\n",
      "4.7\n",
      "5.0\n",
      "5.1000000000000005\n",
      "4.7\n",
      "4.9\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.6000000000000005\n",
      "4.7\n",
      "4.7\n",
      "4.7\n",
      "4.800000000000001\n",
      "4.5\n",
      "4.9\n",
      "4.7\n",
      "4.7\n",
      "4.9\n",
      "4.4\n",
      "4.7\n",
      "5.0\n",
      "4.7\n",
      "5.0\n",
      "4.6000000000000005\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "4.6000000000000005\n",
      "4.7\n",
      "5.0\n",
      "4.800000000000001\n",
      "4.6000000000000005\n",
      "4.9\n",
      "5.0\n",
      "4.7\n",
      "4.5\n",
      "5.1000000000000005\n",
      "4.800000000000001\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.5\n",
      "4.9\n",
      "4.7\n",
      "5.2\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.9\n",
      "5.1000000000000005\n",
      "4.7\n",
      "4.7\n",
      "4.9\n",
      "4.6000000000000005\n",
      "4.5\n",
      "5.1000000000000005\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.9\n",
      "4.6000000000000005\n",
      "4.7\n",
      "4.800000000000001\n",
      "4.6000000000000005\n",
      "4.7\n",
      "5.2\n",
      "4.9\n",
      "5.1000000000000005\n",
      "4.9\n",
      "5.1000000000000005\n",
      "4.7\n",
      "4.7\n",
      "4.9\n",
      "4.9\n",
      "4.800000000000001\n",
      "4.800000000000001\n",
      "4.9\n",
      "5.0\n",
      "4.9\n",
      "4.9\n",
      "4.7\n",
      "5.0\n",
      "4.6000000000000005\n",
      "4.4\n",
      "5.0\n",
      "4.800000000000001\n",
      "4.8\n"
     ]
    }
   ],
   "source": [
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "# Set flag to flase\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        time.sleep(0.01)\n",
    "        if reward != 0:\n",
    "            print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closes the game environment - important given we can only run one at a time \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/get-started/locally/  <- use this site to download pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/35/70/a14c72d280483b86939adf22c780bc81728dde26e89bc7aeb3d2ba63b063/torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/94/ce/b2ee0129bfe1280ae23e6f45d918072945676b14e80af750d7b3d63eecea/torchvision-0.17.2-cp38-cp38-macosx_10_13_x86_64.whl.metadata\n",
      "  Using cached torchvision-0.17.2-cp38-cp38-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Obtaining dependency information for torchaudio from https://files.pythonhosted.org/packages/ff/b6/87316feb58466fe9683f91a3221f7b024e707c2365be23bd60c626067c4c/torchaudio-2.2.2-cp38-cp38-macosx_10_13_x86_64.whl.metadata\n",
      "  Using cached torchaudio-2.2.2-cp38-cp38-macosx_10_13_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/b9/f8/feced7779d755758a52d1f6635d990b8d98dc0a29fa568bbe0625f18fdf3/filelock-3.16.1-py3-none-any.whl.metadata\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.8/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Obtaining dependency information for sympy from https://files.pythonhosted.org/packages/99/ff/c87e0622b1dadea79d2fb0b25ade9ed98954c9033722eb707053d310d4f3/sympy-1.13.3-py3-none-any.whl.metadata\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/a8/05/9d4f9b78ead6b2661d6e8ea772e111fc4a9fbd866ad0c81906c11206b55e/networkx-3.1-py3-none-any.whl.metadata\n",
      "  Using cached networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Obtaining dependency information for jinja2 from https://files.pythonhosted.org/packages/31/80/3a54838c3fb461f6fec263ebf3a3a41771bd05190238de3486aae8540c36/jinja2-3.1.4-py3-none-any.whl.metadata\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/c6/b2/454d6e7f0158951d8a78c2e1eb4f69ae81beb8dca5fee9809c6c99e9d0d0/fsspec-2024.10.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.8/site-packages (from torchvision) (10.4.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/4f/14/6f294b9c4f969d0c801a4615e221c1e084722ea6114ab2114189c5b8cbe0/MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Obtaining dependency information for mpmath<1.4,>=1.1.0 from https://files.pythonhosted.org/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl.metadata\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl (150.6 MB)\n",
      "Using cached torchvision-0.17.2-cp38-cp38-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "Using cached torchaudio-2.2.2-cp38-cp38-macosx_10_13_x86_64.whl (3.4 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl (14 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.16.1 fsspec-2024.10.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.1 sympy-1.13.3 torch-2.2.2 torchaudio-2.2.2 torchvision-0.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines3[extra]==1.3.0\n",
      "  Obtaining dependency information for stable-baselines3[extra]==1.3.0 from https://files.pythonhosted.org/packages/ea/c1/bdf01c4753ca3b4a99248a179446c5f42109f4485c606fd7e96e7090d4b9/stable_baselines3-1.3.0-py3-none-any.whl.metadata\n",
      "  Downloading stable_baselines3-1.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting gym<0.20,>=0.17 (from stable-baselines3[extra]==1.3.0)\n",
      "  Downloading gym-0.19.0.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from stable-baselines3[extra]==1.3.0) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.8.1 in ./.venv/lib/python3.8/site-packages (from stable-baselines3[extra]==1.3.0) (2.2.2)\n",
      "Requirement already satisfied: cloudpickle in ./.venv/lib/python3.8/site-packages (from stable-baselines3[extra]==1.3.0) (3.1.0)\n",
      "Collecting pandas (from stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/78/a8/07dd10f90ca915ed914853cd57f79bfc22e1ef4384ab56cb4336d2fc1f2a/pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (from stable-baselines3[extra]==1.3.0) (3.7.5)\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.8/site-packages (from stable-baselines3[extra]==1.3.0) (4.10.0.84)\n",
      "Collecting atari-py~=0.2.0 (from stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for atari-py~=0.2.0 from https://files.pythonhosted.org/packages/ad/4c/30800b0a5366abb58df3c85057a878818d86156051fc2c26540639eb13c3/atari_py-0.2.9-cp38-cp38-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading atari_py-0.2.9-cp38-cp38-macosx_10_12_x86_64.whl.metadata (269 bytes)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.8/site-packages (from stable-baselines3[extra]==1.3.0) (10.4.0)\n",
      "Collecting tensorboard>=2.2.0 (from stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for tensorboard>=2.2.0 from https://files.pythonhosted.org/packages/bc/a2/ff5f4c299eb37c95299a76015da3f30211468e29d8d6f1d011683279baee/tensorboard-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.8/site-packages (from stable-baselines3[extra]==1.3.0) (6.1.0)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.8/site-packages (from atari-py~=0.2.0->stable-baselines3[extra]==1.3.0) (1.16.0)\n",
      "Collecting cloudpickle (from stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for cloudpickle from https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/04/66/2c54202710a09c0fd7ef024e449e49cbc3cfd8ae739a209db6dab0349007/grpcio-1.67.1-cp38-cp38-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading grpcio-1.67.1-cp38-cp38-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/27/1f/3a72917afcb0d5cd842cbccb81bf7a8a7b45b4c66d8dc4556ccb3b016bfc/google_auth-2.35.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for google-auth-oauthlib<1.1,>=0.5 from https://files.pythonhosted.org/packages/4a/07/8d9a8186e6768b55dfffeb57c719bc03770cf8a970a074616ae6f9e26a57/google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/3f/08/83871f3c50fc983b88547c196d11cf8c3340e37c32d2e9d6152abe2c61f7/Markdown-3.7-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf>=3.19.6 (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for protobuf>=3.19.6 from https://files.pythonhosted.org/packages/1c/f2/baf397f3dd1d3e4af7e3f5a0382b868d25ac068eefe1ebde05132333436c/protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for requests<3,>=2.21.0 from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0) (65.5.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/b7/85/dabeaf902892922777492e1d253bb7e1264cadce3cea932f7ff599e53fea/tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/6c/69/05837f91dfe42109203ffa3e488214ff86a6d68b2ed6c167da6cdc42349b/werkzeug-3.0.6-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in ./.venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0) (0.38.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from torch>=1.8.1->stable-baselines3[extra]==1.3.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.8/site-packages (from torch>=1.8.1->stable-baselines3[extra]==1.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.8/site-packages (from torch>=1.8.1->stable-baselines3[extra]==1.3.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.8/site-packages (from torch>=1.8.1->stable-baselines3[extra]==1.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from torch>=1.8.1->stable-baselines3[extra]==1.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.8/site-packages (from torch>=1.8.1->stable-baselines3[extra]==1.3.0) (2024.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3[extra]==1.3.0) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3[extra]==1.3.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3[extra]==1.3.0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3[extra]==1.3.0) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3[extra]==1.3.0) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3[extra]==1.3.0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3[extra]==1.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3[extra]==1.3.0) (6.4.5)\n",
      "Collecting pytz>=2020.1 (from pandas->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/11/c3/005fcca25ce078d2cc29fd559379817424e94885510568bc1bc53d7d5846/pytz-2024.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for tzdata>=2022.1 from https://files.pythonhosted.org/packages/a6/ab/7e5f53c3b9d14972843a647d8d7a853969a58aecc7559cb3267302c94774/tzdata-2024.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a4/07/14f8ad37f2d12a5ce41206c21820d8cb6561b728e51fad4530dff0552a67/cachetools-5.5.0-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for pyasn1-modules>=0.2.1 from https://files.pythonhosted.org/packages/77/89/bc88a6711935ba795a679ea6ebee07e128050d6382eaa35a0a47c8032bdc/pyasn1_modules-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for requests-oauthlib>=0.7.0 from https://files.pythonhosted.org/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->stable-baselines3[extra]==1.3.0) (3.20.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0) (8.5.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/94/d4/2b21cb277bac9605026d2d91a4a8872bc82199ed11072d035dc674c27223/charset_normalizer-3.4.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.4.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/ce/d9/5f4c13cecde62396b0d3fe530a50ccea91e7dfc1ccf0e09c228841bb5ba8/urllib3-2.2.3-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/12/90/3c9ff0512038035f59d279fddeb79f5f1eccd8859f06d6163c58798b9487/certifi-2024.8.30-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.8/site-packages (from sympy->torch>=1.8.1->stable-baselines3[extra]==1.3.0) (1.3.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for pyasn1<0.7.0,>=0.4.6 from https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->stable-baselines3[extra]==1.3.0)\n",
      "  Obtaining dependency information for oauthlib>=3.0.0 from https://files.pythonhosted.org/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl.metadata\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading atari_py-0.2.9-cp38-cp38-macosx_10_12_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl (11.7 MB)\n",
      "Downloading stable_baselines3-1.3.0-py3-none-any.whl (174 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.67.1-cp38-cp38-macosx_10_9_universal2.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m414.7/414.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.0-cp38-cp38-macosx_10_9_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m124.5/124.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.19.0-py3-none-any.whl size=1663087 sha256=d3881babc80acd81cd359356cfc08c244f492991b4969a0884dfe32fe0e5f4cc\n",
      "  Stored in directory: /Users/suyogkhanal/Library/Caches/pip/wheels/11/36/28/628f4dd3779e4037a6fca1aaed76827ffa4315c3ab6bfadcf6\n",
      "Successfully built gym\n",
      "Installing collected packages: pytz, werkzeug, urllib3, tzdata, tensorboard-data-server, pyasn1, protobuf, oauthlib, idna, grpcio, cloudpickle, charset-normalizer, certifi, cachetools, atari-py, absl-py, rsa, requests, pyasn1-modules, pandas, markdown, gym, stable-baselines3, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.1.0\n",
      "    Uninstalling cloudpickle-3.1.0:\n",
      "      Successfully uninstalled cloudpickle-3.1.0\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.21.0\n",
      "    Uninstalling gym-0.21.0:\n",
      "      Successfully uninstalled gym-0.21.0\n",
      "Successfully installed absl-py-2.1.0 atari-py-0.2.9 cachetools-5.5.0 certifi-2024.8.30 charset-normalizer-3.4.0 cloudpickle-1.6.0 google-auth-2.35.0 google-auth-oauthlib-1.0.0 grpcio-1.67.1 gym-0.19.0 idna-3.10 markdown-3.7 oauthlib-3.2.2 pandas-2.0.3 protobuf-5.28.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 pytz-2024.2 requests-2.32.3 requests-oauthlib-2.0.0 rsa-4.9 stable-baselines3-1.3.0 tensorboard-2.14.0 tensorboard-data-server-0.7.2 tzdata-2024.2 urllib3-2.2.3 werkzeug-3.0.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'stable-baselines3[extra]==1.3.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above is mac specific, remove quotes and run if not using zsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/4e/41/2a2f5ed6c997367ab7055185cf66d536c228b15a12b8e112a274808f48b5/optuna-4.0.0-py3-none-any.whl.metadata\n",
      "  Using cached optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/cb/06/8b505aea3d77021b18dcbd8133aa1418f1a1e37e432a465b14c46b2c0eaa/alembic-1.14.0-py3-none-any.whl.metadata\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/e3/51/9b208e85196941db2f0654ad0357ca6388ab3ed67efdbfc799f35d1f83aa/colorlog-6.9.0-py3-none-any.whl.metadata\n",
      "  Using cached colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from optuna) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from optuna) (24.1)\n",
      "Collecting sqlalchemy>=1.3.0 (from optuna)\n",
      "  Obtaining dependency information for sqlalchemy>=1.3.0 from https://files.pythonhosted.org/packages/db/da/443679a0b9e0a009f00d1542595c8a4d582ece1809704e703c4843f18768/SQLAlchemy-2.0.36-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached SQLAlchemy-2.0.36-cp38-cp38-macosx_10_9_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting tqdm (from optuna)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/41/73/02342de9c2d20922115f787e101527b831c0cffd2105c946c4a4826bcfd4/tqdm-4.66.6-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting PyYAML (from optuna)\n",
      "  Obtaining dependency information for PyYAML from https://files.pythonhosted.org/packages/74/d9/323a59d506f12f498c2097488d80d16f4cf965cee1791eab58b56b19f47a/PyYAML-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached PyYAML-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/48/22/bc14c6f02e6dccaafb3eba95764c8f096714260c2aa5f76f654fd16a23dd/Mako-1.3.6-py3-none-any.whl.metadata\n",
      "  Using cached Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in ./.venv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: importlib-metadata in ./.venv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (8.5.0)\n",
      "Requirement already satisfied: importlib-resources in ./.venv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (6.4.5)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.3.0->optuna)\n",
      "  Obtaining dependency information for greenlet!=0.4.17 from https://files.pythonhosted.org/packages/97/83/bdf5f69fcf304065ec7cf8fc7c08248479cfed9bcca02bf0001c07e000aa/greenlet-3.1.1-cp38-cp38-macosx_11_0_universal2.whl.metadata\n",
      "  Using cached greenlet-3.1.1-cp38-cp38-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.8/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./.venv/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Using cached optuna-4.0.0-py3-none-any.whl (362 kB)\n",
      "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached SQLAlchemy-2.0.36-cp38-cp38-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "Using cached colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Using cached PyYAML-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl (183 kB)\n",
      "Using cached tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "Using cached greenlet-3.1.1-cp38-cp38-macosx_11_0_universal2.whl (271 kB)\n",
      "Using cached Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, PyYAML, Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "Successfully installed Mako-1.3.6 PyYAML-6.0.2 alembic-1.14.0 colorlog-6.9.0 greenlet-3.1.1 optuna-4.0.0 sqlalchemy-2.0.36 tqdm-4.66.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suyogkhanal/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the optimzation frame - HPO\n",
    "import optuna\n",
    "# PPO algo for RL\n",
    "from stable_baselines3 import PPO\n",
    "# Bring in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import the sb3 monitor for logging \n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "# Import os to deal with filepaths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return test hyperparameters - define the object function\n",
    "def optimize_ppo(trial): \n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps', 2048, 8192), # number of frames used in one batch of training (must use a factor of 64) (maybe take a number and multiply it by 64? )\n",
    "        'gamma':trial.suggest_loguniform('gamma', 0.8, 0.9999), # discount rate\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate', 1e-5, 1e-4), # how fast we tune optimizer (Critic and Actor for PPO)\n",
    "        'clip_range':trial.suggest_uniform('clip_range', 0.1, 0.4), # how far we want to clip for our advantage value in PPO\n",
    "        'gae_lambda':trial.suggest_uniform('gae_lambda', 0.8, 0.99) # smoothing parameter (used when calculating advantage)\n",
    "    }\n",
    "\n",
    "# IF U WANT TO USE OTHER ALGOS THE HYPERPARAMS MUST BE SWITCHED AS WELL (DQN, SAC, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative version to use later to bypass factor 64 error\n",
    "\n",
    "\n",
    "\n",
    "# # PPO optimization parameters\n",
    "\n",
    "# PPO_PARAMS = {\n",
    "\n",
    "#     'n_steps_range': (2048, 8192),\n",
    "\n",
    "#     'gamma_range': (0.8, 0.9999),\n",
    "\n",
    "#     'learning_rate_range': (1e-5, 1e-4),\n",
    "\n",
    "#     'clip_range_range': (0.1, 0.4),\n",
    "\n",
    "#     'gae_lambda_range': (0.8, 0.99),\n",
    "\n",
    "# }\n",
    "\n",
    "# # Define the optimization function for PPO\n",
    "\n",
    "# def optimize_ppo(trial): \n",
    "\n",
    "#     n_steps = trial.suggest_categorical('n_steps', range(PPO_PARAMS['n_steps_range'][0], PPO_PARAMS['n_steps_range'][1], 64))  # Steps of 64\n",
    "\n",
    "#     return {\n",
    "\n",
    "#         'n_steps': n_steps,\n",
    "\n",
    "#         'gamma': trial.suggest_loguniform('gamma', *PPO_PARAMS['gamma_range']),\n",
    "\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', *PPO_PARAMS['learning_rate_range']),\n",
    "\n",
    "#         'clip_range': trial.suggest_uniform('clip_range', *PPO_PARAMS['clip_range_range']),\n",
    "\n",
    "#         'gae_lambda': trial.suggest_uniform('gae_lambda', *PPO_PARAMS['gae_lambda_range']),\n",
    "\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training loop and return mean reward \n",
    "def optimize_agent(trial):\n",
    "    # try:\n",
    "        model_params = optimize_ppo(trial)  # get dict of hyperparams\n",
    "\n",
    "        # Create environment \n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR) # allowd for logging mean episode reward and mean episode length\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        # Create algo \n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params) # uses model params dict\n",
    "        model.learn(total_timesteps=10000) # 100000 for production model (longer if can)\n",
    "\n",
    "        # Evaluate model \n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=1) # 5 different games usually but went lower cuz i dont got cuda (those who know)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    # except Exception as e: # used in case hyperparam throws error (allows to continue training)\n",
    "    #     return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-04 00:21:22,524] A new study created in memory with name: no-name-75b592f5-3a58-44ac-9aec-20831c7c628e\n",
      "/var/folders/d4/_v_lx6gx66z8htqrwxr89lvc0000gn/T/ipykernel_86005/3706438210.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'gamma':trial.suggest_loguniform('gamma', 0.8, 0.9999), # discount rate\n",
      "/var/folders/d4/_v_lx6gx66z8htqrwxr89lvc0000gn/T/ipykernel_86005/3706438210.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate':trial.suggest_loguniform('learning_rate', 1e-5, 1e-4), # how fast we tune optimizer (Critic and Actor for PPO)\n",
      "/var/folders/d4/_v_lx6gx66z8htqrwxr89lvc0000gn/T/ipykernel_86005/3706438210.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'clip_range':trial.suggest_uniform('clip_range', 0.1, 0.4), # how far we want to clip for our advantage value in PPO\n",
      "/var/folders/d4/_v_lx6gx66z8htqrwxr89lvc0000gn/T/ipykernel_86005/3706438210.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'gae_lambda':trial.suggest_uniform('gae_lambda', 0.8, 0.99) # smoothing parameter (used when calculating advantage)\n",
      "[W 2024-11-04 00:21:22,612] Trial 0 failed with parameters: {'n_steps': 2744, 'gamma': 0.8779151472586653, 'learning_rate': 4.658266155214521e-05, 'clip_range': 0.36718642934780177, 'gae_lambda': 0.970603977599398} because of the following error: RuntimeError('Cannot create multiple emulator instances per process, make sure to call env.close() on each environment before creating a new one').\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/suyogkhanal/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/d4/_v_lx6gx66z8htqrwxr89lvc0000gn/T/ipykernel_86005/1670576652.py\", line 7, in optimize_agent\n",
      "    env = StreetFighter()\n",
      "  File \"/var/folders/d4/_v_lx6gx66z8htqrwxr89lvc0000gn/T/ipykernel_86005/831802053.py\", line 11, in __init__\n",
      "    self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED) # used to get valid button combos\n",
      "  File \"/Users/suyogkhanal/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/retro/__init__.py\", line 55, in make\n",
      "    return RetroEnv(game, state, inttype=inttype, **kwargs)\n",
      "  File \"/Users/suyogkhanal/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/retro/retro_env.py\", line 87, in __init__\n",
      "    self.em = retro.RetroEmulator(rom_path)\n",
      "RuntimeError: Cannot create multiple emulator instances per process, make sure to call env.close() on each environment before creating a new one\n",
      "[W 2024-11-04 00:21:22,613] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot create multiple emulator instances per process, make sure to call env.close() on each environment before creating a new one",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Creating the experiment \u001b[39;00m\n\u001b[1;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# since mean reward is positive we maximize, otherwise minimize\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimize_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# for prod used n_trials=100\u001b[39;00m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[65], line 7\u001b[0m, in \u001b[0;36moptimize_agent\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      4\u001b[0m model_params \u001b[38;5;241m=\u001b[39m optimize_ppo(trial)  \u001b[38;5;66;03m# get dict of hyperparams\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create environment \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mStreetFighter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m env \u001b[38;5;241m=\u001b[39m Monitor(env, LOG_DIR) \u001b[38;5;66;03m# allowd for logging mean episode reward and mean episode length\u001b[39;00m\n\u001b[1;32m      9\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env])\n",
      "Cell \u001b[0;32mIn[58], line 11\u001b[0m, in \u001b[0;36mStreetFighter.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menemy_health \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m144\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Startup and instance of the game \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame \u001b[38;5;241m=\u001b[39m \u001b[43mretro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStreetFighterIISpecialChampionEdition-Genesis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_restricted_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mActions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFILTERED\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/retro/__init__.py:55\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(game, state, inttype, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGame not found: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Did you make sure to import the ROM?\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m game)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRetroEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minttype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minttype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/retro/retro_env.py:87\u001b[0m, in \u001b[0;36mRetroEnv.__init__\u001b[0;34m(self, game, state, scenario, info, use_restricted_actions, record, players, inttype, obs_type)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# We can't have more than one emulator per process. Before creating an\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# emulator, ensure that unused ones are garbage-collected\u001b[39;00m\n\u001b[1;32m     86\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem \u001b[38;5;241m=\u001b[39m \u001b[43mretro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRetroEmulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrom_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem\u001b[38;5;241m.\u001b[39mconfigure_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot create multiple emulator instances per process, make sure to call env.close() on each environment before creating a new one"
     ]
    }
   ],
   "source": [
    "# Creating the experiment \n",
    "study = optuna.create_study(direction='maximize') # since mean reward is positive we maximize, otherwise minimize\n",
    "study.optimize(optimize_agent, n_trials=1, n_jobs=1) # for prod used n_trials=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_steps': 7717,\n",
       " 'gamma': 0.8951474854534238,\n",
       " 'learning_rate': 1.5173785862671653e-05,\n",
       " 'clip_range': 0.13355095227019517,\n",
       " 'gae_lambda': 0.9181855173148512}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, state=TrialState.COMPLETE, values=[2000.0], datetime_start=datetime.datetime(2024, 10, 22, 1, 12, 54, 242354), datetime_complete=datetime.datetime(2024, 10, 22, 1, 14, 56, 805445), params={'n_steps': 7717, 'gamma': 0.8951474854534238, 'learning_rate': 1.5173785862671653e-05, 'clip_range': 0.13355095227019517, 'gae_lambda': 0.9181855173148512}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_steps': IntDistribution(high=8192, log=False, low=2048, step=1), 'gamma': FloatDistribution(high=0.9999, log=True, low=0.8, step=None), 'learning_rate': FloatDistribution(high=0.0001, log=True, low=1e-05, step=None), 'clip_range': FloatDistribution(high=0.4, log=False, low=0.1, step=None), 'gae_lambda': FloatDistribution(high=0.99, log=False, low=0.8, step=None)}, trial_id=0, value=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'opt/trial_0_best_model.zip.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOPT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrial_0_best_model.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:680\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    678\u001b[0m     get_system_info()\n\u001b[0;32m--> 680\u001b[0m data, params, pytorch_variables \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_zip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_system_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo params found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:403\u001b[0m, in \u001b[0;36mload_from_zip_file\u001b[0;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_zip_file\u001b[39m(\n\u001b[1;32m    377\u001b[0m     load_path: Union[\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath, io\u001b[38;5;241m.\u001b[39mBufferedIOBase],\n\u001b[1;32m    378\u001b[0m     load_data: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     print_system_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    Load model data from a .zip archive\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m        and dict of pytorch variables\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mopen_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/functools.py:875\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    873\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:240\u001b[0m, in \u001b[0;36mopen_path_str\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;129m@open_path\u001b[39m\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_path_str\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m, verbose: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, suffix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m io\u001b[38;5;241m.\u001b[39mBufferedIOBase:\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    that the path exists.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:291\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    285\u001b[0m         path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# if opening was successful uses the open_path() function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m#   with corrections\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:272\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    270\u001b[0m             path, suffix \u001b[38;5;241m=\u001b[39m newpath, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:264\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m open_path(\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, mode, verbose, suffix)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m suffix \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/pathlib.py:1221\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_closed()\n\u001b[0;32m-> 1221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m               \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/pathlib.py:1077\u001b[0m, in \u001b[0;36mPath._opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_opener\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, flags, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o666\u001b[39m):\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[0;32m-> 1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'opt/trial_0_best_model.zip.zip'"
     ]
    }
   ],
   "source": [
    "model = PPO.load(os.path.join(OPT_DIR, 'trial_0_best_model.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback \n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback): # continuously learn by starting from best parameters done above\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "VecFrameStack only works with gym.spaces.Box and gym.spaces.Dict observation spaces",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m Monitor(env, LOG_DIR)\n\u001b[1;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env])\n\u001b[0;32m----> 5\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mVecFrameStack\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlast\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:22\u001b[0m, in \u001b[0;36mVecFrameStack.__init__\u001b[0;34m(self, venv, n_stack, channels_order)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, venv: VecEnv, n_stack: \u001b[38;5;28mint\u001b[39m, channels_order: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Mapping[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     23\u001b[0m         venv\u001b[38;5;241m.\u001b[39mobservation_space, (spaces\u001b[38;5;241m.\u001b[39mBox, spaces\u001b[38;5;241m.\u001b[39mDict)\n\u001b[1;32m     24\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVecFrameStack only works with gym.spaces.Box and gym.spaces.Dict observation spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs \u001b[38;5;241m=\u001b[39m StackedObservations(venv\u001b[38;5;241m.\u001b[39mnum_envs, n_stack, venv\u001b[38;5;241m.\u001b[39mobservation_space, channels_order)\n\u001b[1;32m     27\u001b[0m     observation_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs\u001b[38;5;241m.\u001b[39mstacked_observation_space\n",
      "\u001b[0;31mAssertionError\u001b[0m: VecFrameStack only works with gym.spaces.Box and gym.spaces.Dict observation spaces"
     ]
    }
   ],
   "source": [
    "# Create environment \n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_params \u001b[38;5;241m=\u001b[39m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params\u001b[49m\n\u001b[1;32m      2\u001b[0m model_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_steps\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7488\u001b[39m  \u001b[38;5;66;03m# set n_steps to 7488 or a factor of 64\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model_params['learning_rate'] = 5e-7 -> if really slow at training\u001b[39;00m\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/optuna/study/study.py:119\u001b[0m, in \u001b[0;36mStudy.best_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_params\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return parameters of the best trial in the study.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_trial\u001b[49m\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/optuna/study/study.py:162\u001b[0m, in \u001b[0;36mStudy.best_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_multi_objective():\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 162\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_study_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If the trial with the best value is infeasible, select the best trial from all feasible\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# trials. Note that the behavior is undefined when constrained optimization without the\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# violation value in the best-valued trial.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m constraints \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39msystem_attrs\u001b[38;5;241m.\u001b[39mget(_CONSTRAINTS_KEY)\n",
      "File \u001b[0;32m~/Coding Projects/StreetFighterAI/.venv/lib/python3.8/site-packages/optuna/storages/_in_memory.py:232\u001b[0m, in \u001b[0;36mInMemoryStorage.get_best_trial\u001b[0;34m(self, study_id)\u001b[0m\n\u001b[1;32m    229\u001b[0m best_trial_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mbest_trial_id\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_trial_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo trials are completed yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mdirections) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "model_params = study.best_params\n",
    "model_params['n_steps'] = 7488  # set n_steps to 7488 or a factor of 64\n",
    "# model_params['learning_rate'] = 5e-7 -> if really slow at training\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, tensorboard_log\u001b[38;5;241m=\u001b[39mLOG_DIR, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mmodel_params\u001b[49m) \u001b[38;5;66;03m# verbose 1 shows results as training\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_params' is not defined"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params) # verbose 1 shows results as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reload previous weights from HPO\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OPT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial_0_best_model.zip\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Reload previous weights from HPO\n",
    "model.load(os.path.join(OPT_DIR, 'trial_0_best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Kick off training \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, callback\u001b[38;5;241m=\u001b[39mcallback) \u001b[38;5;66;03m# timestep 5000000 recommended\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Kick off training \n",
    "model.learn(total_timesteps=5000, callback=callback) # timestep 5000000 recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=. \n",
    "# cd to logs\n",
    "# ^ use to visually see learning progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\StreetFighterRL\\env_3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:435: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  th_object = th.load(file_content, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load('./train/best_model_7000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\StreetFighterRL\\env_3\\lib\\site-packages\\pyglet\\libs\\win32\\__init__.py:318: UserWarning: Could not set COM MTA mode. Unexpected behavior may occur.\n",
      "  warnings.warn(\"Could not set COM MTA mode. Unexpected behavior may occur.\")\n"
     ]
    }
   ],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, render=True, n_eval_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5900.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(model.predict(obs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "# Set flag to flase\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.01)\n",
    "        print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mucking aboyt.213213231112331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([[[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]], [[[255]\n",
      "  [255]\n",
      "  [255]\n",
      "  ...\n",
      "  [255]\n",
      "  [255]\n",
      "  [255]]\n",
      "\n",
      " [[255]\n",
      "  [255]\n",
      "  [255]\n",
      "  ...\n",
      "  [255]\n",
      "  [255]\n",
      "  [255]]\n",
      "\n",
      " [[255]\n",
      "  [255]\n",
      "  [255]\n",
      "  ...\n",
      "  [255]\n",
      "  [255]\n",
      "  [255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255]\n",
      "  [255]\n",
      "  [255]\n",
      "  ...\n",
      "  [255]\n",
      "  [255]\n",
      "  [255]]\n",
      "\n",
      " [[255]\n",
      "  [255]\n",
      "  [255]\n",
      "  ...\n",
      "  [255]\n",
      "  [255]\n",
      "  [255]]\n",
      "\n",
      " [[255]\n",
      "  [255]\n",
      "  [255]\n",
      "  ...\n",
      "  [255]\n",
      "  [255]\n",
      "  [255]]], (84, 84, 1), uint8)\n"
     ]
    }
   ],
   "source": [
    "env = StreetFighter()\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# was on protobuf 5.28.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please note that pyglet 1.3.2 was for tensorboard, the rendering was for newesrt version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
